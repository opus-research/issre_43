[
  {
    "_id": "6621bcc9ac254a19ac29c2d0",
    "number": 5475,
    "body": "Bumps the pip group with 2 updates in the /examples/speech_to_speech/asr_bleu directory: [torch](https://github.com/pytorch/pytorch) and [transformers](https://github.com/huggingface/transformers).\r\n\r\n\r\nUpdates `torch` from 1.12.1 to 1.13.1\r\n- [Release notes](https://github.com/pytorch/pytorch/releases)\r\n- [Changelog](https://github.com/pytorch/pytorch/blob/main/RELEASE.md)\r\n- [Commits](https://github.com/pytorch/pytorch/compare/v1.12.1...v1.13.1)\r\n\r\nUpdates `transformers` from 4.21.1 to 4.36.0\r\n- [Release notes](https://github.com/huggingface/transformers/releases)\r\n- [Commits](https://github.com/huggingface/transformers/compare/v4.21.1...v4.36.0)\r\n\r\n---\r\nupdated-dependencies:\r\n- dependency-name: torch dependency-type: direct:production dependency-group: pip\r\n- dependency-name: transformers dependency-type: direct:production dependency-group: pip ...\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dependabot/pip/examples/speech_to_speech/asr_bleu/pip-60279dec1a",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bccaac254a19ac29c2d1",
    "number": 5474,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bccbac254a19ac29c2d2",
    "number": 5471,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nIntegrates TFGridNet from ESPNET into fairseq2.\r\n\r\n## Testing\r\n\r\nTo test the updated denoise_and_vad_audio.py locally.\r\n\r\nNote: This script is tested on an .tsv file.\r\n.tsv file contains filename/path for the audio.\r\n\r\nThis PR deals with additional arguments for SeperateSpeech i.e. --config : configuration.yaml file and --pth-model : path for the model .pth file.\r\nHow to Test:\r\nLocally, just run the following:\r\n\r\nTo test master64:\r\npython denoise_and_vad_audio.py --audio-manifest $INPUT_MANIFEST --output-dir $OUTPUT_DIR --denoise --vad\r\n\r\n($INPUT_MANIFEST is path for .tsv file)\r\n\r\nTo test SeperateSpeech:\r\npython denoise_and_vad_audio.py --audio-manifest $INPUT_MANIFEST --output-dir $OUTPUT_DIR --model SeperateSpeech --config /path/to/config.yaml --pth-model /path/to/model.pth --denoise --vad\r\n\r\nIt will generate two output audios: denoise and vad audio files in their respective directory.\r\n",
    "head_branch": "tfgridnet_integration",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcccac254a19ac29c2d3",
    "number": 5468,
    "body": "The [latest pytorch release](https://github.com/pytorch/pytorch/releases/tag/v2.2.2) (2.2.2) requires gcc 9 or higher. This impacts torchaudio, and broke our build. This PR adds an upper bound on the torchaudio version so this doesn't break.",
    "head_branch": "upper-bound-torch-audio",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bccdac254a19ac29c2d4",
    "number": 5465,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "typo",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcceac254a19ac29c2d5",
    "number": 5463,
    "body": "fix imports of metrics.py\r\n\r\n\r\n## What does this PR do?\r\nIt will resolve the imports of metrics.py\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nNo\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @rx28! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235463). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bccfac254a19ac29c2d6",
    "number": 5458,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bigfootjon-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Delete .circleci directory (#5458)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bccfac254a19ac29c2d7",
    "number": 5449,
    "body": "## What does this PR do?\r\nThe script for running fairseq-hydra-train is incorrect and uses the invalid flag -config-dir. Fixed it to the correct flag --config-dir.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes.\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @adithya-gv! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235449). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "i was wondering why my code wasn't working, thanks for pointing this out!!!!",
      "thanks for the clarification, hang tight!",
      "appreciate this clarification, thanks for sharing!",
      "Lifesaver! ",
      "This is amazing! I appreciate the carry"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd0ac254a19ac29c2d8",
    "number": 5432,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n# Description\r\nFixes to the README for the MMS `data_prep`:\r\n* Mention sox install through apt, on top of the Python wrapper\r\n* Fix argument name in example command\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "Thanks for the fixes !"
    ],
    "commit_messages": [
      "MMS alignment README fixes (#5432)\n\n* Mention sox install through apt, on top of the Python wrapper\r\n* Fix argument name in example command"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd1ac254a19ac29c2d9",
    "number": 5428,
    "body": "#5423 issue dataset link is updated\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @Akash-Vijaysingh-Shekhavat! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235428). Thanks!",
      "Update Dataset WMT 16 link"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd2ac254a19ac29c2da",
    "number": 5427,
    "body": "`save-state` and `set-output` commands used in GitHub Actions are deprecated and [GitHub recommends using environment files](https://github.blog/changelog/2023-07-24-github-actions-update-on-save-state-and-set-output-commands/).\n\nThis PR updates the usage of `set-output` to `$GITHUB_OUTPUT`\n\nInstructions for envvar usage from GitHub docs:\n\nhttps://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-an-output-parameter",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @arunsathiya! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235427). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd3ac254a19ac29c2db",
    "number": 5426,
    "body": "bitarray doesnt support util, \r\nutil.rindex() -> index(,right=1)\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @balaramas! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235426). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd4ac254a19ac29c2dc",
    "number": 5425,
    "body": "librosa.filters.mel() function changes in newer version\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @balaramas! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235425). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd4ac254a19ac29c2dd",
    "number": 5407,
    "body": "In the same line of thoughts as the MMS ASR finetuning, I've created a [repo](https://github.com/ylacombe/finetune-hf-vits) that allows MMS TTS finetuning, and updated the MMS README to reflect it!\r\n\r\nYou can try some finetuned models [in this demo](https://huggingface.co/spaces/hf-audio/compare-vits-finetuned) and [in that one](https://huggingface.co/spaces/hf-audio/explore-vits).\r\n\r\n## Did you have fun?\r\nTons of :hugs: ",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Thank you ! "
    ],
    "commit_messages": [
      "Update README.md (#5407)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd5ac254a19ac29c2de",
    "number": 5403,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3371 and extends #3627 to include the ability to return the frame numbers of all non-blank characters of a hypothesis for all wav2letter decoder classes, not only just for `W2lKenLMDecoder`. A method called `get_symbols()` was also added to the parent class for all the decoders (`W2lDecoder`) so that the non-blank characters of the hypothesis can be returned as a list of natural language characters and not just token ids. This helps in finding the word-boundary tokens later when calculating the word-level timestamp information using the following formula:\r\n\r\ntimestamp = frame_num * (audio_len / (num_frames * sample_rate))\r\n\r\nwhere:\r\n\r\n- frame_num = the timestep of the symbol, as returned in the 'timesteps' field of Wl2Decoder.decode() outputs.\r\n- audio_len = the number of samples in the loaded audio file corresponding to the transcript (if using batched w2v2 acoustic model inference, will be zero padded to the length of the longest loaded audio file in the batch).\r\n- num_frames = the number of frames in the emission matrix returned by the w2v2 acoustic model inference for that audio file (if using batched inference, the number of frames for each audio file will be the same as in this case all loaded audio files are padded to the length of the longest audio file in the batch).\r\n- sample_rate = sample rate of loaded audio files (usually 16000 Hz).\r\n\r\n## PR review\r\n@alexeib",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd6ac254a19ac29c2df",
    "number": 5389,
    "body": "# Before submitting\r\n\r\n- [ no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ no need for an update] Did you make sure to update the docs?\r\n- [ no need] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nFixed the metadata on line 1103 the ``help`` key was not in quotes.\r\nI found this through my compiler which kept giving me this error\r\n```  \r\nFile \"/home/disk1/q/main.dist/fairseq/dataclass/configs.py\", line 1103, in EMAConfig\r\n    default=False, metadata={help: \"store exponential moving average shadow model\"}\r\n\r\nNameError: name 'help' is not defined\r\n```\r\nI then opened up the script in VSC and discovered the quotes where forgot so I just added them in and now it works. :)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\nYup!",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Just wondering what the status of this pr. :)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd7ac254a19ac29c2e0",
    "number": 5385,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "xp/incremental_enable",
    "is_a_fork": true,
    "comments": [
      "Hi @18582088138! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235385). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd8ac254a19ac29c2e1",
    "number": 5365,
    "body": "Fixes issue #5366.\r\nFixes deprecated `from fairseq import metrics` changing the import with the new location `from fairseq.logging import metrics` on the `speech_dlm_criterion` used in the `fairseq-train` run on the [SimpleLSTM tutorial](https://fairseq.readthedocs.io/en/latest/tutorial_simple_lstm.html).\r\n",
    "head_branch": "eq/speech-dlm-criterion",
    "is_a_fork": true,
    "comments": [
      "@Efrainq07\r\nI checked your fixed code and looked for same issue with below command on main branch (da8fb63). \r\n`find . -name '*.py' | xargs grep \"import.*metrics\" | less`\r\nAs a result, There is only code where you mentioned.\r\nI'm afraid that I am not a reviewer. I hope this pr is merged.\r\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcd9ac254a19ac29c2e2",
    "number": 5363,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdding new multiresolution HuBERT implementation to fairseq https://arxiv.org/abs/2310.02720\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n# TODOs (in progress)\r\n- [x] Add training configs/preprocessing scripts\r\n- [x] Add documentations\r\n- [x] Upload pre-trained models\r\n\r\n",
    "head_branch": "multires_hubert",
    "is_a_fork": true,
    "comments": [
      "Many thanks for the code review! @annasun28\r\nThe PR is ready to merge on my side. Could you please help merge the PR?"
    ],
    "commit_messages": [
      "Multires hubert (#5363)\n\n* multires hubert core\r\n\r\n* update core codebase on multiresolution hubert\r\n\r\n* add examples\r\n\r\n* adding entries to pretrained models (not finished)\r\n\r\n* add other abalation models\r\n\r\n* add multilinugal\r\n\r\n* add decode.sh train.sh finetune.sh and update links for README.md\r\n\r\n* fix readme\r\n\r\n* clean the codebase\r\n\r\n---------\r\n\r\nCo-authored-by: Anna Sun <13106449+annasun28@users.noreply.github.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdaac254a19ac29c2e3",
    "number": 5362,
    "body": "The ['corpus_key'] in batch['net_input']['corpus_key'] provides false information. Fix this bug in multi_corpus_dataset.py file.",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdbac254a19ac29c2e4",
    "number": 5359,
    "body": "# Before submitting\r\n\r\n- [âˆš] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [âˆš] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [âˆš] Did you make sure to update the docs?\r\n- [âˆš] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5012 (issue).\r\nThis PR updates supports for Python 3.11 and also add the adaptation of hydra by updating the version of hydra to 1.3.2.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nSure!\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Fix a dependency conflicts.\r\nNow anyone use Python 3.11 can install this lib by \r\n```\r\ngit clone https://github.com/pytorch/fairseq\r\ncd fairseq\r\npip install --editable ./\r\n```",
      "Switching the operator from `<` to `>` for `omegaconf` is not a good idea in general if it is not known why it was there in the first place. Anyways, there is still an issue with the dependencies. I get this error when running the preprocess command\r\n```\r\nfairseq-preprocess \\\r\n    --only-source \\\r\n    --trainpref $TEXT/wiki.train.tokens \\\r\n    --validpref $TEXT/wiki.valid.tokens \\\r\n    --testpref $TEXT/wiki.test.tokens \\\r\n    --destdir data-bin/wikitext-103 \\\r\n    --workers 5\r\ncommon - <dataclasses._MISSING_TYPE object at 0x7f881a973490>\r\nTraceback (most recent call last):\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/bin/fairseq-preprocess\", line 5, in <module>\r\n    from fairseq_cli.preprocess import cli_main\r\n  File \"/mnt/c/Users/mohamed/phd/fairseq/fairseq_cli/preprocess.py\", line 18, in <module>\r\n    from fairseq import options, tasks, utils\r\n  File \"/mnt/c/Users/mohamed/phd/fairseq/fairseq/__init__.py\", line 31, in <module>\r\n    hydra_init()\r\n  File \"/mnt/c/Users/mohamed/phd/fairseq/fairseq/dataclass/initialize.py\", line 24, in hydra_init\r\n    cs.store(name=k, node=v)\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/hydra/core/config_store.py\", line 85, in store\r\n    cfg = OmegaConf.structured(node)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/omegaconf.py\", line 125, in structured\r\n    return OmegaConf.create(obj, parent, flags)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/omegaconf.py\", line 178, in create\r\n    return OmegaConf._create_impl(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/omegaconf.py\", line 900, in _create_impl\r\n    format_and_raise(node=None, key=None, value=None, msg=str(e), cause=e)\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/_utils.py\", line 899, in format_and_raise\r\n    _raise(ex, cause)\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/_utils.py\", line 797, in _raise\r\n    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/mohamed/miniconda3/envs/fairseq/lib/python3.11/site-packages/omegaconf/omegaconf.py\", line 896, in _create_impl\r\n    raise ValidationError(\r\nomegaconf.errors.ValidationError: Object of unsupported type: '_MISSING_TYPE'\r\n    full_key: \r\n    object_type=None\r\n  ```",
      "Sorry for I didn't go through a lot of tests. I am pretty new to this lib.\r\n\r\nThis seems an error for omegaconf(any version) which is not compatible for Python 3.11's dataclasses.\r\n\r\nI fix this by change code in `omegaconf/omegaconf.py`\r\n\r\nJust like below:\r\n```py\r\nfrom dataclasses import _MISSING_TYPE\r\n...\r\n...\r\n\r\n\r\n\r\n    @staticmethod\r\n    def _create_impl(  # noqa F811\r\n        obj: Any = _EMPTY_MARKER_, parent: Optional[BaseContainer] = None\r\n    ) -> Union[DictConfig, ListConfig]:\r\n        try:\r\n           ...\r\n           ...\r\n                    if isinstance(obj, type):\r\n                        raise ValidationError(\r\n                            f\"Input class '{obj.__name__}' is not a structured config. \"\r\n                            \"did you forget to decorate it as a dataclass?\"\r\n                        )\r\n\r\n\r\n                    # Add a judgement for missing type\r\n                    elif isinstance(obj, _MISSING_TYPE):\r\n                        return DictConfig(content={}, parent=parent)\r\n\r\n                    else:\r\n                        raise ValidationError(\r\n                            f\"Object of unsupported type: '{type(obj).__name__}'\"\r\n                        )\r\n```\r\n\r\nIt looks like I need to improve compatibility with other libraries first.",
      "excuse me, did this work, at least when in sync with the updates on the base branch?\r\nand, how is this supposed to fix the conflict with fairseq in the first place?",
      "this is background for the omegaconf and hydra-cor pinned requirements:\r\n\r\nhttps://github.com/facebookresearch/fairseq/pull/3722",
      "> Sorry for I didn't go through a lot of tests. I am pretty new to this lib.\r\n> \r\n> This seems an error for omegaconf(any version) which is not compatible for Python 3.11's dataclasses.\r\n> \r\n> I fix this by change code in `omegaconf/omegaconf.py`\r\n> \r\n> Just like below:\r\n> \r\n> ```python\r\n> from dataclasses import _MISSING_TYPE\r\n> ...\r\n> ...\r\n> \r\n> \r\n> \r\n>     @staticmethod\r\n>     def _create_impl(  # noqa F811\r\n>         obj: Any = _EMPTY_MARKER_, parent: Optional[BaseContainer] = None\r\n>     ) -> Union[DictConfig, ListConfig]:\r\n>         try:\r\n>            ...\r\n>            ...\r\n>                     if isinstance(obj, type):\r\n>                         raise ValidationError(\r\n>                             f\"Input class '{obj.__name__}' is not a structured config. \"\r\n>                             \"did you forget to decorate it as a dataclass?\"\r\n>                         )\r\n> \r\n> \r\n>                     # Add a judgement for missing type\r\n>                     elif isinstance(obj, _MISSING_TYPE):\r\n>                         return DictConfig(content={}, parent=parent)\r\n> \r\n>                     else:\r\n>                         raise ValidationError(\r\n>                             f\"Object of unsupported type: '{type(obj).__name__}'\"\r\n>                         )\r\n> ```\r\n> \r\n> It looks like I need to improve compatibility with other libraries first.\r\n\r\ndo this face 'NameError: name \"_EMPTY_MARKER_\" is not defined'ï¼Œ whyï¼Ÿ"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdbac254a19ac29c2e5",
    "number": 5355,
    "body": "incomptabile -> incompatible\r\nalingment -> alignment\r\nenviroments -> environments\r\neditted -> edited",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdcac254a19ac29c2e6",
    "number": 5354,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix `ValueError: not enough values to unpack (expected 2, got 1)`\r\n\r\nThe error occur when I format data as follow `*.tsv`, there's no `nsample` on each line like the code write, so I correct the code to read the data by yield `nsample` equal to None.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @tuanio! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235354). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcddac254a19ac29c2e7",
    "number": 5352,
    "body": "## What does this PR do?\r\nThere is a mismatch between the audio config and the audio checkpoint. Change the `base_audio_only_task.yaml` file from depth=12, prenet_depth=0 to depth=8, prenet_depth=4 will not affect the training but fix the bug for loading the audio checkpoint.\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @ddlBoJack! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235352). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdeac254a19ac29c2e8",
    "number": 5350,
    "body": "Search paths in alphabetical order when creating the manifest file for wav2vec so that the resulting tsv is deterministic.",
    "head_branch": "alphabetical_order",
    "is_a_fork": true,
    "comments": [
      "Hi @qianlivia! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235350). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdfac254a19ac29c2e9",
    "number": 5346,
    "body": "This PR updates references to \"Meta AI\" as \"FAIR\" (or \"FAIR (Fundamental Artificial Intelligence Research)\" depending on the context) according to the new branding guidelines.",
    "head_branch": "meta",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Change Meta AI to FAIR (#5346)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcdfac254a19ac29c2ea",
    "number": 5344,
    "body": "# Before submitting\r\n\r\n- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [yes] Did you make sure to update the docs?\r\n- [yes] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAssuming an epoch consist of N batches. When we load a checkpoint from mid epoch, there is a number indicate the number of iterations n has been passed in the epoch. When restarting the training job, after the batches are loaded, n batches are removed from the beginning of the iterator by FrozenBatchSampler. Then the CounteringIterator would add n iteration count in front of the batch, so that the batch count in the epoch is correct, i.e. N-n+n = N.\r\nHowever, when skip_reminder_batch is set to True, the CounteringIterator would truncate, incorrectly, the batch count from N batches to N-n, causing the epoch to have less number of total batches. As a result, you would see a jump in the fblearner ui on the progress --- not because the iteration counter increase, but rather because the total number of batches decrease.\r\nThis diffs would fix the problem above.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_iterator",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fix iterator when loading from checkpoint (#5344)\n\nCo-authored-by: Junteng Jia <juntengjia@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce0ac254a19ac29c2eb",
    "number": 5332,
    "body": "There is a typo in the README\r\n\r\nFixes #5214 \r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce1ac254a19ac29c2ec",
    "number": 5330,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nIn #5328, we introduced the logic of saving task level checkpoints. However, the task name was naively set to `supernet`. This PR fixes this issue by leveraging task's class name instead.\r\n\r\n## PR review\r\n@cbalioglu \r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "keep_task_level_cp_name_generic",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Keep task level checkpoint key name generic (#5330)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce2ac254a19ac29c2ed",
    "number": 5329,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3064.\r\nFixes #3705.\r\nFixes #1309.\r\n\r\nTLDR; **This PR fixes the bug that duplicates the symbols that were meant to be overwritten in the vocabulary file.** See detailed explanation in this [blog post](https://lydianishimwe.ke/blog/fairseq-overwrite/).\r\n\r\n### Expected behavior:\r\n\r\nA Dictionary object has an `indices` dict and two lists (`symbols` and `counts`). By default, when loading a vocabulary from a file, a Dictionary instance is first created by adding 4 special tokens (`<s>`,  `<pad>`, `</s>` and `<unk>` in that order). Then, all the entries from the file are appended to the Dictionary. If the vocabulary file already has some of the special tokens, their file entry should contain `#fairseq:overwrite`, otherwise a \"duplicate\" error will be raised at runtime. Furthermore, during preprocessing, the saved dictionary should not contain any of the special symbols. \r\n\r\n### Current behavior: \r\n\r\nThe `add_symbol` function is responsible for adding the symbols to the Dictionary. It has an `overwrite` argument that is set to `True` when the corresponding line in the file has `#fairseq:overwrite`.  Rather than testing `if word in self.indices and overwrite`, it is currently testing `if word in self.indices and not overwrite`, which makes it ignore the case where the symbol should actually be overwritten. Hence, the symbol is appended to the `symbols` list, and its index is changed in the `indices` dict. This results in duplicate symbols and incorrect indices. Generally, only the special symbols will be affected. However, because the number of special tokens is set during initialization, it remains correct.\r\n\r\nFor example, a dictionary with 50K tokens that already has `<s>`, `<pad>` , `</s>` and `<unk>` with the `#fairseq:overwrite` tag will end up having 50004 tokens when loaded. This will also propagate to the subsequent model which will have an embedding dimension of 50004 instead of 50K. Also, with `fairseq-preprocess`, the resulting dictionary will skip the first 4 special symbols but will still contain the duplicate ones.\r\n\r\n### Domino effects and backward compatibility: \r\n\r\nBy fixing this bug, dictionary files will be loaded properly. However, this fix might cause problems in pipelines that use existing architectures and pretrained models because of the mismatch in sentencepiece encoding and/or embedding dimension.\r\n\r\nFor the sake of backward compatibility, a `#fairseq:duplicate` flag is introduced to ensure that duplicates are kept in the dictionary just like the bug. When used with `fairseq-preprocess`, the produced dict.txt file will also write `#fairseq:duplicate` next to the same symbols.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes, I did ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce3ac254a19ac29c2ee",
    "number": 5328,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThis PR adds support for the following:\r\n* Saving task state/attributes to the checkpoint\r\n* Repopulating the task state/attributes while loading the checkpoint\r\n* The respective unit test cases have been added to validate the new functionality\r\n\r\nBesides, this PR also removes an unused import.\r\n\r\n## PR review\r\nCC @cbalioglu\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "preserve_task_checkpoint",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "initial revision (#5328)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce4ac254a19ac29c2ef",
    "number": 5326,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "tailr",
    "is_a_fork": true,
    "comments": [
      "Hi @flbbb! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235326). Thanks!",
      "Manipulation error, sorry."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce4ac254a19ac29c2f0",
    "number": 5317,
    "body": "Fix MMS alignment code\r\n\r\nCopied from https://github.com/yaya-sy/fairseq/commit/b70a9fc46669e4ab2d11561b99931668d0a8077e\r\n\r\nFixes https://github.com/facebookresearch/fairseq/issues/5248 and https://github.com/facebookresearch/fairseq/issues/5242 ",
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update align_and_segment.py (#5317)\n\nFix MMS alignment code"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce5ac254a19ac29c2f1",
    "number": 5316,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n\r\n\r\n## What does this PR do?\r\nFixes # (#5313)\r\n[Crashing in Sagmaker as a result of attempting to overwrite wandb config #5313](https://github.com/facebookresearch/fairseq/issues/5313)\r\n\r\nadds check if fairseq is [being run in SM](https://github.com/fdsig/fairseq/blob/241128b906dd6d78a91f9a11df58a5ce00ce81de/fairseq/logging/progress_bar.py#L505) and then [calls check ](https://github.com/fdsig/fairseq/blob/241128b906dd6d78a91f9a11df58a5ce00ce81de/fairseq/logging/progress_bar.py#L515) and does not update config if running in SM\r\n\r\n## PR review\r\n\r\ncc @vklyukin \r\n\r\n## Did you have fun?\r\n\r\nAlways fun to dive into new code and help folks track experiments.\r\n\r\nMake sure you had fun coding ðŸ™ƒ",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce6ac254a19ac29c2f2",
    "number": 5312,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixing the issue #5248. The latest version of nightly torchaudio uses a 'log_probs' method expecting a tensor of 3 dimensions for the inputs and 2 dimensions for the targets. The 'input_lengths' and 'taget_lengths' need to have 2 dimensions too. The actual method 'get_alignments' of the file 'align_and_segment.py' creates tensors that don't have these expected dimensions. This pull request fixes this issue by unsqueezing the tensors in order to fit them to the expected dimensions for the 'log_probs' and 'forced_align' methods.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_mms_torch_audio_log_probs_dimension_error",
    "is_a_fork": true,
    "comments": [
      "The modifications of this pull request was integrated in #5317"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce7ac254a19ac29c2f3",
    "number": 5299,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @penah1350! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235299). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce8ac254a19ac29c2f4",
    "number": 5294,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bce9ac254a19ac29c2f5",
    "number": 5291,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5290.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_adafactor_mean_to_sum",
    "is_a_fork": true,
    "comments": [
      "Hi @isamu-isozaki! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235291). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bceaac254a19ac29c2f6",
    "number": 5288,
    "body": "Currently, there is no information about lexicons for wav2vec 2.0 decoding in the README. I added both a download link for KenLM lexicon (taken from #2502) and the corresponding argument in the command.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests? \r\n\r\n## What does this PR do?\r\nImplements README update discussed in #2502.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nAbsolutely!\r\n",
    "head_branch": "wav2vec_readme_update",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcebac254a19ac29c2f7",
    "number": 5285,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n\r\nCo-authored-by: Bowen Shi <bshi@meta.com>\r\n",
    "head_branch": "hubert_bf16",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add batchnorm option to hubert/wav2vec2 positional convolution layer for hubert bf16 models (#5285)\n\n* add conv_batch_norm for hubert to support bf16\r\n\r\n* linting\r\n\r\nCo-authored-by: Bowen Shi <bshi@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcebac254a19ac29c2f8",
    "number": 5274,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5273.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hotfix/invalid_condition",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcecac254a19ac29c2f9",
    "number": 5267,
    "body": "Fix \"help\" key not being quoted",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcedac254a19ac29c2fa",
    "number": 5251,
    "body": "revise comments",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @shawnthu! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235251). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bceeac254a19ac29c2fb",
    "number": 5240,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nupdates the ASR-BLEU pipeline to be compatible with 3-letter language codes.\r\n\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "lang_key",
    "is_a_fork": true,
    "comments": [
      "Kindly take a look @lpw0 ",
      "LGTM!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcefac254a19ac29c2fc",
    "number": 5237,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThis diff makes cosmetic changes to make RotaryPositionalEmbedding jit-compatible.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "rotary_jit",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Make RotaryPositionalEmbedding jit-compatible (#5237)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf0ac254a19ac29c2fd",
    "number": 5234,
    "body": "## What does this PR do?\r\nspelling mistake\r\n\r\n## Did you have fun?\r\nðŸ™ƒ\r\n",
    "head_branch": "patch-5",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf0ac254a19ac29c2fe",
    "number": 5233,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "Linux",
    "is_a_fork": true,
    "comments": [
      "Hi @chaos130! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235233). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf1ac254a19ac29c2ff",
    "number": 5230,
    "body": "Fixing typo in the download instructions\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @maliky! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235230). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf2ac254a19ac29c300",
    "number": 5228,
    "body": "missing \"\r\n\r\n# Before submitting\r\n\r\n- [ Ã—] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [âˆš ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ âˆš] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @NotIsHusky! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235228). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf3ac254a19ac29c301",
    "number": 5222,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nWith the upcoming major release of Aim (https://github.com/aimhubio/aim) the SDK backwards compatibility will be broken. Setting upper limit for the installed Aim version to prevent failures of fairseq training runs.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/set-aim-upper-limit",
    "is_a_fork": true,
    "comments": [
      "Great great",
      "Hi! Do you know something about SEO? Can I add a link from another website in here? Please I need help. "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf4ac254a19ac29c302",
    "number": 5215,
    "body": "# Before submitting\r\n\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n\r\n## What does this PR do?\r\nFixes #5214 , The link was broken, and it removed the link from the README.md, and there was no substitute link for the model example in the repo.\r\n\r\n## PR review\r\nThis involves fixing the removed link. Thus, no tests were needed. This PR is open to be reviewed/\r\n\r\n\r\n## Did you have fun?\r\nIt was very fun exploring this amazing project and trying to contribute from my end.\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @shubham-attri! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235215). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf5ac254a19ac29c303",
    "number": 5213,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n  Discussed this in a [GitHub issue](https://github.com/pytorch/pytorch/issues/75287#issuecomment-1602330466) of the pytorch repo.\r\n\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n\r\n- [ ] Did you make sure to update the docs?\r\n  Not applicable\r\n\r\n- [ ] Did you write any new necessary tests?\r\n  No. But I've tested `deeplearning/projects/fairseq-py:test_cpu` in Meta's fbcode repo, and this diff does not introduce any new test failures.\r\n\r\n## What does this PR do?\r\n\r\nThe module `SinusoidalPositionalEmbedding` has the problem that its `weights` attribute is not moved to CPU or CUDA when the module is moved.\r\n\r\nRegistering `weights` as a buffer solves the problem.\r\nThis also eliminates the need for the buffer `_float_tensor`, which is used to keep track of whether the module is on CPU or CUDA.\r\n\r\nMaking `weights` a non-persistent buffer means it won't be saved to or loaded from a `state_dict`.\r\n\r\nWith the changes in this diff, the `state_dict` of a `SinusoidalPositionalEmbedding` module should contain neither `weights` or `_float_tensor`.\r\nThis diff ignores them by overriding the `_load_from_state_dict` method of the `SinusoidalPositionalEmbedding` module, instead of duplicating the code in many `upgrade_state_dict` functions.\r\n\r\nTO DISCUSS: Is it OK for me to override `_load_from_state_dict`? It's a private function, but I see people have overriden it in many places, including in fairseq:\r\nhttps://github.com/search?q=super()._load_from_state_dict&type=code\r\nhttps://github.com/search?q=repo%3Afacebookresearch%2Ffairseq%20super()._load_from_state_dict&type=code\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Does this keep backwards compatability?",
      "> Does this keep backwards compatability?\r\n\r\nYes. The buffer `weights` is created upon construction of a `SinusoidalPositionalEmbedding` object; it doesn't need to be loaded from a `state_dict`.\r\nWith the changes in this PR, no matter whether a `state_dict` contains the keys `weights` and `_float_tensor` or not, these keys will be ignored.",
      "BTW In the discussion on the [pytorch issue](https://github.com/pytorch/pytorch/issues/75287#issuecomment-1603408881), they've said it's OK to override the `_load_from_state_dict` method."
    ],
    "commit_messages": [
      "Register `weights` as a non-persistent buffer of `SinusoidalPositionalEmbedding` (#5213)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf6ac254a19ac29c304",
    "number": 5211,
    "body": "This PR adds a link to the blog post about MMS.\r\n\r\ncc @vineelpratap \r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update README.md (#5211)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf6ac254a19ac29c305",
    "number": 5210,
    "body": "# Before submitting\r\n\r\n- [ x ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x ] Did you make sure to update the docs?\r\n- [ x ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @asr-pub! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235210). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf7ac254a19ac29c306",
    "number": 5207,
    "body": "# Before submitting\r\n\r\n- [V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [V] Did you make sure to update the docs?\r\n- [V] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5205.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "androstj-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "add new instructions on how to get manifest *.tsv file (#5207)\n\nCo-authored-by: Andros Tjandra <androstj@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf8ac254a19ac29c307",
    "number": 5202,
    "body": "- Add huggingface spaces link \r\n- Add HF hub link for various models ",
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update MMS README.md (#5202)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcf9ac254a19ac29c308",
    "number": 5200,
    "body": "Bumps [transformers](https://github.com/huggingface/transformers) from 4.21.1 to 4.30.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/huggingface/transformers/releases\">transformers's releases</a>.</em></p>\n<blockquote>\n<h2>v4.30.0: 100k, Agents improvements, Safetensors core dependency, Swiftformer, Autoformer, MobileViTv2, timm-as-a-backbone</h2>\n<h2>100k</h2>\n<p>Transformers has just reached 100k stars on GitHub, and to celebrate we wanted to highlight 100 projects in the vicinity of <code>transformers</code>  and we have decided to create an <a href=\"https://github.com/huggingface/transformers/blob/main/awesome-transformers.md\">awesome-transformers</a> page to do just that.</p>\n<p>We accept PRs to add projects to the list!</p>\n<ul>\n<li>Top 100  by <a href=\"https://github.com/LysandreJik\"><code>@â€‹LysandreJik</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/22912\">#22912</a></li>\n<li>Add LlamaIndex to awesome-transformers.md  by <a href=\"https://github.com/ravi03071991\"><code>@â€‹ravi03071991</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23484\">#23484</a></li>\n<li>add cleanlab to awesome-transformers tools list  by <a href=\"https://github.com/jwmueller\"><code>@â€‹jwmueller</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23440\">#23440</a></li>\n</ul>\n<h2>4-bit quantization and QLoRA</h2>\n<p>By leveraging the <code>bitsandbytes</code> library by <a href=\"https://github.com/TimDettmers\"><code>@â€‹TimDettmers</code></a>, we add 4-bit support to <code>transformers</code> models!</p>\n<ul>\n<li>4-bit QLoRA via bitsandbytes (4-bit base model + LoRA)  by <a href=\"https://github.com/TimDettmers\"><code>@â€‹TimDettmers</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23479\">#23479</a></li>\n</ul>\n<h2>Agents</h2>\n<p>The Agents framework has been improved and continues to be stabilized. Among bug fixes, here are the important new features that were added:</p>\n<ul>\n<li>Local agent capabilities, to load a generative model directly from <code>transformers</code> instead of relying on APIs.</li>\n<li>Prompts are now hosted on the Hub, which means that anyone can fork the prompts and update them with theirs, to let other community contributors re-use them</li>\n<li>We add an <code>AzureOpenAiAgent</code> class to support Azure OpenAI agents.</li>\n</ul>\n<ul>\n<li>Add local agent  by <a href=\"https://github.com/sgugger\"><code>@â€‹sgugger</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23438\">#23438</a></li>\n<li>Enable prompts on the Hub  by <a href=\"https://github.com/sgugger\"><code>@â€‹sgugger</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23662\">#23662</a></li>\n<li>Add AzureOpenAiAgent  by <a href=\"https://github.com/sgugger\"><code>@â€‹sgugger</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/24058\">#24058</a></li>\n</ul>\n<h2>Safetensors</h2>\n<p>The <code>safetensors</code> library is a safe serialization framework for machine learning tensors. It has been audited and will become the default serialization framework for several organizations (Hugging Face, EleutherAI, Stability AI).</p>\n<p>It has now become a core dependency of <code>transformers</code>.</p>\n<ul>\n<li>Making <code>safetensors</code> a core dependency.  by <a href=\"https://github.com/Narsil\"><code>@â€‹Narsil</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/23254\">#23254</a></li>\n</ul>\n<h2>New models</h2>\n<h3>Swiftformer</h3>\n<p>The SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called â€˜SwiftFormerâ€™ is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2Ã— faster compared to MobileViT-v2.</p>\n<ul>\n<li>Add swiftformer  by <a href=\"https://github.com/shehanmunasinghe\"><code>@â€‹shehanmunasinghe</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/22686\">#22686</a></li>\n</ul>\n<h3>Autoformer</h3>\n<p>This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.</p>\n<ul>\n<li>[Time-Series] Autoformer model  by <a href=\"https://github.com/elisim\"><code>@â€‹elisim</code></a> in <a href=\"https://redirect.github.com/huggingface/transformers/issues/21891\">#21891</a></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/huggingface/transformers/commit/fe861e578f50dc9c06de33cd361d2f625017e624\"><code>fe861e5</code></a> [<code>GPT2</code>] Add correct keys on <code>_keys_to_ignore_on_load_unexpected</code> on all chil...</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/b3e27a80578d022301611363b890107244e12354\"><code>b3e27a8</code></a> Update the pin on Accelerate (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24110\">#24110</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/53e1f5cf66d320b9c809f3940c707b6fef435d2d\"><code>53e1f5c</code></a> [<code>Trainer</code>] Correct behavior of <code>_load_best_model</code> for PEFT models (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24103\">#24103</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/17db177714b03103bb94cd71b7dd414bc63bffd5\"><code>17db177</code></a> reset accelerate env variables after each test (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24107\">#24107</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/905892f09027cab690918c7766fea1bb51bcdd26\"><code>905892f</code></a> Release: v4.30.0</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/c3572e6bfba13ce6dc3fedb05cd1a946ea109576\"><code>c3572e6</code></a> Add AzureOpenAiAgent (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24058\">#24058</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/5eb3d3c7023ed0522d3c743ee2e13d896a3aa788\"><code>5eb3d3c</code></a> Up pinned accelerate version (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24089\">#24089</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/d1c039e39864a41f6eb8b770a65f123c40164ea5\"><code>d1c039e</code></a> fix accelerator prepare during eval only mode (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24014\">#24014</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/2c887cf8e0cb1ac96d28361ff3235a77f83c36ee\"><code>2c887cf</code></a> Do not prepare lr scheduler as it as the right number of steps (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24088\">#24088</a>)</li>\n<li><a href=\"https://github.com/huggingface/transformers/commit/12298cb65c7e9d615b749dde935a0b4966f4ae49\"><code>12298cb</code></a> fix executable batch size issue (<a href=\"https://redirect.github.com/huggingface/transformers/issues/24067\">#24067</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/huggingface/transformers/compare/v4.21.1...v4.30.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=transformers&package-manager=pip&previous-version=4.21.1&new-version=4.30.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/facebookresearch/fairseq/network/alerts).\n\n</details>",
    "head_branch": "dependabot/pip/examples/speech_to_speech/asr_bleu/transformers-4.30.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfaac254a19ac29c309",
    "number": 5186,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests? No just docs\r\n\r\nJust a PR to add Transformers' MMS implementation to the docs",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add transformers MMS checkpoints to docs (#5186)\n\n* Add transformers MMS checkpoints to docs\r\n\r\n* Apply suggestions from code review\r\n\r\n* Apply suggestions from code review\r\n\r\n* Update examples/mms/README.md\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Sanchit Gandhi <93869735+sanchit-gandhi@users.noreply.github.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfaac254a19ac29c30a",
    "number": 5176,
    "body": "# Before submitting\r\n\r\n- [V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [V] Did you make sure to update the docs?\r\n- [V] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5174.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "androstj-patch-2",
    "is_a_fork": false,
    "comments": [
      "@androstj Thanks for looking into it, I did the same yesterday and the issue is fixed now."
    ],
    "commit_messages": [
      "fix missing extra args in ConformerLayer (#5176)\n\n* fix missing extra args in ConformerLayer\r\n\r\n* fix extra args issue\r\n\r\n---------\r\n\r\nCo-authored-by: Andros Tjandra <androstj@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfbac254a19ac29c30b",
    "number": 5175,
    "body": "# Before submitting\r\n\r\n- [V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [V] Did you make sure to update the docs?\r\n- [V] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #5174.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "androstj-patch-1",
    "is_a_fork": false,
    "comments": [
      "Wrong PR"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfcac254a19ac29c30c",
    "number": 5168,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mms_tts_chars",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "MMS TTS Romanian char fix + MPS support + full checkpoint (#5168)\n\n* Fix È› filtering in Romanian at inference\r\n\r\n* mps support + full checkpoints (discriminator+optimizer)\r\n\r\n---------\r\n\r\nCo-authored-by: Bowen Shi <bshi@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfdac254a19ac29c30d",
    "number": 5165,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mms_tts_colab",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "MMS TTS Colab Notebook (#5165)\n\n* Add TTS Colab notebook\r\n\r\n\r\n---------\r\n\r\nCo-authored-by: Bowen Shi <bshi@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfeac254a19ac29c30e",
    "number": 5160,
    "body": null,
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "[MMS] Add a tutorial on CC LM decoding for ASR model (#5160)\n\n* Update MMS_ASR_Inference_Colab.ipynb\r\n\r\n* Update mms_infer.py"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcfeac254a19ac29c30f",
    "number": 5157,
    "body": null,
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "[MMS] Create Colab notebook for LID inference (#5157)\n\n* [MMS] Create Colab Notebook for LID task\r\n\r\n* Update README.md\r\n\r\n* Update README.md"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bcffac254a19ac29c310",
    "number": 5151,
    "body": "Added tutorial in Google Colab IPYNB fashion with small modification. Credit to epk2112 https://github.com/epk2112/fairseq_meta_mms_Google_Colab_implementation\r\n\r\n# Before submitting\r\n\r\n- [V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [V] Did you make sure to update the docs?\r\n- [V] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "androstj-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Create MMS_ASR_Inference_Colab.ipynb (#5151)\n\n* Create MMS_ASR_Inference_Colab.ipynb\r\n\r\nAdded tutorial in Google Colab IPYNB fashion with small modification. Credit to epk2112 https://github.com/epk2112/fairseq_meta_mms_Google_Colab_implementation\r\n\r\n* Add readme & ipynb\r\n\r\n* Add readme & ipynb\r\n\r\n* change colab hyperlink\r\n\r\n---------\r\n\r\nCo-authored-by: Andros Tjandra <androstj@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd00ac254a19ac29c311",
    "number": 5149,
    "body": "# Before submitting\r\n\r\n- [V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [V] Did you make sure to update the docs?\r\n- [V] Did you write any new necessary tests? Tested locally.\r\n\r\n## What does this PR do?\r\nFixes #5147.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "androstj-fix-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix wrong input-output ASR input utts order (#5149)\n\nCo-authored-by: Andros Tjandra <androstj@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd01ac254a19ac29c312",
    "number": 5148,
    "body": "## What does this PR do?\r\nBug Fix: Resolve issue with reading UTF-8 vocab files in the project\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nI'm partying at home\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Feature: Implement UTF-8 readfile support in MMS for TTS example (#5148)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd02ac254a19ac29c313",
    "number": 5144,
    "body": "# Before submitting\r\n\r\n- [it's docs] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [y] Did you make sure to update the docs?\r\n- [not appliable] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThe documentation for MMS regarding language identification has an incorrect format for the manifest. Following the current instructions, the code will give the following error:\r\n\r\n```\r\n  File \"infer.py\", line 153, in <module>\r\n    infer_dataset = FileAudioDataset(infer_manifest, sample_rate=task.cfg.sample_rate, max_sample_size=10 ** 10, min_sample_size=1, pad=True, normalize=task.cfg.normalize)\r\n  File \"/home/kirill/anaconda3/envs/nganu/lib/python3.8/site-packages/fairseq/data/audio/raw_audio_dataset.py\", line 274, in __init__\r\n    assert len(items) == 2, line\r\nAssertionError: sample_english_16.wav\r\n```\r\nCorrect format is actually mentioned within the ASR section before the LID. This format has second column, indicating the number of samples in the audio file. The provided PR provides correct example in docs.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mms_lid_docs",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix the MMS doc about LID manifest (#5144)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd02ac254a19ac29c314",
    "number": 5140,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixing issues in MMS TTS.\r\n* Added uromanization (used for ~5 languages out of 11k)\r\n* support CPU inference\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mms_tts_uroman",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "[MMS] TTS text uromanization + cpu inference (#5140)\n\n* mms tts uroman + cpu support for inference\r\n\r\n* remove mps support to accommodate all pytorch versions\r\n\r\n* add explanation to arg\r\n\r\n---------\r\n\r\nCo-authored-by: Bowen Shi <bshi@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd03ac254a19ac29c315",
    "number": 5139,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "deepspeed_merge",
    "is_a_fork": true,
    "comments": [
      "Hi @Csinclair0! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235139). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd04ac254a19ac29c316",
    "number": 5138,
    "body": "Use `forced_align` instead of `force_align`\r\n",
    "head_branch": "vineelpratap-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "MMS: Fix forced alignment API  usage (#5138)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd05ac254a19ac29c317",
    "number": 5137,
    "body": "Add dictionary files for MMS ASR models\r\n\r\n",
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "[MMS] Update README.md (#5137)\n\nAdd dictionary files for MMS ASR models"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd06ac254a19ac29c318",
    "number": 5133,
    "body": "1. Fix the dimension used to concatenate individual emissions for long audio file\r\n2. Also fix a bug where context was being removed along label dimension (instead of time dimension)",
    "head_branch": "vineelpratap-patch-3",
    "is_a_fork": false,
    "comments": [
      "#5133 seems fixed the dimension issue, but now have this issue \r\n  File \"./examples/mms/data_prep/align_and_segment.py\", line 90, in get_alignments\r\n    path, _ = F.force_align(\r\nAttributeError: module 'torchaudio.functional' has no attribute 'force_align'",
      "Please install torchaudio nightly build. See `Step 1` here for more details - https://github.com/facebookresearch/fairseq/tree/main/examples/mms/data_prep. ",
      "Yes, the error message was after step 1.   conda list show: \r\ntorch                        2.1.0.dev20230523+cu118          pypi_0    pypi\r\ntorchaudio                2.1.0.dev20230523+cu118          pypi_0    pypi\r\n"
    ],
    "commit_messages": [
      "[MMS] Fix concatenation of emissions (#5133)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd06ac254a19ac29c319",
    "number": 5120,
    "body": "## What does this PR do?\r\nAdds support for m1/m2 chips for MMS TTS example\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @yrik! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235120). Thanks!",
      "Thanks @yrik ! Could you also add a few extra lines for non-m1/m2 CPU support like the following?\r\n```\r\nif torch.cuda.is_available():\r\n    torch_device = torch.device(\"cuda\")\r\nelif torch.backends.mps.is_available() and torch.backends.mps.is_built():\r\n    torch_device = torch.device(\"mps\")\r\nelse:\r\n    torch_device = torch.device(\"cpu\")\r\n```\r\n",
      "CPU inference is added now. See [this PR](https://github.com/facebookresearch/fairseq/pull/5140). The m1/m2 chip inference was not added to accomodate non-mac pytorch versions."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd07ac254a19ac29c31a",
    "number": 5118,
    "body": "Fix link to fine-tuning",
    "head_branch": "vineelpratap-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update README.md (#5118)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd08ac254a19ac29c31b",
    "number": 5116,
    "body": "Remove repeated argument\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @Hrazhan! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235116). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Hi, this is not a bug. You can pass multiple audio files in the command. "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd09ac254a19ac29c31c",
    "number": 5115,
    "body": null,
    "head_branch": "vineelpratap-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update MMS README (#5115)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0aac254a19ac29c31d",
    "number": 5114,
    "body": "As per title",
    "head_branch": "vineelpratap-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update blog post link for MMS (#5114)\n\n* Update blog post link for MMS\r\n\r\n* Update blog post link for MMS"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0aac254a19ac29c31e",
    "number": 5110,
    "body": "Releasing MMS code \r\n\r\nCo-authors - @androstj @chevalierNoir @michaelauli ",
    "head_branch": "mms_release_public_v0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Mms release (#3948) (#5110)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0bac254a19ac29c31f",
    "number": 5109,
    "body": "The index of symbols in token dict and word dict should be the same for lexicon free decoder to work. This PR fixes it. \r\n\r\nTested on Librispeech using an n-gram character LM. \r\n\r\n\r\nBEFORE: (Use LM weight = 4)\r\n```\r\n | t h e | o l d h e r s | w h o | h u d | m a r e d | m a n d | h l d | n o t n c e | n | n t h e | l t l | o l | o n | p n c e | m a r y | h l | h u n r o u n d | h e r | b r o t h e r s | n e c e | b u t | s e n | t h e v | e | m p e r e | h o w | t h e | p l s r | h e o w | h n d | t o | r e t u r n | h e | h o l | h m a e |\r\n```\r\nAFTER: (Use LM weight = 4)\r\n```\r\n\r\n | t h e | s o l d i e r s | w h o | h a d | c a r e d | f o r | a n d | h a d | n o t i c e d | a n d | t a k e n | t h e | t i t l e | g o l d | y o n | p r i n c e s | m a r y | h a d | h u n g | r o u n d | h e r | b r o t h e r ' s | n e c k | b u t | s e i n g | t h e | f a v o u r | t h e | e m p e r o r | s h o w e d | t h e | p r i s o n e r s | t h e y | n o w | h a s t e n e d | t o | r e t u r n | t h e | h o l y | i m a g e | |\r\n```",
    "head_branch": "lexfree_fix2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fix dict order (#5109)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0cac254a19ac29c320",
    "number": 5103,
    "body": "- [librosa v0.9.0 implements keyword only arguments throughout the package.](https://github.com/librosa/librosa/pull/1431) Calling `librosa.filters.mel()` needs keywords `sr`, `n_fft`, etc. The order of keywords is the same as v0.8.1, so should be compatible with this change. \r\n- `postprocess_results()` converts `attr` into a NumPy array, but `dump_result()` expects it as a Torch tensor.",
    "head_branch": "mel-kwargs",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0dac254a19ac29c321",
    "number": 5100,
    "body": "RT",
    "head_branch": "update-xstorycloze-dataset-description",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "update XStoryCloze dataset description (#5100)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0eac254a19ac29c322",
    "number": 5082,
    "body": "## What does this PR do?\r\n\r\nWhen translating using a model from hub_utils in the production setting, if skip_invalid_size_inputs is set to True, then the output list of translations may be different in length from the input list of strings.\r\n\r\nThese changes enforce empty responses in case of failure instead of no responses which will lead to mismatching the input sentences from the hyp outputs when using batching.\r\n\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "This is a massively helpful PR. I routinely have to re-align my source with my generated output which can be very messy. This is a very easy PR to accept.",
      "Thanks @erip ! Would appreciate if someone from the team can look into it."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0eac254a19ac29c323",
    "number": 5080,
    "body": "The function `_get_mask_indices_dims` used the wrong variable when checking if a length was already cached.  This will increase performance\r\n",
    "head_branch": "patch-4",
    "is_a_fork": true,
    "comments": [
      "Just noticed that it is the same as this one - https://github.com/facebookresearch/fairseq/pull/5018#issue-1618732029 not sure why it was closed"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd0fac254a19ac29c324",
    "number": 5069,
    "body": "DiverseBeamSearch result is noticed to be not diverse.\r\na) In DiverseBeamSearch (search.py), we are trying to enforce distance among different groups and each group contains [2 x group_beam_size](https://github.com/facebookresearch/fairseq/blob/main/fairseq/search.py#L133) candidates (group_beam_size = total [beam_size / num_groups](https://github.com/facebookresearch/fairseq/blob/main/fairseq/search.py#L587)).\r\nb) However, in during sequence generation (sequence_generator.py), we are selecting final[ top beam_size tokens](https://github.com/facebookresearch/fairseq/blob/main/fairseq/sequence_generator.py#L535) among all group all candidates. Basically it is not aware of groups used in DiverseBeamSearch.\r\nc) We iterate a) and b) at each step. This could lead to finally all groups converge to descendent candidates from the same group. This tend to happen as the original first group is the one not receiving any diversity penalty and high in fluency score.\r\n\r\nThis patch includes two changes:\r\n\r\n1. Chooses cumulative diversity. It also offers a way to interpolate between cumulative diversity and Hamming diversity through diversity_discount[^1].\r\n2. Limits search num of candidates as a workaround for candidate selection bug (dropping diversity groups)\r\n\r\nThe additional bookkeeping needed for cumulative diversity is estimated to incur 5% latency overhead.\r\nThis is measured on a BART-base model with batch_size=9 and num_groups=12 on V100.\r\n\r\n[^1]:  Diversity function illustration:\r\nA) I like dogs.\r\nB) I like ____.\r\nC) There are ___.\r\nAssuming each word is a token and we are at step=2, trying to fill in the blank:\r\n**Current/Hamming diversity**:\r\nPenalty for B from A is 1 for \"dogs\" and 0 for any other like \"cats\".\r\nPenalty for C from A is 1 for \"dogs\" and 0 for any other like \"cats\".\r\n**Cumulative diversity**:\r\nPenalty for B from A is 3 for \"dogs\" and 0 for any other like \"cats\".\r\nPenalty for C from A is 1 for \"dogs\" and 0 for any other like \"cats\".\r\nB and C differ because B matches with A for \"I\" and \"like\" at respective steps incurring 2 cumulative penalty.\r\n**Using divesrity_discount to interpolate between these two**:\r\nIf diverstiy_discount = 0.5, then\r\nPenalty for B from A is 1.75 (1 + 0.5 + 0.25) for \"dogs\" and 0 for any other words like \"cats\".\r\nPenalty for C from A is 1 for \"dogs\" and 0 for any other words like \"cats\".\r\n\"I\" and \"like\" matched for B and A at step 0 and 1 respectively. Since \"I\" is two steps away and \"like\" is one step away, they are discounted by (0.5)^2 and 0.5 respectively.\r\nWhen diversity_discount = 0, we recover Hamming diversity and when diversity_discount = 1, we recover cumulative diversity.\r\n\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix DiverseBeamSearch so that no diversity groups will be dropped. (#5069)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd10ac254a19ac29c325",
    "number": 5063,
    "body": null,
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd11ac254a19ac29c326",
    "number": 5062,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\nYes this was, bug issue #5012\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\nYes\r\n- [x] Did you make sure to update the docs?\r\n- There was no need\r\n- [ ] Did you write any new necessary tests?\r\n- Further amendments need to be done to the Hydra library. Working on it at the moment.\r\n\r\n## What does this PR do?\r\nFixes # (5012).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @Medoalmasry! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235062). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Unfortunately, after these changes, fireseq becomes incompatible with hydra. You do not know how to fix this problem? I also have python 3.11.\r\nOn image: caused by an exception in file hydra\\core\\config_store.py\r\n![image](https://github.com/facebookresearch/fairseq/assets/89173189/73dfaabb-c7ca-47f6-a1dc-dd3a5d16c025)",
      "> hydra\r\n\r\nWhich warehouse are you referring to hydra? Is https://pypi.org/project/hydra-core?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd12ac254a19ac29c327",
    "number": 5045,
    "body": "## What does this PR do?\r\nFixes #5307.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n",
    "head_branch": "d2v2_update_readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "add data2vec2 nlp model dictionary (#5045)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd13ac254a19ac29c328",
    "number": 5023,
    "body": "## What does this PR do?\r\n`user_dir: ${env:PWD}/examples/data2vec` is missing in the finetunings configs, without this config the data2vec models are not registered ([here](https://github.com/facebookresearch/fairseq/blob/0338cdc3094ca7d29ff4d36d64791f7b4e4b5e6e/fairseq_cli/train.py#L48)) to `TASK_DATACLASS_REGISTRY` and the model can not be loaded.\r\n\r\n\r\n## Did you have fun?\r\n:)  I did :) very flexible code :-o\r\n\r\nI think it is related to @alexeib \r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd13ac254a19ac29c329",
    "number": 5018,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a minor cache error\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bugfix/_get_mask_indices_dims",
    "is_a_fork": true,
    "comments": [
      "Hi @sadahry! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%235018). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd14ac254a19ac29c32a",
    "number": 5010,
    "body": "- [X] Python <3.8 versions deleted (Deprecated 27 Jun 2023)\r\n- [X] Python 3.10.6 (added as default) (CircleCI images have 3.10.6)\r\n- [X] Pytorch <1.12 delete. Deprecated (https://pytorch.org/blog/deprecation-cuda-python-support/)\r\n- [X] Updated linux-cuda-11:2023.02.1 GPU https://discuss.circleci.com/t/cuda-11-8-gpu-cuda-image-any-plans/47240/3\r\n- [X] New image circleCI Cuda 11.8 and they are working to CUDA 12. https://discuss.circleci.com/t/cuda-11-8-gpu-cuda-image-any-plans/47240/4\r\n- [X] Updated test compatible with CUDA and Pytorch > 1.10\r\n- [X] Unified workflow github and CircleCI python versions: Python 3.10.6\r\n- [X] Added CUDA variable on CircleCI: You now can select CUDA VERSION.\r\n- [X] Ready For Pytorch 2.0 and CUDA 12.0/12.1 CircleCI(CircleCI working on release cuda 12 images at the end of the month)\r\n- [X] Delete pytorch <1.12. Deprecation: https://pytorch.org/blog/deprecation-cuda-python-support/\r\n\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd15ac254a19ac29c32b",
    "number": 5007,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n* Implements specAugment with torch only.\r\n* New feature: Both augmentations (warp and mask) offer a greater parameter flexibility.\r\n* New feature: functional implementation\r\n* New feature: Initialize params based on policies (LibriSpeech, Switchboard)\r\n* New feature: run with batches\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "spec-aug-torch",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd16ac254a19ac29c32c",
    "number": 5006,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #TypeError: mel() takes 0 positional arguments but 5 was given\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd17ac254a19ac29c32d",
    "number": 4987,
    "body": "the fetched items in `__getitem__` method was overwritten rather than being updated based on the passed config.",
    "head_branch": "mohsen/fix_getitem_language_pair_dataset",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd17ac254a19ac29c32e",
    "number": 4981,
    "body": "Adding transcript option for ASR BLEU.\r\n",
    "head_branch": "asr_bleu_transcripts",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add transcript option for asr-bleu (#4981)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd18ac254a19ac29c32f",
    "number": 4973,
    "body": "When --skip-remainder-batch=True, restoring training from a checkpoint may crash. For example, each epoch has 10k steps, training stops and saves at the 8k-th step. When restoring training, initial_offset=8k in the Line 493, len(self.epoch_batch_sampler) will become 2k. Then itr.take(1999) in the Line 528 and self._itr.take(max(1999 - 10k, 0)) in the Line 81, making an empty dataset. The main training loop will crash.",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd19ac254a19ac29c330",
    "number": 4969,
    "body": "Bumps [torch](https://github.com/pytorch/pytorch) from 1.12.1 to 1.13.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/pytorch/pytorch/releases\">torch's releases</a>.</em></p>\n<blockquote>\n<h2>PyTorch 1.13.1 Release, small bug fix release</h2>\n<p>This release is meant to fix the following issues (regressions / silent correctness):</p>\n<ul>\n<li>RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88669\">#88669</a></li>\n<li>Installation via pip  on Amazon Linux 2, regression <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88869\">#88869</a></li>\n<li>Installation using poetry on Mac M1, failure <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88049\">#88049</a></li>\n<li>Missing masked tensor documentation <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89734\">#89734</a></li>\n<li>torch.jit.annotations.parse_type_line is not safe (command injection) <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88868\">#88868</a></li>\n<li>Use the Python frame safely in _pythonCallstack <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88993\">#88993</a></li>\n<li>Double-backward with full_backward_hook causes RuntimeError <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88312\">#88312</a></li>\n<li>Fix logical error in get_default_qat_qconfig <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88876\">#88876</a></li>\n<li>Fix cuda/cpu check on NoneType and unit test <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88854\">#88854</a> and <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88970\">#88970</a></li>\n<li>Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88504\">#88504</a></li>\n<li>Onnx operator_export_type on the new registry <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/87735\">#87735</a></li>\n<li>torchrun AttributeError caused by file_based_local_timer on Windows <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/85427\">#85427</a></li>\n</ul>\n<p>The <a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89855\">release tracker</a> should contain all relevant pull requests related to this release as well as links to related issues</p>\n<h2>PyTorch 1.13: beta versions of functorch and improved support for Appleâ€™s new M1 chips are now available</h2>\n<h1>Pytorch 1.13 Release Notes</h1>\n<ul>\n<li>Highlights</li>\n<li>Backwards Incompatible Changes</li>\n<li>New Features</li>\n<li>Improvements</li>\n<li>Performance</li>\n<li>Documentation</li>\n<li>Developers</li>\n</ul>\n<h1>Highlights</h1>\n<p>We are excited to announce the release of PyTorch 1.13! This includes stable versions of BetterTransformer. We deprecated CUDA 10.2 and 11.3 and completed migration of CUDA 11.6 and 11.7. Beta includes improved support for Apple M1 chips and functorch, a library that offers composable vmap (vectorization) and autodiff transforms, being included in-tree with the PyTorch release. This release is composed of over 3,749 commits and 467 contributors since 1.12.1. We want to sincerely thank our dedicated community for your contributions.</p>\n<p>Summary:</p>\n<ul>\n<li>\n<p>The BetterTransformer feature set supports fastpath execution for common Transformer models during Inference out-of-the-box, without the need to modify the model. Additional improvements include accelerated add+matmul linear algebra kernels for sizes commonly used in Transformer models and Nested Tensors is now enabled by default.</p>\n</li>\n<li>\n<p>Timely deprecating older CUDA versions allows us to proceed with introducing the latest CUDA version as they are introduced by NvidiaÂ®, and hence allows support for C++17 in PyTorch and new NVIDIA Open GPU Kernel Modules.</p>\n</li>\n<li>\n<p>Previously, functorch was released out-of-tree in a separate package. After installing PyTorch, a user will be able to <code>import functorch</code> and use functorch without needing to install another package.</p>\n</li>\n<li>\n<p>PyTorch is offering native builds for AppleÂ® silicon machines that use Apple's new M1 chip as a beta feature, providing improved support across PyTorch's APIs.</p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Stable</th>\n<th>Beta</th>\n<th>Prototype</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><!-- raw HTML omitted --><!-- raw HTML omitted -->Better Transformer<!-- raw HTML omitted --><!-- raw HTML omitted -->CUDA 10.2 and 11.3 CI/CD Deprecation <!-- raw HTML omitted --><!-- raw HTML omitted --></td>\n<td><!-- raw HTML omitted --><!-- raw HTML omitted -->Enable IntelÂ® VTuneâ„¢ Profiler's Instrumentation and Tracing Technology APIs<!-- raw HTML omitted --><!-- raw HTML omitted -->Extend NNC to support channels last and bf16<!-- raw HTML omitted --><!-- raw HTML omitted -->Functorch now in PyTorch Core Library<!-- raw HTML omitted --><!-- raw HTML omitted -->Beta Support for M1 devices<!-- raw HTML omitted --><!-- raw HTML omitted --></td>\n<td><!-- raw HTML omitted --><!-- raw HTML omitted -->ArmÂ® Compute Library backend support for AWS Graviton<!-- raw HTML omitted --><!-- raw HTML omitted --> CUDA Sanitizer<!-- raw HTML omitted --><!-- raw HTML omitted --></td>\n</tr>\n</tbody>\n</table>\n<p>You can check the blogpost that shows the new features <a href=\"https://pytorch.org/blog/PyTorch-1.13-release/\">here</a>.</p>\n<h1>Backwards Incompatible changes</h1>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/pytorch/pytorch/blob/master/RELEASE.md\">torch's changelog</a>.</em></p>\n<blockquote>\n<h1>Releasing PyTorch</h1>\n<!-- raw HTML omitted -->\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#release-compatibility-matrix\">Release Compatibility Matrix</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#general-overview\">General Overview</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#cutting-a-release-branch-preparations\">Cutting a release branch preparations</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#cutting-release-branches\">Cutting release branches</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#pytorchpytorch\"><code>pytorch/pytorch</code></a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#pytorchbuilder--pytorch-domain-libraries\"><code>pytorch/builder</code> / PyTorch domain libraries</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#making-release-branch-specific-changes-for-pytorch\">Making release branch specific changes for PyTorch</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#making-release-branch-specific-changes-for-domain-libraries\">Making release branch specific changes for domain libraries</a></li>\n</ul>\n</li>\n<li><a href=\"#drafting-rcs-release-candidates-for-pytorch-and-domain-libraries\">Drafting RCs (https://github.com/pytorch/pytorch/blob/master/Release Candidates) for PyTorch and domain libraries</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#release-candidate-storage\">Release Candidate Storage</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#release-candidate-health-validation\">Release Candidate health validation</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#cherry-picking-fixes\">Cherry Picking Fixes</a></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#promoting-rcs-to-stable\">Promoting RCs to Stable</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#additional-steps-to-prepare-for-release-day\">Additional Steps to prepare for release day</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#modify-release-matrix\">Modify release matrix</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#open-google-colab-issue\">Open Google Colab issue</a></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#patch-releases\">Patch Releases</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#patch-release-criteria\">Patch Release Criteria</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#patch-release-process\">Patch Release Process</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#triage\">Triage</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#issue-tracker-for-patch-releases\">Issue Tracker for Patch releases</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#building-a-release-schedule--cherry-picking\">Building a release schedule / cherry picking</a></li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#building-binaries--promotion-to-stable\">Building Binaries / Promotion to Stable</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#hardware--software-support-in-binary-build-matrix\">Hardware / Software Support in Binary Build Matrix</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#python\">Python</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#tldr\">TL;DR</a></li>\n</ul>\n</li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#accelerator-software\">Accelerator Software</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#special-support-cases\">Special support cases</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#special-topics\">Special Topics</a>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/blob/master/#updating-submodules-for-a-release\">Updating submodules for a release</a></li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n<h2>Release Compatibility Matrix</h2>\n<p>Following is the Release Compatibility Matrix for PyTorch releases:</p>\n<table>\n<thead>\n<tr>\n<th>PyTorch version</th>\n<th>Python</th>\n<th>Stable CUDA</th>\n<th>Experimental CUDA</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>2.0</td>\n<td>&gt;=3.8, &lt;=3.11</td>\n<td>CUDA 11.7, CUDNN 8.5.0.96</td>\n<td>CUDA 11.8, CUDNN 8.7.0.84</td>\n</tr>\n<tr>\n<td>1.13</td>\n<td>&gt;=3.7, &lt;=3.10</td>\n<td>CUDA 11.6, CUDNN 8.3.2.44</td>\n<td>CUDA 11.7, CUDNN 8.5.0.96</td>\n</tr>\n<tr>\n<td>1.12</td>\n<td>&gt;=3.7, &lt;=3.10</td>\n<td>CUDA 11.3, CUDNN 8.3.2.44</td>\n<td>CUDA 11.6, CUDNN 8.3.2.44</td>\n</tr>\n</tbody>\n</table>\n<h2>General Overview</h2>\n<p>Releasing a new version of PyTorch generally entails 3 major steps:</p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/49444c3e546bf240bed24a101e747422d1f8a0ee\"><code>49444c3</code></a> [BE] Do not package caffe2 in wheel (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/87986\">#87986</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90433\">#90433</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/56de8a39c595777f35e342a7cde9d602d57cca32\"><code>56de8a3</code></a> Add manual cuda deps search logic (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90411\">#90411</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90426\">#90426</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/a4d16e0fb670246f18d8c07396808cd5e3766f0b\"><code>a4d16e0</code></a> Fix ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88504\">#88504</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90104\">#90104</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/80abad3e7460415efe480ab21c1d5c90fc345a27\"><code>80abad3</code></a> Handle Tensor.<strong>deepcopy</strong> via clone(), on IPU (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89129\">#89129</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89999\">#89999</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/73a852acd7946dff8beb818ec723ffa453e7b242\"><code>73a852a</code></a> [Release only change] Fix rocm5.1.1 docker image (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90321\">#90321</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/029ec163f2b3a7c46ccb3e8d8b377c9319db463a\"><code>029ec16</code></a> Add platform markers for linux only extra_install_requires (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88826\">#88826</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89924\">#89924</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/197c5c0b849cfdb4f6844f90c49bb8adba85e1bb\"><code>197c5c0</code></a> Fix cuda/cpu check on NoneType (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88854\">#88854</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90068\">#90068</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/aadbeb7416e20a9be694f1da415626135c5c1097\"><code>aadbeb7</code></a> Make TorchElastic timer importable on Windows (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/88522\">#88522</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/90045\">#90045</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/aa9443306a3ba6e8412e24dd99d17eab3f90e818\"><code>aa94433</code></a> Mark IPU device as not supports_as_strided (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89130\">#89130</a>) (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89998\">#89998</a>)</li>\n<li><a href=\"https://github.com/pytorch/pytorch/commit/59b4f3be3bd073b1243e20284fbd09ff43bc66f5\"><code>59b4f3b</code></a> Use the Python frame safely in _pythonCallstack (<a href=\"https://github-redirect.dependabot.com/pytorch/pytorch/issues/89997\">#89997</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/pytorch/pytorch/compare/v1.12.1...v1.13.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=torch&package-manager=pip&previous-version=1.12.1&new-version=1.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/facebookresearch/fairseq/network/alerts).\n\n</details>",
    "head_branch": "dependabot/pip/examples/speech_to_speech/asr_bleu/torch-1.13.1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1aac254a19ac29c331",
    "number": 4968,
    "body": "## What does this PR do?\r\n\r\nPorting the ust branch speech laser model from speech_matrix to the main branch:\r\nhttps://github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix\r\n\r\nThis is a single existing model based on wav2vec2 models, so it's a quick port to main as everything is already there.\r\n\r\n## Testing:\r\n\r\nUsed the sample code from the speech_matrxi README to load a model and embed an audio file. Compared that the output is the same for the ust branch and the main branch.\r\n",
    "head_branch": "wav2vec_laser",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "wav2vec2_laser (#4968)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1bac254a19ac29c332",
    "number": 4965,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "nllb",
    "is_a_fork": true,
    "comments": [
      "Looking to open-source some additional XSTS materials related to translation and the NLLB project.\r\n\r\nMight need feedback/edits for formatting / citation forming / licensing consistency with the rest of the public NLLB site."
    ],
    "commit_messages": [
      "copied in XSTS open-sourcing files and readme from previous attempt (#4965)\n\nCo-authored-by: Daniel Licht <dlicht@devfair0124.h2.fair>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1bac254a19ac29c333",
    "number": 4964,
    "body": "This PR fixes some minor typos I saw while going through the documentation.\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1cac254a19ac29c334",
    "number": 4958,
    "body": "# Before submitting\r\n\r\n- [ no, this just adds a README ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ yes ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ yes ] Did you make sure to update the docs?\r\n- [ not needed ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdds support and documentation for XLM-V, the latest multilingual model from Meta.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @davisliang! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234958). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1dac254a19ac29c335",
    "number": 4957,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nOpen source the paper: [Generative Spoken Dialogue Language Modeling](https://speechbot.github.io/dgslm/).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dgslm_release_public",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Generative Spoken Dialogue Language Modeling Paper Open Source (#4957)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1eac254a19ac29c336",
    "number": 4951,
    "body": "## What does this PR do?\r\nAt the fine-tuning example for roberta on the glue dataset, the fine-tuning command was missing a `-` before the config-dir (was `-config-dir` instead of `--config-dir`).\r\n\r\nThis caused this error to occur:\r\n`fairseq-hydra-train: error: argument --cfg/-c: invalid choice: 'onfig-dir' (choose from 'job', 'hydra', 'all')` , since fairseq-hydra-train interpret `-config-dir` as the flag `-c`(= `--cfg` from [Hydra docs](https://hydra.cc/docs/advanced/hydra-command-line-flags/#internaldocs-banner))\r\n",
    "head_branch": "fix_roberta_glue_example_typo",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1fac254a19ac29c337",
    "number": 4949,
    "body": "https://app.circleci.com/pipelines/github/fairinternal/fairseq-py/12635/workflows/3befbae2-79c4-458d-9fc4-aad4484183b4/jobs/26767\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update config to fix circleci failure (#4949)\n\nhttps://app.circleci.com/pipelines/github/fairinternal/fairseq-py/12635/workflows/3befbae2-79c4-458d-9fc4-aad4484183b4/jobs/26767"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd1fac254a19ac29c338",
    "number": 4931,
    "body": "The table column name is wrong. I fixed it as in paper\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd20ac254a19ac29c339",
    "number": 4929,
    "body": "Typo in the readme file (python execute line).\r\n\r\nI changed \"python fairseq_cli/hydra-train\" to \"python fairseq_cli/hydra_train.py\"\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @dlwlgus53! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234929). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd21ac254a19ac29c33a",
    "number": 4920,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nProvide transcriptions of mined speech for future research.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd22ac254a19ac29c33b",
    "number": 4918,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4915.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "Fix-for-issue-#4915",
    "is_a_fork": true,
    "comments": [
      "Hi @FayZ676! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234918). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd23ac254a19ac29c33c",
    "number": 4917,
    "body": "Add \"autoregressive\" option to \"fairseq/models/hubert/hubert_asr.py:HubertAsrConfig\" to load model from hubert checkpoint normally.\r\n\r\n```python\r\nautoregressive: bool = field(\r\n    default=False,\r\n    metadata={\r\n        \"help\": \"required for autoregressive decoders (like seq2seq models); \"\r\n        \"adds 'prev_output_tokens' to input and appends eos to target\"\r\n    },\r\n)\r\n```\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes [#4916](https://github.com/facebookresearch/fairseq/issues/4916).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hubert_config_bugfix",
    "is_a_fork": true,
    "comments": [
      "Hi @dohe0342! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234917). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd23ac254a19ac29c33d",
    "number": 4914,
    "body": "## What does this PR do?\r\n\r\nThe ASR-BLEU tool is copied to main branch from ust branch. This is done upon request towards frequent uses (cc @xutaima).\r\n",
    "head_branch": "asr_bleu_in_main",
    "is_a_fork": false,
    "comments": [
      "Thank you so much @uralik ! LGTM!",
      "LGTM. The test failures don't look relevant to the PR. So approving!"
    ],
    "commit_messages": [
      "ASR BLEU tool copied from ust branch into main (#4914)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd24ac254a19ac29c33e",
    "number": 4913,
    "body": "add blog post\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update README.md to add data2vec blog post (#4913)\n\n* Update README.md"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd25ac254a19ac29c33f",
    "number": 4910,
    "body": "\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\noffical -> official\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd26ac254a19ac29c340",
    "number": 4906,
    "body": null,
    "head_branch": "d2v_apex",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "make apex optional (#4906)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd27ac254a19ac29c341",
    "number": 4905,
    "body": "fixes not being able to load older tasks\r\npartially addresses #3799",
    "head_branch": "fix_w2v",
    "is_a_fork": false,
    "comments": [
      "@alexeib, this seems to break some test: https://github.com/facebookresearch/fairseq/actions/runs/3683275218/jobs/6231680434\r\n\r\nthe CI in main are not passing anymore.\r\n\r\nDo you have any idea what's up?",
      "is that failure reliably failing on this commit and reliably passing on 1 commit behind ?\r\n\r\nthe idea here is that if we are loading from a checkpoint, we should not crash if task config definition has changed since then\r\n\r\nnot sure how it would break this test. can also just add a \"build_model\" variation to the task without from_checkpoint arg"
    ],
    "commit_messages": [
      "remove missing config entries when loading task from checkpoint (#4905)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd28ac254a19ac29c342",
    "number": 4904,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\nNo\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\nYes\r\n- [ ] Did you make sure to update the docs?\r\nYes\r\n- [ ] Did you write any new necessary tests?\r\nNo\r\n\r\n## What does this PR do?\r\nThe existing Adam optimizer used by speech models is not efficient as it has many small operators (as shown in the below picture). As the speech team does not use latest PyTorch version Adam optimizer with the multi_tensor (foreach fusion) support, this diff is to support multi_tensor Adam implementation (by fusing many smaller operators into fewer number of operators via PyTorch foreach APIs) for the customized adam_sam optimizer used by the speech team.\r\n![image](https://user-images.githubusercontent.com/11676894/207144331-d61b6e9f-8d4e-4c1d-ac4f-b45f405cec5d.png)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "foreach",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "using foreach to reduce kernel (#4904)\n\n* using foreach to reduce kernel\r\n\r\n* set reproducibility to looser threshold\r\n\r\n* revert optimzer\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\nCo-authored-by: juntengjia <juntengjia@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd28ac254a19ac29c343",
    "number": 4903,
    "body": "This PR contains all changes necessary for training and inference of data2vec 2.0 ",
    "head_branch": "d2v2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "data2vec v2.0 (#4903)\n\ndata2v2c 2.0\r\nCo-authored-by: Arun Babu <arbabu@fb.com>\r\nCo-authored-by: Wei-Ning Hsu <wnhsu@csail.mit.edu>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd29ac254a19ac29c344",
    "number": 4895,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThis PR introduces the source code for the \"Textless Speech Emotion Conversion using Discrete & Decomposed Representations\" paper published in EMNLP 2022.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "emotion_conversion_public",
    "is_a_fork": false,
    "comments": [
      "Could you please provide the pretrained models?"
    ],
    "commit_messages": [
      "Emotion Conversion Paper Open Source (#4895)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2aac254a19ac29c345",
    "number": 4893,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "padentomasello-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update README.md (#4893)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2bac254a19ac29c346",
    "number": 4892,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "padentomasello-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update STOP dataset README to include proper link. (#4892)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2cac254a19ac29c347",
    "number": 4891,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "padentomasello-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add file to generate manifests for stop dataset. (#4891)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2dac254a19ac29c348",
    "number": 4888,
    "body": "Flashlight bindings for beam search decoding now live in https://github.com/flashlight/text#readme, and bindings for Viterbi algorithm implementations and ASG loss live in https://github.com/flashlight/sequence#readme.\r\n\r\nUpdate documentation/logs throughout to point to these new repos, and to remove references to https://github.com/flashlight/flashlight, which no longer contains bindings.\r\n",
    "head_branch": "flashlight_source",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2dac254a19ac29c349",
    "number": 4886,
    "body": "This implements a dummy lock in the pdb module to replace the `multiprocessing.Lock` on platforms where not all features are available to support the multiprocessing module. At the very least, this is the case on AWS Lambda, where importing the `fairseq` package triggers importing `fairseq.pdb`, which fails thus:\r\n\r\n```\r\n[ERROR] OSError: [Errno 38] Function not implemented Traceback (most recent call last):\r\n  File \"/opt/pyenv/versions/3.8.15/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/app/APP_DIRECTORY/__init__.py\", line 1, in <module>\r\n    from .transformer import transformer\r\n  File \"/app/APP_DIRECTORY/transformer/transformer.py\", line 1, in <module>\r\n    from fairseq.models import register_model_architecture\r\n  File \"/app/.venv/lib/python3.8/site-packages/fairseq/__init__.py\", line 39, in <module>\r\n    import fairseq.pdb  # noqa\r\n  File \"/app/.venv/lib/python3.8/site-packages/fairseq/pdb.py\", line 16, in <module>\r\n    _stdin_lock = multiprocessing.Lock()\r\n  File \"/opt/pyenv/versions/3.8.15/lib/python3.8/multiprocessing/context.py\", line 68, in Lock\r\n    return Lock(ctx=self.get_context())\r\n  File \"/opt/pyenv/versions/3.8.15/lib/python3.8/multiprocessing/synchronize.py\", line 162, in __init__\r\n    SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\r\n  File \"/opt/pyenv/versions/3.8.15/lib/python3.8/multiprocessing/synchronize.py\", line 57, in __init__\r\n    sl = self._semlock = _multiprocessing.SemLock(\r\n```\r\n\r\nThe modified code will catch the OSError and supply a fake lock instead. This is safe because in the event `_cmdloop` executes under a dummy lock, the code won't be running in a multiprocessing environment.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-pdb-os-error",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2eac254a19ac29c34a",
    "number": 4879,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nInitially add the Readme to d-GSLM model\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add Generative Spoken Dialogue Language Modeling (#4879)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd2fac254a19ac29c34b",
    "number": 4877,
    "body": "Fixing the error that the variable `skipped` doesn't get updated when skipping samples.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\nWe don't need to update any docs since this is quite a trivial change\r\n\r\n## What does this PR do?\r\nFixes #4876 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd30ac254a19ac29c34c",
    "number": 4868,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nFixes a discrepancy between the `get_activation_fn()` method and the `get_available_activation_fns()` method (enabling \"swish\" and \"relu_squared\" as available). \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-get_available_activation_fns",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd31ac254a19ac29c34d",
    "number": 4861,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a typo.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd32ac254a19ac29c34e",
    "number": 4857,
    "body": "â€¦from becoming a performance bottleneck during distributed training.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4856 \r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd33ac254a19ac29c34f",
    "number": 4855,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nImplemented simple perturbations described in [Rethinking Perturbations in Encoder-Decoders for Fast Training](https://aclanthology.org/2021.naacl-main.460/) for training Transformer encoder-decoders.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simple_perturbations",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd33ac254a19ac29c350",
    "number": 4848,
    "body": "Before this PR `get_model_input` returns CPU tensors for input tokens which prevents working with GPU models. I added logic similar to [speech_to_speech/hub_interface.py](https://github.com/artkorenev/fairseq/blob/ust/fairseq/models/speech_to_speech/hub_interface.py#L67) where the input is put on the device if needed. Similarly I also added putting the model to the device if the device is available.",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add gpu support for speech-to-text for input processing (#4848)\n\nCo-authored-by: Artem Korenev <artk@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd34ac254a19ac29c351",
    "number": 4847,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix",
    "is_a_fork": true,
    "comments": [
      "@alexeib \r\n\r\nI think this should fix the Torchscript compilation error introduced in https://github.com/facebookresearch/fairseq/pull/4818\r\n\r\nmeanwhile is there recommended test to run in addition to the CI tests?",
      "some tests are still failing",
      "> some tests are still failing\r\n\r\n@alexeib ok after some investigation, seems these build failures are introduced by PyTorch 1.13.0 version release. (which is on Oct 28)\r\n\r\nAfter I install pytorch version 1.12.1 locally and rerun the tests by `pytest`, seems the errors from\r\n`tests/test_binaries.py::TestTranslation::test_transformer_pointer_generator` and `tests/test_multihead_attention.py::test_xformers_single_forward_parity` are gone.\r\n\r\nfailures under tests/test_plasma_utils.py::TestPlasmaView seems irrelevant:\r\n```\r\nOSError: Could not connect to socket /tmp/tmphatl4hub\r\n```\r\n\r\nI guess we could proceed with this PR and fix other issues separately?",
      "thanks for investigating @zhxchen17 . Shall we open another issue for the pytorch test breakage  and reference it here?",
      "> thanks for investigating @zhxchen17 . Shall we open another issue for the pytorch test breakage and reference it here?\r\n\r\nsure lemme open an issue.\r\n"
    ],
    "commit_messages": [
      "Fix Torchscript typing in transformer_encoder.py (#4847)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd35ac254a19ac29c352",
    "number": 4842,
    "body": "# Before submitting\r\n\r\nâŒ Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\nâœ… Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\nâŒ Did you make sure to update the docs?\r\nâŒ Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nAccounts for the fewer `ntokens` in `label_smoothed_cross_entropy` (and other related criteria) due to `--prefix-size`.\r\nWithout it, the averaging of the loss is done with `nsentences * args.prefix_size` more tokens, which results in slightly lower values.\r\n",
    "head_branch": "fix-loss-with-prefix-size",
    "is_a_fork": true,
    "comments": [
      "Hi @johntsi! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234842). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd36ac254a19ac29c353",
    "number": 4840,
    "body": "# Before submitting\r\n\r\n- [ x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4838 .\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_4838",
    "is_a_fork": true,
    "comments": [
      "@cbalioglu thanks for having a look! Resolved the conflicts, should be good to go now.",
      "Merged"
    ],
    "commit_messages": [
      "fix imports referencing moved metrics.py file (#4840)\n\n* fix imports referencing moved metrics.py file\r\n\r\n* Make representation computation branchless in TransformerEncoderBase (#4818)\r\n\r\nSummary:\r\nWe want to make the computation branchless here because fairseq code may be\r\nexported and traced for deployment purposes, and tracing mechanisms can\r\nbreak the correctness for a captured program if it's dependent on input data.\r\nIn this diff we try to rewrite the code to remove one branch so that tracer\r\ncan proceed here and preserve the correct semantics of the model.\r\n\r\nTest Plan:\r\nCI\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\n* Fix Torchscript typing in transformer_encoder.py (#4847)\r\n\r\n* Add Generative Spoken Dialogue Language Modeling (#4879)\r\n\r\n* Update deprecated torch.qr in glow.py example (#4685)\r\n\r\ntorch.qr is deprecated for a long time and is being removed by https://github.com/pytorch/pytorch/pull/70989.\r\n\r\nThis PR makes the example compatible with new and old PyTorch versions.\r\n\r\n* Emotion Conversion Paper Open Source (#4895)\r\n\r\n* data2vec v2.0 (#4903)\r\n\r\ndata2v2c 2.0\r\nCo-authored-by: Arun Babu <arbabu@fb.com>\r\nCo-authored-by: Wei-Ning Hsu <wnhsu@csail.mit.edu>\r\n\r\n* remove missing config entries when loading task from checkpoint (#4905)\r\n\r\n* make apex optional (#4906)\r\n\r\n* Add file to generate manifests for stop dataset. (#4891)\r\n\r\n* Update STOP dataset README to include proper link. (#4892)\r\n\r\n* Update README.md (#4893)\r\n\r\n* using foreach to reduce kernel (#4904)\r\n\r\n* using foreach to reduce kernel\r\n\r\n* set reproducibility to looser threshold\r\n\r\n* revert optimzer\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\nCo-authored-by: juntengjia <juntengjia@fb.com>\r\n\r\n* Update README.md to add data2vec blog post (#4913)\r\n\r\n* Update README.md\r\n\r\n* Update config to fix circleci failure (#4949)\r\n\r\nhttps://app.circleci.com/pipelines/github/fairinternal/fairseq-py/12635/workflows/3befbae2-79c4-458d-9fc4-aad4484183b4/jobs/26767\r\n\r\n* Generative Spoken Dialogue Language Modeling Paper Open Source (#4957)\r\n\r\n* wav2vec2_laser (#4968)\r\n\r\n* ASR BLEU tool copied from ust branch into main (#4914)\r\n\r\n* Add transcript option for asr-bleu (#4981)\r\n\r\n---------\r\n\r\nCo-authored-by: zhxchen17 <zhxchen17@outlook.com>\r\nCo-authored-by: zhxchen17 <zhxchen17@fb.com>\r\nCo-authored-by: Nguyen Tu Anh <nguyentuanh208@gmail.com>\r\nCo-authored-by: Sergii Dymchenko <kit1980@gmail.com>\r\nCo-authored-by: Felix Kreuk <felixkreuk@gmail.com>\r\nCo-authored-by: Alexei Baevski <alexei.b@gmail.com>\r\nCo-authored-by: padentomasello <pdtomasello@gmail.com>\r\nCo-authored-by: Junteng Jia <juntengjia@hotmail.com>\r\nCo-authored-by: juntengjia <juntengjia@fb.com>\r\nCo-authored-by: arbabu123 <arbabu@fb.com>\r\nCo-authored-by: dianaml0 <82468439+dianaml0@users.noreply.github.com>\r\nCo-authored-by: Pierre Andrews <mortimer@fb.com>\r\nCo-authored-by: Ilia Kulikov <kulikov@cs.nyu.edu>\r\nCo-authored-by: Xutai Ma <xutaima@gmail.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd37ac254a19ac29c354",
    "number": 4836,
    "body": "Change O to S in documentation of source text.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a Typo in the documentation.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd38ac254a19ac29c355",
    "number": 4831,
    "body": "Rename `ResamplingDataset`'s attribute `batch_by_size` to `_batch_by_size` so that it does not shadow the method `batch_by_size` from the parent class\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @qmeeus! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234831). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd38ac254a19ac29c356",
    "number": 4830,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n- Fixed `src_lengths` in `sequence_generator_multi_decoder.py`\r\n- Fixed `build_multitask_decoder` for XMTransformer and UnitY\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_unity",
    "is_a_fork": false,
    "comments": [
      "Wondering what are the plans to update the ust branch and seamless_main internally? cc @vedanuj "
    ],
    "commit_messages": [
      "Fix UnitY (#4830)\n\n* Fix build_multitask_decoder\r\n\r\n* Fix src_lengths in UnitY decoding"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd39ac254a19ac29c357",
    "number": 4829,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nRemoved redundant build_embedding() by using TransformerModelBase.build_embedding().\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "unify_build_emb",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3aac254a19ac29c358",
    "number": 4828,
    "body": "1. Update of hub_interface.py: Add add_if_not_exist parameter in the encode function of hub_interface.py of BART model, because otherwise there are mismatch issues.\r\n\r\n2. Update denoising task: Fix mistakes in the default values, type, and explanation of the parameters permute_sentences, shuffle_instance, and replace_length\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # Solve the mismatch issues of encoder embeddings during inference. Also update the parameters of the denoising task, because otherwise, the sentence permutation perturbation scheme doesn't work correctly by using the default value. Moreover, fix the misleading default values and descriptions of some parameters.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @iakovos777! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234828). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3bac254a19ac29c359",
    "number": 4821,
    "body": null,
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3cac254a19ac29c35a",
    "number": 4820,
    "body": "S3 link for dataset downloading is provided.",
    "head_branch": "xglm_xtorycloze_open_source",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "XGLM paper camera-ready: add XStoryCloze data opensource (#4820)\n\n* add XStoryCloze data\r\n\r\n* upload XStoryCloze dataset files to s3 instead of git\r\n\r\n* minor fixes\r\n\r\n* minor fixes\r\n\r\n* minor fixes\r\n\r\n* minor fixes\r\n\r\n* fix broken dataset doc link"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3dac254a19ac29c35b",
    "number": 4818,
    "body": "Summary:\r\nWe want to make the computation branchless here because fairseq code may be exported and traced for deployment purposes, and tracing mechanisms can break the correctness for a captured program if it's dependent on input data. In this diff we try to rewrite the code to remove one branch so that tracer can proceed here and preserve the correct semantics of the model.\r\n\r\nTest Plan:\r\nCI\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "cc @suo",
      "@dianaml0 do you mind taking a look at this?",
      "cc @alexeib ",
      "thanks!",
      "the tests are failing now because of this change:\r\n\r\nVariable 'has_pads' is annotated with type Tensor but is being assigned to a value of type bool:\r\n  File \"/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/fairseq/models/transformer/transformer_encoder.py\", line 205\r\n        # compute padding mask\r\n        encoder_padding_mask = src_tokens.eq(self.padding_idx)\r\n        has_pads: Tensor = (\r\n        ~~~~~~~~ <--- HERE\r\n            torch.tensor(src_tokens.device.type == \"xla\") or encoder_padding_mask.any()\r\n        )\r\n'TransformerEncoderBase.forward_scriptable' is being compiled since it was called from 'TransformerEncoderBase.forward'\r\n  File \"/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/fairseq/models/transformer/transformer_encoder.py\", line 165\r\n                  Only populated if *return_all_hiddens* is True.\r\n        \"\"\"\r\n        return self.forward_scriptable(\r\n               ~~~~~~~~~~~~~~~~~~~~~~~~\r\n            src_tokens, src_lengths, return_all_hiddens, token_embeddings\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE",
      "> the tests are failing now because of this change:\r\n> \r\n> Variable 'has_pads' is annotated with type Tensor but is being assigned to a value of type bool: File \"/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/fairseq/models/transformer/transformer_encoder.py\", line 205 # compute padding mask encoder_padding_mask = src_tokens.eq(self.padding_idx) has_pads: Tensor = ( ~~~~~~~~ <--- HERE torch.tensor(src_tokens.device.type == \"xla\") or encoder_padding_mask.any() ) 'TransformerEncoderBase.forward_scriptable' is being compiled since it was called from 'TransformerEncoderBase.forward' File \"/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/fairseq/models/transformer/transformer_encoder.py\", line 165 Only populated if _return_all_hiddens_ is True. \"\"\" return self.forward_scriptable( ~~~~~~~~~~~~~~~~~~~~~~~~ src_tokens, src_lengths, return_all_hiddens, token_embeddings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n@alexeib I'm looking into this issue, and I'll try to send in a fix real quick."
    ],
    "commit_messages": [
      "Make representation computation branchless in TransformerEncoderBase (#4818)\n\nSummary:\r\nWe want to make the computation branchless here because fairseq code may be\r\nexported and traced for deployment purposes, and tracing mechanisms can\r\nbreak the correctness for a captured program if it's dependent on input data.\r\nIn this diff we try to rewrite the code to remove one branch so that tracer\r\ncan proceed here and preserve the correct semantics of the model.\r\n\r\nTest Plan:\r\nCI\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3eac254a19ac29c35c",
    "number": 4814,
    "body": "Fixes https://github.com/facebookresearch/fairseq/issues/4813",
    "head_branch": "fix-remainder-batch",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3eac254a19ac29c35d",
    "number": 4811,
    "body": "It ain't much, but it's honest work ",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd3fac254a19ac29c35e",
    "number": 4810,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "I tried to load and use LID model with fairseq but have an error:\r\n\r\nAssertionError: Could not infer task type from {'_name': 'audio_classification', 'data': '/fsx/data/VoxLingua107/manifest/', 'labels': 'label', 'sample_rate': 16000, 'normalize': True, 'enable_padding': False, 'max_sample_size': 160000, 'min_sample_size': 16000, 'multiple_train_files': False, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.1, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'tpu': False}. Available argparse tasks: dict_keys(['audio_pretraining', 'audio_finetuning', 'cross_lingual_lm', 'denoising', 'speech_to_text', 'text_to_speech', 'frm_text_to_speech', 'hubert_pretraining', 'language_modeling', 'legacy_masked_lm', 'masked_lm', 'multilingual_denoising', 'multilingual_language_modeling', 'multilingual_masked_lm', 'speech_unit_modeling', 'translation', 'multilingual_translation', 'online_backtranslation', 'semisupervised_translation', 'sentence_prediction', 'sentence_prediction_adapters', 'sentence_ranking', 'simul_speech_to_text', 'simul_text_to_text', 'speech_to_speech', 'translation_from_pretrained_bart', 'translation_from_pretrained_xlm', 'translation_lev', 'translation_multi_simple_epoch', 'dummy_lm', 'dummy_masked_lm', 'dummy_mt']). Available hydra tasks: dict_keys(['audio_pretraining', 'audio_finetuning', 'hubert_pretraining', 'language_modeling', 'masked_lm', 'multilingual_language_modeling', 'speech_unit_modeling', 'translation', 'sentence_prediction', 'sentence_prediction_adapters', 'simul_text_to_text', 'translation_from_pretrained_xlm', 'translation_lev', 'dummy_lm', 'dummy_masked_lm'])",
      "Given that I don't have access to '/fsx/data/VoxLingua107/manifest/', I tried to load the model with this code:\r\n\r\n`\r\nstate = checkpoint_utils.load_checkpoint_to_cpu(model_path)\r\n`\r\n`\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\r\n    [model_path],\r\n    state=state\r\n)\r\n`",
      "loading [XLS-R 300M + ft Voxlingua107](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_300m_voxlingua107_ft.pt)\r\n,and no register model is \"wav2vec_classification\"?\r\n`{'_name': 'wav2vec_classification', 'w2v_path': '/fsx/data/model_versions/100pct_300m_params_1m_alpha5_voxlingua/checkpoint_last.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'checkpoint_activations': False, 'offload_activations': False, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_channel_length': 64, 'mask_channel_prob': 0.1, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.1, 'normalize': True, 'data': '/fsx/data/VoxLingua107/manifest/', 'w2v_args': None, 'freeze_encoder': False, 'mask_min_space': 1, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'latent_embed_dim': 1024, 'pooling': 'mean_fast', 'activation_fn': 'linear'}\r\n`\r\n\r\n\r\n\r\n\r\n"
    ],
    "commit_messages": [
      "Add LID generate & eval script (#4810)\n\nCo-authored-by: Andros Tjandra <androstj@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd40ac254a19ac29c35f",
    "number": 4809,
    "body": "Add LID inference documentation.\r\n\r\n# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nJust adding documentation for XLSR LID inference.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update README.md (#4809)\n\nAdd LID inference documentation."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd41ac254a19ac29c360",
    "number": 4808,
    "body": "this will revert #4708 and #4805 and the issue would be fixed upstream, in pytorch.",
    "head_branch": "revert_sinusoidal_positional_embedding_changes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Revert changes to sinusoidal_positional_embedding.py (#4808)\n\n* Revert #4805 and #4708"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd42ac254a19ac29c361",
    "number": 4807,
    "body": "Same as https://github.com/fairinternal/fairseq-py/pull/3682",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #4807 from cndn/ust\n\n[EZ] Add es/fr vocoder"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd43ac254a19ac29c362",
    "number": 4806,
    "body": "Same as https://github.com/fairinternal/fairseq-py/pull/3682",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd43ac254a19ac29c363",
    "number": 4805,
    "body": "after changes in #4708, the affected params are not tensor, hence we can't use view(.).",
    "head_branch": "fix_4708",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fix #4708 (#4805)\n\nCo-authored-by: moslehpour <moslehpour@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd44ac254a19ac29c364",
    "number": 4804,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "diana_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update release.yml (#4804)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd45ac254a19ac29c365",
    "number": 4803,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd46ac254a19ac29c366",
    "number": 4802,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix GLIBC 2.14 not found error (#4802)\n\n* Fix GLIBC 2.14 not found error\r\n\r\n* Update release.yml"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd47ac254a19ac29c367",
    "number": 4801,
    "body": "# Before submitting\r\n\r\n- [ ] (No, it's a minor edit) Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThe `azureml-logging` option is only working during the training loop, here: https://github.com/facebookresearch/fairseq/blob/ad3bec5a07962a994ac25ad1609ab926b13e0c0f/fairseq_cli/train.py#L298\r\n\r\nThis option does not work during the validation loop, and metrics don't appear in AzureML, because `progress_bar` doesn't get the `azureml_logging` argument:\r\nhttps://github.com/facebookresearch/fairseq/blob/ad3bec5a07962a994ac25ad1609ab926b13e0c0f/fairseq_cli/train.py#L491\r\n\r\nThis PR fixes that issue. I tested the patch on AzureML.\r\n\r\n## Did you have fun?\r\nYes!\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd48ac254a19ac29c368",
    "number": 4800,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nEven though `skip_invalid_size_inputs_valid_test` was set to true, validation data samples with a length exceeding max_tokens are being taken. In fact, the entire validation data was taken into account, as opposed to skip the longer length samples. This PR will resolve the issue. It will choose just those samples whose length is shorter than the 'max_tokens' value.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @shuvohishab! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234800). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd48ac254a19ac29c369",
    "number": 4798,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Lower numpy version requirement (#4798)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd49ac254a19ac29c36a",
    "number": 4797,
    "body": "Reverts facebookresearch/fairseq#4796",
    "head_branch": "revert-4796-dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Revert \"Update release.yml (#4796)\" (#4797)\n\nThis reverts commit e40c4352663ce040ecd980e7ca4912dc85cb1dce."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4aac254a19ac29c36b",
    "number": 4796,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update release.yml (#4796)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4bac254a19ac29c36c",
    "number": 4795,
    "body": "Reverts facebookresearch/fairseq#4793",
    "head_branch": "revert-4793-dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Revert \"Update release.yml (#4793)\" (#4795)\n\nThis reverts commit 8adff217f0faa8b187cdef4e609e4fc05fb02cd5."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4cac254a19ac29c36d",
    "number": 4794,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Lower required numpy (#4794)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4cac254a19ac29c36e",
    "number": 4793,
    "body": "Add print statement, add cpython versions\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update release.yml (#4793)\n\nAdd print statement, add cpython versions"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4dac254a19ac29c36f",
    "number": 4792,
    "body": "# Before submitting\r\n\r\nIncompatible with new requirements. \r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove cpython support as well (#4792)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4eac254a19ac29c370",
    "number": 4791,
    "body": "# Before submitting\r\n\r\nIncompatible with current requirements in setup.py. \r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-4",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove support for cpython 3.6 (#4791)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd4fac254a19ac29c371",
    "number": 4790,
    "body": "# Before submitting\r\n\r\nError: `ModuleNotFoundError: No module named 'torch'`\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-4",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix error in workflow (#4790)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd50ac254a19ac29c372",
    "number": 4787,
    "body": "Same as https://github.com/fairinternal/fairseq-py/pull/3672",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [
      "Could you help merge this? Thx!"
    ],
    "commit_messages": [
      "support tensor input in s2s hub interface (#4787)\n\n* model update\r\n\r\n* hub fixes\r\n\r\n* support tensor input in s2s hub interface\r\n\r\n* format"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd50ac254a19ac29c373",
    "number": 4784,
    "body": "fix a bug in PR#4708: `bspair` needs to be a tensor type",
    "head_branch": "fix_bug_PR4708",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd51ac254a19ac29c374",
    "number": 4783,
    "body": "Tests passed: P537894777 mimics HF audio-to-audio pipeline",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Hub fixes for HF integration  (#4783)\n\n* model update\r\n\r\n* hub fixes"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd52ac254a19ac29c375",
    "number": 4781,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4780.\r\n\r\nFix broken links for wav2vec2 large conformer weights files.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-readme-links-for-wav2vec-conformer",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd53ac254a19ac29c376",
    "number": 4779,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_broken_tests",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fix dynamicconv test (#4779)\n\nfix broken tests after merging #4775"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd54ac254a19ac29c377",
    "number": 4778,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nI found the preprocessing script for MuST-C with `--use-audio-input` is really slow. Many processes are based on a massive for-loop, without any kind of parallelization. So, I decided to improve this script in different points of the code. Some of these also improve, indirectly, other speech-to-text preprocessing scripts.\r\n\r\nHere, I list the improvements:\r\n\r\n### YAML Loader\r\n\r\nI changed the loader from `BaseLoader` to `CBaseLoader`, which reduces from 1:30 minutes to less than 10 seconds.\r\n\r\nThis improvement also applies the MuST-C preprocessing without `--use-audio-input`.\r\n\r\n\r\n### Audio file conversion + saving to FLAC\r\n\r\nThis is by far the longest process in the script. After 1 hour, it just converted the 5% of the files in the en-de train split. After parallelizing it, using 16 CPUs, in 2 hours I converted the whole en-de split.\r\n\r\n\r\n### Zip file creation\r\n\r\nZipping the converted audio files also takes a long time with the current code (>20 minutes). After parallelizing it, using 16 CPUs, it can be done in around 3 minutes.\r\n\r\nThis improves the `create_zip` from `data_utils.py`, and hence it also optimizes all the preprocessing scripts using it.\r\n\r\n\r\n### Zip manifest\r\n\r\nThe current `get_zip_manifest` function takes around 8 minutes to execute. After parallelizing it, using 16 CPUs, it runs in less than 1 minute.\r\n\r\nThis improves the `get_zip_manifest` from `data_utils.py`, and hence it also optimizes all the preprocessing scripts using it.\r\n\r\n\r\n### TSV manifest generation\r\n\r\nParallelizing this process with 16 CPUs, the execution time is reduced from around 7 minutes to 1-2 minutes.\r\n\r\nThis improvement also applies the MuST-C preprocessing without `--use-audio-input`.\r\n",
    "head_branch": "prep-mustc-multiprocessing",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd55ac254a19ac29c378",
    "number": 4777,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_dynamicconv",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd56ac254a19ac29c379",
    "number": 4776,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix the feat dims issue for \"fbank80_w_utt_cmvn\" input and Tensor as the audio type. Issue is found during building hf demos for S2UT and UnitY Hok models:\r\nFor example, the input audio is torch.Size([1, 115200]), and then convert the feat shape from (718, 80) to (1, 718, 80)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_feat_dim",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Expand the feat dims for fbank80_w_utt_cmvn input type (#4776)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd57ac254a19ac29c37a",
    "number": 4775,
    "body": "This PR will make lightconv model scriptable.",
    "head_branch": "lightconv_scripting",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "make lightconv scriptable (#4775)\n\nmake lightconv scriptable"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd57ac254a19ac29c37b",
    "number": 4774,
    "body": "Same as fairinternal #3660. ",
    "head_branch": "ust",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "model update (#4774)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd58ac254a19ac29c37c",
    "number": 4773,
    "body": "This PR will make MultiheadAttention module scripable.",
    "head_branch": "multiheadattention_scripting",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "make Multihead_attention scriptable (#4773)\n\nCo-authored-by: moslehpour <moslehpour@meta.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd59ac254a19ac29c37d",
    "number": 4772,
    "body": "This PR will make a scriptable version of DynamicConv class used in lightconv model. ",
    "head_branch": "dynamicconv_scripting",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "make a scriptable dynamicconv (#4772)\n\nMake dynamicconv scriptable"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5aac254a19ac29c37e",
    "number": 4771,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "knowledge_distillation",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5bac254a19ac29c37f",
    "number": 4768,
    "body": "TSIA\r\n\r\nVerified that all the 4 models and the new vocoder model added work as expected. \r\n\r\n- S2UT models - \"xm_transformer_s2ut_hk-en\" ,\"xm_transformer_s2ut_en-hk\"\r\n- UnitY models - \"xm_transformer_unity_hk-en\", \"xm_transformer_unity_en-hk\"\r\n- Hk vocoder - unit_hifigan_HK_layer12.km2500_frame_TAT-TTS \r\n\r\nThe new model path will look like pytorch/fairseq:ust:unit_hifigan_HK_layer12.km2500_frame_TAT-TTS instead of pytorch/fairseq:unit_hifigan_HK_layer12.km2500_frame_TAT-TTS for the new models we add. he previous models will be left unchanged.",
    "head_branch": "migrate_to_ust",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Migrate hub loading for tts models in the config to be based on ust branch (#4768)\n\n* verify using ust branch\r\n\r\n* verify using ust branch"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5bac254a19ac29c380",
    "number": 4767,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n- Fixed calculation of input statistics. The default `--gcmvn-max-num` was too small. It didn't use the whole training samples to calculate the statistics.\r\n- Supported waveform inputs + global cmvn.\r\n- Fixed data filtering for dev and test sets. Previously, both dev and test sets were also filtered based on the input lengths (max=3000, min=5).\r\n- Added a `src_text` column to manifests, which would be helpful for joint ASR+ST training.\r\n- Support waveform segmentation, which is used for SimulEval, for streaming ASR\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_mustc",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5cac254a19ac29c381",
    "number": 4764,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4763.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "xmt_mtl2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add a missing method to XMTransformer (#4764)\n\n* Add build_multitask_decoder()\r\n\r\n* Fix import"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5dac254a19ac29c382",
    "number": 4763,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nSupported multi-task learning with aux task for XMTransformer.\r\nSo far, XMTransormer hasn't introduced the function because the model is pre-trained.\r\nTo do multi-task learning with XMTransormer based on this update, it is required to set `--criteiron` to `speech_to_unit` rather than `label_smoothed_cross_entropy`.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "xmt_mtl",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Support multitask for XMTransformer (#4763)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5eac254a19ac29c383",
    "number": 4762,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes issue  ([#3765](https://github.com/facebookresearch/fairseq/issues/3765)).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "pr_change_token_block_size_dtype",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd5fac254a19ac29c384",
    "number": 4757,
    "body": "## What does this PR do?\r\nDon't install Apex and Meagtron in GPU CI.\r\n\r\nThe GPU CI is spending most of its time compiling megatron and apex and they copying binaries from/to the cache.\r\nThen it only run some basic tests: `pytest tests/gpu/test_binaries_gpu.py` \r\nBy cutting out those the CI time went from 1 hour to 10 minutes.\r\n\r\nhttps://app.circleci.com/pipelines/github/facebookresearch/fairseq\r\n",
    "head_branch": "run_tests",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "drop apex/megatron in CI (not used) (#4757)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd60ac254a19ac29c385",
    "number": 4756,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs? NA\r\n- [ ] Did you write any new necessary tests? NA\r\n\r\n## What does this PR do?\r\nReplace `dict` constructor with `dict` comprehension\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "include-dict-comprehension",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd61ac254a19ac29c386",
    "number": 4752,
    "body": "# Before submitting\r\n\r\n- [*] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [*] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [*] Did you make sure to update the docs?\r\n- [*] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nImplemented \"Knowledge Distillation with Translation\" and \"Factorized Embeddings\"\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd61ac254a19ac29c387",
    "number": 4751,
    "body": "Speech to text dataset provides its own src_lengths field as part of the `net_input`, because it uses a different way to pad input audio samples (!= dict.pad) and also does not contain eos token.\r\n\r\nBecause of this the existing code was computing src_lengths incorrectly: it set every seq length as the length of the minibatch i.e. max length. This was not affecting the model's generations, because src_lengths was used only if `match_src_length` set to true which we didn't do in the case of audio input.\r\n\r\nUnpredictable behavior arise when we decide to use src_lengths for smth else during generation. This fix addresses the issue by re-using the src_lengths from net_input if possible in that if-else branch.\r\n\r\nUnrelated to this fix, probably it is better to use 'features' or 'source' as the input field in speech to text dataset, because in this case the src_lengths will be recomputed correctly using the padding mask.",
    "head_branch": "speech_src_lengths_generator",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "use src_lengths from net_input if possible if src_tokens in the input (#4751)\n\n* use src_lengths from net_input if possible\r\n\r\n* lint+black"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd62ac254a19ac29c388",
    "number": 4747,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nMoved sub-modules used for S2ST models to separate files. I also made minor fixes specific to speech-to-text/speech translation tasks.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "s2st_refactor",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Refactor S2ST (#4747)\n\n* Move S2ST submodules to separate files\r\n\r\n* Refactoring\r\n\r\n* Fix import"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd63ac254a19ac29c389",
    "number": 4733,
    "body": "## What does this PR do?\r\n\r\nCurrently only tests files with a `if __name__ == '__main__'` will be run.\r\nWe want to run all tests in tests/\r\n\r\nExample: https://github.com/facebookresearch/fairseq/blob/279796224f7c1e89d1c431a8a3d223b471b36bf9/tests/test_binarizer.py#L121-L123\r\n\r\nSince Fairseq installation was broken #4726 but the CI was green I also realize the CI was relying too much on `pip install --editable` and was making it hard to reproduce some of the bugs users would see.\r\nI propose to run CI with a true installation.\r\n\r\nThis also lead me to find other bugs in setup.py\r\nFairseq needs pytorch to correctly installed it's binaries, so pytorch should  be installed **before** we look at setup.py.\r\nSo I promoted pytorch to a build requirements.\r\nI also bumped the required version of numpy and pytorch so that we fetch version compatible with python 3.8.\r\n",
    "head_branch": "run_tests",
    "is_a_fork": false,
    "comments": [
      "Note that using this PR I managed to reproduce #4429 in CI: https://github.com/facebookresearch/fairseq/actions/runs/3093135329/jobs/5005173667\r\n\r\n```\r\n2022-09-20 20:14:41 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/__init__.py\", line 33, in <module>\r\n    import fairseq.criterions  # noqa\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/criterions/__init__.py\", line 36, in <module>\r\n    importlib.import_module(\"fairseq.criterions.\" + file_name)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/criterions/ctc.py\", line 21, in <module>\r\n    from fairseq.tasks import FairseqTask\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/tasks/__init__.py\", line 136, in <module>\r\n    import_tasks(tasks_dir, \"fairseq.tasks\")\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/tasks/__init__.py\", line 117, in import_tasks\r\n    importlib.import_module(namespace + \".\" + task_name)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/tasks/online_backtranslation.py\", line 34, in <module>\r\n    from fairseq.sequence_generator import SequenceGenerator\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/sequence_generator.py\", line 16, in <module>\r\n    from fairseq.models import FairseqIncrementalDecoder\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/__init__.py\", line 235, in <module>\r\n    import_models(models_dir, \"fairseq.models\")\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/__init__.py\", line 217, in import_models\r\n    importlib.import_module(namespace + \".\" + model_name)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/speech_to_speech/__init__.py\", line 7, in <module>\r\n    from .s2s_conformer import *  # noqa\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/speech_to_speech/s2s_conformer.py\", line 13, in <module>\r\n    from fairseq.models.speech_to_speech.s2s_transformer import S2UTTransformerModel\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/speech_to_speech/s2s_transformer.py\", line 22, in <module>\r\n    from fairseq.models.speech_to_text import S2TTransformerEncoder\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/speech_to_text/__init__.py\", line 7, in <module>\r\n    from .convtransformer import *  # noqa\r\n  File \"/opt/hostedtoolcache/Python/3.9.13/x64/lib/python3.9/site-packages/fairseq/models/speech_to_text/convtransformer.py\", line 23, in <module>\r\n    from fairseq.models.speech_to_text.modules.convolution import infer_conv_output_dim\r\nModuleNotFoundError: No module named 'fairseq.models.speech_to_text.modules'\r\nError: Process completed with exit code 1.\r\n```\r\n\r\nIt only happens when using `pip install fairseq` and not `pip install -e .` and when using another directory so that fairseq isn't part of the path.\r\nI hope this new CI will catch more installation bugs upstream.",
      "I've modified the CI to run on a more recent pytorch release: 1.10.\r\nI also wanted to make it run on 1.12, but the Apex version hardcoded in the config doesn't work there, I'll need to investigate which recent version of Apex work and whether it's actually tested.\r\n",
      "@gwenzek After this commit was merged, I encountered  an error when doing `pip install --editable .` as below.\r\nI did check that before this commit  the installation is fine under exactly the same environment, and ninja is already installed. Any ideas of what the cause is?\r\n(update: have the same error when doing `pip install .`)\r\n\r\n```\r\n2022-10-02 03:28:06 Building wheels for collected packages: fairseq\r\n2022-10-02 03:28:06   Building editable for fairseq (pyproject.toml): started\r\n2022-10-02 03:28:09   Building editable for fairseq (pyproject.toml): finished with status 'error'\r\n2022-10-02 03:28:09   error: subprocess-exited-with-error\r\n2022-10-02 03:28:09   \r\n2022-10-02 03:28:09   Ã— Building editable for fairseq (pyproject.toml) did not run successfully.\r\n2022-10-02 03:28:09   â”‚ exit code: 1\r\n2022-10-02 03:28:09   â•°â”€> [63 lines of output]\r\n2022-10-02 03:28:09       running editable_wheel\r\n2022-10-02 03:28:09       creating /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info\r\n2022-10-02 03:28:09       writing /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/PKG-INFO\r\n2022-10-02 03:28:09       writing dependency_links to /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/dependency_links.txt\r\n2022-10-02 03:28:09       writing entry points to /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/entry_points.txt\r\n2022-10-02 03:28:09       writing requirements to /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/requires.txt\r\n2022-10-02 03:28:09       writing top-level names to /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/top_level.txt\r\n2022-10-02 03:28:09       writing manifest file '/tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/SOURCES.txt'\r\n2022-10-02 03:28:09       Traceback (most recent call last):\r\n2022-10-02 03:28:09         File \"/miniconda/envs/fairseq/bin/ninja\", line 5, in <module>\r\n2022-10-02 03:28:09           from ninja import ninja\r\n2022-10-02 03:28:09       ModuleNotFoundError: No module named 'ninja'\r\n2022-10-02 03:28:09       /tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py:411: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\r\n2022-10-02 03:28:09         warnings.warn(msg.format('we could not find ninja.'))\r\n2022-10-02 03:28:09       reading manifest file '/tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/SOURCES.txt'\r\n2022-10-02 03:28:09       reading manifest template 'MANIFEST.in'\r\n2022-10-02 03:28:09       adding license file 'LICENSE'\r\n2022-10-02 03:28:09       writing manifest file '/tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq.egg-info/SOURCES.txt'\r\n2022-10-02 03:28:09       creating '/tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq-0.12.2.dist-info'\r\n2022-10-02 03:28:09       adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\r\n2022-10-02 03:28:09       creating /tmp/pip-wheel-ew6gt3p0/tmpz_li8n9z/fairseq-0.12.2.dist-info/WHEEL\r\n2022-10-02 03:28:09       /tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n2022-10-02 03:28:09         warnings.warn(\r\n2022-10-02 03:28:09       running build_py\r\n2022-10-02 03:28:09       running build_ext\r\n2022-10-02 03:28:09       Traceback (most recent call last):\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/editable_wheel.py\", line 140, in run\r\n2022-10-02 03:28:09           self._create_wheel_file(bdist_wheel)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/editable_wheel.py\", line 330, in _create_wheel_file\r\n2022-10-02 03:28:09           files, mapping = self._run_build_commands(dist_name, unpacked, lib, tmp)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/editable_wheel.py\", line 261, in _run_build_commands\r\n2022-10-02 03:28:09           self._run_build_subcommands()\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/editable_wheel.py\", line 288, in _run_build_subcommands\r\n2022-10-02 03:28:09           self.run_command(name)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 319, in run_command\r\n2022-10-02 03:28:09           self.distribution.run_command(command)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/dist.py\", line 1217, in run_command\r\n2022-10-02 03:28:09           super().run_command(command)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 987, in run_command\r\n2022-10-02 03:28:09           cmd_obj.run()\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 84, in run\r\n2022-10-02 03:28:09           _build_ext.run(self)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\r\n2022-10-02 03:28:09           _build_ext.build_ext.run(self)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 346, in run\r\n2022-10-02 03:28:09           self.build_extensions()\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 434, in build_extensions\r\n2022-10-02 03:28:09           self._check_cuda_version(compiler_name, compiler_version)\r\n2022-10-02 03:28:09         File \"/tmp/pip-build-env-5gsmf_xe/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 812, in _check_cuda_version\r\n2022-10-02 03:28:09           raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))\r\n2022-10-02 03:28:09       RuntimeError:\r\n2022-10-02 03:28:09       The detected CUDA version (11.6) mismatches the version that was used to compile\r\n2022-10-02 03:28:09       PyTorch (10.2). Please make sure to use the same CUDA versions.\r\n2022-10-02 03:28:09       \r\n2022-10-02 03:28:09       error: Support for editable installs via PEP 660 was recently introduced\r\n2022-10-02 03:28:09       in `setuptools`. If you are seeing this error, please report to:\r\n2022-10-02 03:28:09       \r\n2022-10-02 03:28:09       https://github.com/pypa/setuptools/issues\r\n```",
      "Hi, could you report the pip version you're using ? and torch ? `pip show torch pip`\r\n\r\nAlso for me the error message is pretty clear, and not related to my commit. ",
      "@gwenzek Hi, my torch/pip version is:\r\n```\r\nName: torch\r\nVersion: 1.12.1\r\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\r\nHome-page: https://pytorch.org/\r\nAuthor: PyTorch Team\r\nAuthor-email: packages@pytorch.org\r\nLicense: BSD-3\r\nLocation: /miniconda/envs/fairseq/lib/python3.8/site-packages\r\nRequires: typing_extensions\r\nRequired-by: torchaudio, torchvision\r\n---\r\nName: pip\r\nVersion: 22.1.2\r\nSummary: The PyPA recommended tool for installing Python packages.\r\nHome-page: https://pip.pypa.io/\r\nAuthor: The pip developers\r\nAuthor-email: distutils-sig@python.org\r\nLicense: MIT\r\nLocation: /miniconda/envs/fairseq/lib/python3.8/site-packages\r\nRequires: \r\nRequired-by: \r\n```\r\nThe message is strange because my torch is indeed compiled with CUDA 11.6 (`import torch; torch.version.cuda` gives `11.6`). I have tried the commit right before this PR was merged and there was no such errors.",
      "@freewym  hi,I have encountered the same problem. Have you solved itï¼Ÿ",
      "@sevensix617 I haven't. Are you using the same version of PyTorch?",
      "Hi, I'm sorry, but I have a hard time reproducing.\r\nHere is how I do it:\r\n\r\n```\r\nconda create python==3.8 -n fairseq_torch_1_10_3\r\nconda activate fairseq_torch_1_10_3\r\npip install torch==1.10.1+cu111 torchaudio==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\r\npip install -e .\r\n```\r\n\r\nCould someone share a detailed script from a new conda env to the error ?\r\nMy guess is that the issue if with the \"isolated build\" feature of setuptools.\r\nIn isolated build, setuptool will create a new env just for building fairseq, which means redownloading a pytorch version which is not the one you have on your system.\r\nSo please also try `pip install --no-build-isolation -e .` \r\n ",
      "@freewym Yes, I am the same as your torch and cuda versions, and the errors are the same.",
      "> Hi, I'm sorry, but I have a hard time reproducing. Here is how I do it:\r\n> \r\n> ```\r\n> conda create python==3.8 -n fairseq_torch_1_10_3\r\n> conda activate fairseq_torch_1_10_3\r\n> pip install torch==1.10.1+cu111 torchaudio==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\r\n> pip install -e .\r\n> ```\r\n> \r\n> Could someone share a detailed script from a new conda env to the error ? My guess is that the issue if with the \"isolated build\" feature of setuptools. In isolated build, setuptool will create a new env just for building fairseq, which means redownloading a pytorch version which is not the one you have on your system. So please also try `pip install --no-build-isolation -e .`\r\n\r\nI ran the build when building a docker:\r\n```\r\nRUN git clone https://github.com/pytorch/fairseq.git && cd fairseq && \\                                                        \r\n        pip install --editable ./ && TORCH_CUDA_ARCH_LIST=\"3.7;6.0;7.0;8.0;8.6\" python setup.py build_ext --inplace\r\n```\r\nafter adding \"--no-build-isolation\" to pip install, pip install seems to work now. However, when running the following `setup.py` command, it has another error when building a cuda lib extension:\r\n```\r\nbuilding 'fairseq.libnat_cuda' extension\r\n        creating /fairseq/build/temp.linux-x86_64-cpython-38/fairseq/clib/libnat_cuda\r\n        Traceback (most recent call last):\r\n          File \"<string>\", line 2, in <module>\r\n          File \"<pip-setuptools-caller>\", line 34, in <module>\r\n          File \"/fairseq/setup.py\", line 252, in <module>\r\n            do_setup(package_data)\r\n          File \"/fairseq/setup.py\", line 164, in do_setup\r\n            setup(\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/__init__.py\", line 87, in setup\r\n            return distutils.core.setup(**attrs)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 185, in setup\r\n            return run_commands(dist)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\r\n            dist.run_commands()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 973, in run_commands\r\n            self.run_command(cmd)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/dist.py\", line 1217, in run_command\r\n            super().run_command(command)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\r\n            cmd_obj.run()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/command/develop.py\", line 34, in run\r\n            self.install_for_development()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/command/develop.py\", line 114, in install_for_development\r\n            self.run_command('build_ext')\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/cmd.py\", line 319, in run_command\r\n            self.distribution.run_command(command)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/dist.py\", line 1217, in run_command\r\n            super().run_command(command)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/dist.py\", line 992, in run_command\r\n            cmd_obj.run()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 79, in run\r\n            _build_ext.run(self)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\r\n            _build_ext.build_ext.run(self)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 346, in run\r\n            self.build_extensions()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 765, in build_extensions\r\n            build_ext.build_extensions(self)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/Cython/Distutils/old_build_ext.py\", line 195, in build_extensions\r\n            _build_ext.build_ext.build_extensions(self)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 466, in build_extensions\r\n            self._build_extensions_serial()\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 492, in _build_extensions_serial\r\n            self.build_extension(ext)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/command/build_ext.py\", line 202, in build_extension\r\n            _build_ext.build_extension(self, ext)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/setuptools/_distutils/command/build_ext.py\", line 547, in build_extension\r\n            objects = self.compiler.compile(\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 581, in unix_wrap_ninja_compile\r\n            cuda_post_cflags = unix_cuda_flags(cuda_post_cflags)\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 480, in unix_cuda_flags\r\n            cflags + _get_cuda_arch_flags(cflags))\r\n          File \"/miniconda/envs/fairseq/lib/python3.8/site-packages/torch/utils/cpp_extension.py\", line 1694, in _get_cuda_arch_flags\r\n            arch_list[-1] += '+PTX'\r\n        IndexError: list index out of range\r\n        [end of output]\r\n```\r\nI resolved it by adding `TORCH_CUDA_ARCH_LIST=xxx` before `pip install`.\r\n\r\nDo you have any idea of why `--no-build-isolation` is not needed (i.e., has the matched PyTorch version) before? Maybe there is a fix if we know the reason.\r\n",
      "Basically the change I made says that we need torch installed to install fairseq, which was already the case.\r\nSince we can now assume that torch is installed we can directly build the C++ extensions while installing fairseq,\r\nthis removes the need to run `python setup.py build_ext --inplace`. \r\n\r\nThis allows to use fairseq has a dependency in another project. This is important to avoid needing everyone to commit their research experiment inside fairseq.\r\n\r\nThe problem with my approach is that when setuptools sees that \"torch\" is required at build time, it sometime decides to download a new version of pytorch instead of using the one on the current conda env. They call that \"build isolation\" and they think it's a feature: https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/#build-isolation\r\nBut then if you match code compile for torch 1.12 with a torch 1.10 at runtime you get errors.\r\n\r\nBut currently there is no way of saying \"I want the same version of pytorch during build and install\". See https://discuss.python.org/t/support-for-build-and-run-time-dependencies/1513/80 for lengthy discussion about this.\r\nAFAIU The conclusion of the thread is that pip can't fix this, but setuptools or alternative build tools should support that.\r\n\r\nSo the best workaround I found so far is to disable build isolation so that you're sure to reuse the same pytorch.\r\nThe thing I don't get is why I can't reproduce the issue in CI or my machine.\r\n\r\nIn anycase before my PR:\r\n\r\n```\r\nconda install torch torchaudio\r\ngit clone https://github.com/facebookresearch/fairseq\r\ncd fairseq\r\npip install -e .\r\npython setup.py build_ext --inplace\r\n```\r\n\r\nNow:\r\n\r\n```\r\nconda install torch torchaudio\r\npip install --no-build-isolation fairseq\r\n```\r\n\r\nAnd the `--no-build-isolation` doesn't seems to be always required, I haven't figured out when setuptools feel like it need to upgrade pytorch.\r\nSo I think it's a net win for me using fairseq as a library. But let me know if you feel strongly against that.\r\n\r\n\r\n",
      "@gwenzek I see. Thanks for your detailed explanations!"
    ],
    "commit_messages": [
      "run all tests (#4733)\n\n* run all tests\r\n\r\n* make torch a build-time dependency\r\n\r\n* add 'dev' extra deps to install black, flake, pytest at once\r\n\r\n* Build docs in CI\r\n\r\nThis should also help catch some import bugs, since sphinx inspect a lot of code\r\n\r\n* CI should do the real install not \"--editable\"\r\n\r\n* check installation succeeded\r\n\r\n* add missing __init__.py file\r\n\r\n* add check installation\r\n\r\n* move check_installation.py to its own script\r\n\r\n* fix pytest import mode, force recent numpy, torch\r\n\r\n* run black before flake and tests\r\n\r\n* torch >= 1.10.0\r\n\r\n* use torch 1.10  for GPU tests"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd64ac254a19ac29c38a",
    "number": 4732,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nSupported fine-grained multi-task learning implemented in the speech-to-speech translation task for the speech-to-text task.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "s2t_mtl",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Support multi-task learning for speech-to-text task (#4732)\n\n* Move TextTargetMultitaskData\r\n\r\n* Support MTL for speech-to-text\r\n\r\n* Fix for black\r\n\r\n* Fix SpeechToTextDatasetCreator\r\n\r\n* Suport online text preprocessing\r\n\r\n* Add keyword to arguments"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd65ac254a19ac29c38b",
    "number": 4730,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdded [R-Drop](https://arxiv.org/abs/2106.14448) regularization.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "rdrop",
    "is_a_fork": false,
    "comments": [
      "Looks good other than comments. Would suggest also checking with Ann if making these changes is ok",
      "They look good to me in general.\r\nBTW, I like the previous implementation that we integrate rdrop label_smoothed_cross_entropy, why do we create a separated one with many redundant code?"
    ],
    "commit_messages": [
      "Add Rdrop (#4730)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd65ac254a19ac29c38c",
    "number": 4727,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdd the condition to support the vocoder inference on cpu only devices.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "vocoder_cpu",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "update vocoder to support cpu (#4727)\n\n* update vocoder to support cpu\r\n\r\n* lint"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd66ac254a19ac29c38d",
    "number": 4724,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdded Conv2dSubsampler as a separate class. In the default setting, Conv1dSubsampler is used for`s2t_transformer.py` while Conv2dSubsampler is used for `convtransformer.py`. But later, it is found that Conv2dSubsampler is slightly better than Conv1dSubsampler regardless of the encoder type.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "2dconv",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add Conv2dSubsampler (#4724)\n\n* Add Conv2dSubsampler\r\n\r\n* Add conv_out_channels option"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd67ac254a19ac29c38e",
    "number": 4723,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nRefactored sequence generator to reuse the function `generate_decoder` for multi-decoder models. The multi-decoder models will be added after this PR.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "refactor_seq_gen",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd68ac254a19ac29c38f",
    "number": 4722,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nIn the current implementation, S2ST models are evaluated with the \"training\" mode during validation. This PR fixes this issue.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_s2st_eval_mode",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix eval mode for S2ST models during training (#4722)\n\n* Fix eval mode for S2ST models during training\r\n\r\n* Add getattr\r\n\r\n* Revert change"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd69ac254a19ac29c390",
    "number": 4721,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixed encoder freezing for Conformer.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_conformer",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix conformer encoder (#4721)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6aac254a19ac29c391",
    "number": 4720,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "st2t_named_entities",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6aac254a19ac29c392",
    "number": 4719,
    "body": "Hi, missing unpacked value here :)\r\n\r\ncc: @kahne ",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @bofenghuang! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234719). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Hi again @kahne ,\r\n\r\n`soundfile.write` accepts an array of `(T x C)`, but `_wavform` here has `(C x T)`. Fixed here in 2nd commit",
      "Hi, `is_audio` arg has been added to `get_zip_manifest` for audio/fbanck in the 3rd commit",
      "Close as mentioned in https://github.com/facebookresearch/fairseq/issues/4493"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6bac254a19ac29c393",
    "number": 4710,
    "body": "â€¦which is helpful if the fetching is io bound\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nMultiCorpusDataset is a wrapper around a bunch of underlying datasets that provide map style access. Currently the __getitem__ function is an io blocking function that wait for the results to return before proceeding. This is okay if the dataset is in memory, however if the dataset is across network, this blocking function call would become a bottleneck.\r\n\r\nTo resolve this issue discussed above, some dataset implementation has provided a async version of the __getitem__, however MultiCorpusDataset does not currently support async function call, which makes them incompatable. This PR fixes this issue.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "asyncio",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "add an option to fetch datapoints within a batch in an async manner, â€¦ (#4710)\n\n* add an option to fetch datapoints within a batch in an async manner, which is helpful if the fetching is io bound\r\n\r\n* add an option to fetch datapoints within a batch in an async manner\r\n\r\nCo-authored-by: juntengjia <juntengjia@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6cac254a19ac29c394",
    "number": 4708,
    "body": "This is a no-op in eager and in ONNX export, but it's better for other\ntracers if this is preserved as shapes directly instead of converted to\na tensor.\n\nThere is a little annoying code duplication with\n`torch.jit.is_scripting()`, which is unforunately necessary because we\ndidn't implement compile-time short circuiting correctly in TorchScript\nlol.\n",
    "head_branch": "fix-trace",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "[fairseq] Guard call to `shape_as_tensor` with `is_in_onnx_export()` (#4708)\n\nThis is a no-op in eager and in ONNX export, but it's better for other\r\ntracers if this is preserved as shapes directly instead of converted to\r\na tensor.\r\n\r\nThere is a little annoying code duplication with\r\n`torch.jit.is_scripting()`, which is unforunately necessary because we\r\ndidn't implement compile-time short circuiting correctly in TorchScript\r\nlol."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6dac254a19ac29c395",
    "number": 4707,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n- Reformatted speech translation related modules with black+isort\r\n- Fixed argment parser\r\n- Moved frontend convolution layers to a separate file\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "reformat_speech_translation",
    "is_a_fork": false,
    "comments": [
      "Hi, this is the root cause of #4726 (can't import fairseq anymore)\r\n\r\nCan you fix it ASAP please ?\r\nMy guess is that you're missing an `__init__.py` file in https://github.com/facebookresearch/fairseq/tree/main/fairseq/models/speech_to_text/modules"
    ],
    "commit_messages": [
      "Reformat speech translation modules (#4707)\n\n* Reformat speech translation modules by black+isort\r\n\r\n* Fix argment parser for Conformer\r\n\r\n* Move convolution\r\n\r\n* Update XMTransformer\r\n\r\n* Remove unnecessary lines\r\n\r\n* Fix import"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6eac254a19ac29c396",
    "number": 4703,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4690 and #4689.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "sacrebleu",
    "is_a_fork": true,
    "comments": [
      "Hi, @gwenzek, could you please help me review this PR?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6fac254a19ac29c397",
    "number": 4699,
    "body": "Adding Guillaume as the code owner for the translation task.\r\n",
    "head_branch": "owners",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update CODEOWNERS (#4699)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd6fac254a19ac29c398",
    "number": 4698,
    "body": "Apparently #4690 had some unintended side effects, spoke with @gwenzek offline and decided to revert it. @BrightXiaoHan it would be nice if you could check out Guillaume's feedback in the original PR and address them.\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "I made some changes for this, can I create a new PR?",
      "> I made some changes for this, can I create a new PR?\r\n\r\nSure!"
    ],
    "commit_messages": [
      "Revert \"Fix for #4689. (#4690)\" (#4698)\n\nThis reverts commit c0c326cbf8c517a6f6d2e63747567a7f3b33a2b6."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd70ac254a19ac29c399",
    "number": 4693,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4676.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @RN0311! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234693). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd71ac254a19ac29c39a",
    "number": 4692,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdds a new noise and audio data augmentation suite with six new augmentation techniques and underlying framework of AudioWaveformTransform and AudioDatasetTransform. Also includes documentation and usage suggestions for these new augmentation techniques.  \r\n\r\n`AudioWaveformTransform` is a base class which allows some transform to be applied to waveforms in the data loading step. `AudioDatasetTransform` is a base class for transforms based on more than one item in a dataset, ex. concatenation of two random samples in a dataset. Further descriptions are given in `examples/speech_to_speech/docs/data_augmentation.md`. \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "ust_augmentation",
    "is_a_fork": false,
    "comments": [
      "Hi Adishree, code changes look good to me. Are we able to verify that we are able to reproduce the vanilla baseline results and the noise Aug baselines with these changes? Also would be good to verify that existing transforms like specAug etc work as expected as well before pushing the code.",
      "@sravyapopuri388 Thanks Sravya! Those have been confirmed. Added the final benchmark numbers to the documentation "
    ],
    "commit_messages": [
      "[feat][ust] Noise and data augmentation suite (#4692)\n\n* Implemented data augmentation for concatenation  (#3516)\r\n\r\n* Implemented ConcatAug as setting from config\r\n\r\n* Switched ConcatAug implementation to sweep script\r\n\r\n* Added rate and max tokens as ConcatAug params\r\n\r\n* Kept original fns, pulled concat_attempts as hyperparam\r\n\r\n* Fixed ConcatAug nits\r\n\r\n* ConcatAug typing recognizes int and np.int\r\n\r\n* Implemented waveform transforms and suite of noise augmentation techniques (#3517)\r\n\r\n* Implemented ConcatAug as setting from config\r\n\r\n* Switched ConcatAug implementation to sweep script\r\n\r\n* Kept original fns, pulled concat_attempts as hyperparam\r\n\r\n* Implemented WaveformTransforms, MusicAug\r\n\r\n* Removed leftovers from debugging\r\n\r\n* Separated out feature_ and waveform_transforms, updated constants, formatting cleanup\r\n\r\n* Added Babble and SporadicNoise augmentations\r\n\r\n* Fixed zero division error\r\n\r\n* Adding BackgroundNoiseAugment\r\n\r\n* Added warning for if using feature transforms with waveform input\r\n\r\n* warnings, SNR fix\r\n\r\n* fix for NoneType extension error\r\n\r\n* fix 2 for NoneType extension error\r\n\r\n* delete print\r\n\r\n* Dataset transform, NoisyOverlapAugment, reframe ConcatAugment (#3533)\r\n\r\n* Dataset transform, NoisyOverlapAugment, reframe ConcatAugment\r\n\r\n* using np.random instead of python random\r\n\r\n* fixed np random upper bound bug\r\n\r\n* cleanup\r\n\r\n* Changed args & return expressions for waveform transform\r\n\r\n* Documented new augmentation features\r\n\r\n* Create augmentation_example.md\r\n\r\n* Update augmentation_example.md\r\n\r\n* Update, benchmarking left to do\r\n\r\n* Move docs to speech_to_speech\r\n\r\n* Remove docs from speech_to_text\r\n\r\n* [docs] Updated clean benchmarks\r\n\r\n* [docs] Add benchmark data"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd72ac254a19ac29c39b",
    "number": 4690,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4689.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "sacrebleu",
    "is_a_fork": true,
    "comments": [
      "Hi @BrightXiaoHan! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234690). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "@BrightXiaoHan thanks for the fix! I see that CI has failed due to some linting issues. Could you please address them?"
    ],
    "commit_messages": [
      "Fix for #4689. (#4690)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd73ac254a19ac29c39c",
    "number": 4685,
    "body": "torch.qr is deprecated for a long time and is being removed by https://github.com/pytorch/pytorch/pull/70989.\r\n\r\nThis PR makes the example compatible with new and old PyTorch versions.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "The test errors seem unrelated - \"OSError: Could not connect to socket\"."
    ],
    "commit_messages": [
      "Update deprecated torch.qr in glow.py example (#4685)\n\ntorch.qr is deprecated for a long time and is being removed by https://github.com/pytorch/pytorch/pull/70989.\r\n\r\nThis PR makes the example compatible with new and old PyTorch versions."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd74ac254a19ac29c39d",
    "number": 4681,
    "body": "# Before submitting\r\n\r\n- [ yes] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ yes] Did you make sure to update the docs?\r\n- [ yes] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix #4679  (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @Bazinga699! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234681). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd74ac254a19ac29c39e",
    "number": 4674,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdds a new noise and audio data augmentation suite with six new augmentation techniques and underlying framework of AudioWaveformTransform and AudioDatasetTransform. \r\n\r\n\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "ust_augmentation",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd75ac254a19ac29c39f",
    "number": 4673,
    "body": "Release NLLB-200 MoE 54.5B model translations.\r\n\r\n\r\nCo-authored-by: Maha Elbayad <elbayadm@users.noreply.github.com>\r\n",
    "head_branch": "nllb_translate",
    "is_a_fork": false,
    "comments": [
      "What is the status of NLLB MoE data being released?"
    ],
    "commit_messages": [
      "[nllb] Release NLLB-200 translations (#4673)\n\nCo-authored-by: Maha Elbayad <elbayadm@users.noreply.github.com>\r\n\r\nCo-authored-by: Maha Elbayad <elbayadm@users.noreply.github.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": true
  },
  {
    "_id": "6621bd76ac254a19ac29c3a0",
    "number": 4670,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThis PR supports new speech-to-speech translation models based on two-pass decoders.\r\n- UnitY (text->unit)\r\n- Translatotron2 (text->spectrogram)\r\n\r\nThese are related to this PR\r\n- https://github.com/facebookresearch/fairseq/pull/4707\r\n- https://github.com/facebookresearch/fairseq/pull/4721\r\n- https://github.com/facebookresearch/fairseq/pull/4722\r\n- https://github.com/facebookresearch/fairseq/pull/4723\r\n- https://github.com/facebookresearch/fairseq/pull/4724\r\n- https://github.com/facebookresearch/fairseq/pull/4730\r\n- https://github.com/facebookresearch/fairseq/pull/4732\r\n- https://github.com/facebookresearch/fairseq/pull/4747\r\n- https://github.com/facebookresearch/fairseq/pull/4763\r\n- https://github.com/facebookresearch/fairseq/pull/4764\r\n\r\nIn addition to supporting new models, I also made several updates that may affect other tasks, \r\n- Support dual cross-attention for Transformer decoder\r\n\r\nEach of them is necessary to obtain the best result with UnitY.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "ust_unity",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "UnitY implementation (#4670)\n\n* Add UnitY implementation\r\n\r\n* Rename for consistency\r\n\r\n* Refactor conformer encoder construction\r\n\r\n* Change the order of arguments for rdrop_alpha\r\n\r\n* Add compute_loss_with_rdrop\r\n\r\n* Move build_multitask_decoder to xm_transformer_unity.py\r\n\r\n* Fix generator selection\r\n\r\n* Fix check in build_criterion\r\n\r\n* Modularize Rdrop\r\n\r\n* Minor fix\r\n\r\n* Refine class names\r\n\r\n* Refactor submodules\r\n\r\n* Fix CE\r\n\r\n* Fix import\r\n\r\n* Fix argments for datasets\r\n\r\n* Add description to AugTransformerDecoderBase\r\n\r\n* Fix SpeechToTextDatasetCreator\r\n\r\n* Fix metavar in arguments\r\n\r\n* Uncomment override_decoder_args\r\n\r\n* Fix comment in warning\r\n\r\n* Add is_fisrt_pass_decoder flag\r\n\r\n* Change Translatotron2SpeechGenerator to MultiDecoderSpeechGenerator\r\n\r\n* Move inference code to examples/speech_to_speech/unity\r\n\r\n* Fix rdrop default value in aux tasks\r\n\r\n* Add language tag mapping option to multitask-config-yaml\r\n\r\n* Rename encoder_out2 and encoder_outs2\r\n\r\n* Rename UnitYXMTransformerModel to XMTransformerModelUnitY\r\n\r\n* Support num_best_checkpoints in average_checkpoints\r\n\r\n* Fix has_multitask\r\n\r\n* Inherit SequenceGenerator\r\n\r\n* Reflect recent updates\r\n\r\n* Minor fix in logging\r\n\r\n* Fix typo\r\n\r\n* Refactor SpeechToSpectrogram2passMultitaskTaskCriterion\r\n\r\n* Minor update for multitask"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd77ac254a19ac29c3a1",
    "number": 4667,
    "body": "Fixes the header file directives of `edit_dist.cu`. Instead of the long deprecated `THC.h` uses the specific `CUDAStream.h`.",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix edit_dist.cu header file directives (#4667)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd78ac254a19ac29c3a2",
    "number": 4663,
    "body": "# Before submitting\r\n\r\n- [Typo] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [No] Did you make sure to update the docs?\r\n- [No] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes the error message for when a custom model is not recognized by the model registry. Previously, only the first string was printed, f\"Could not infer model type from {cfg}.\", and not the later strings which are useful for debugging. Adding the connector + allows all three strings to be printed together following a failed assertion.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd79ac254a19ac29c3a3",
    "number": 4662,
    "body": null,
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix under generation issue for speech to speech translation models by adding optional generation args (#4662)\n\n* OSS ckpts for Interspeech 2022 paper\r\n\r\n* HF interface update\r\n\r\n* local test\r\n\r\n* local test\r\n\r\n* revert local test\r\n\r\n* address comments\r\n\r\n* add Hk<>En models\r\n\r\n* add Hk<>En models\r\n\r\n* add Hk<>En models\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* debug\r\n\r\n* debug\r\n\r\n* debub\r\n\r\n* fix undergeneration for S2UT\r\n\r\n* fix typo\r\n\r\n* fix typo\r\n\r\n* fix bug"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd79ac254a19ac29c3a4",
    "number": 4646,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4645 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7aac254a19ac29c3a5",
    "number": 4635,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nMade changes to add segment embeddings to input embeddings.\r\n",
    "head_branch": "add_segment_embedding",
    "is_a_fork": true,
    "comments": [
      "Hi @Yusuke196! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234635). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7bac254a19ac29c3a6",
    "number": 4632,
    "body": "# Before submitting\r\n\r\n- [ Was requested in a now stale issue, this PR wasn't discussed] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [CLI has help ] Did you make sure to update the docs?\r\n- [Tested during training ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3268.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\nThis pull request adds `--mlflow-logging` cli flag, similar to asureml logging from this PR https://github.com/facebookresearch/fairseq/pull/2999. It enables tracking parameters, metrics, and artifacts using mlflow.\r\n\r\n<img width=\"1703\" alt=\"Screen Shot 2022-08-06 at 8 46 25 AM\" src=\"https://user-images.githubusercontent.com/8658717/183249632-d67e9025-e2da-4b48-9362-040b8bc01608.png\">\r\n\r\n\r\n## Did you have fun?\r\nHeck ya!\r\n",
    "head_branch": "dev/mlflow_progressbar",
    "is_a_fork": true,
    "comments": [
      "Hi @ElefHead! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234632). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Hi @lematt1991, I am not sure if folks who open PRs have to request a review, so I'm tagging you because of https://github.com/facebookresearch/fairseq/issues/3268. Thanks a lot in advance!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7cac254a19ac29c3a7",
    "number": 4629,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "s2t_wav_padding_fix",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang @uralik "
    ],
    "commit_messages": [
      "fix padding of s2t_wav_transformer (#4629)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7dac254a19ac29c3a8",
    "number": 4623,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nFixes #4622\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "rel-pos-mha-init",
    "is_a_fork": true,
    "comments": [
      "Hi @sanchit-gandhi! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234623). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thanks @facebook-github-bot!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [
      "Fix uninitialized bias parameters in RelPositionMultiHeadedAttention (#4623)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7eac254a19ac29c3a9",
    "number": 4621,
    "body": "Fixing the README as seen with @vedanuj",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix NLLB readme (#4621)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7eac254a19ac29c3aa",
    "number": 4620,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? n/a\r\n- [x] Did you write any new necessary tests? n/a\r\n\r\n## What does this PR do?\r\nFixes #4619 in a not great way\r\n\r\nWe need to call `wandb.finish()` at the end of our code to let `wandb` know that the multiprocessing job is over. But this is non-trivial with the current setup of `progress_bar.py`. I tried adding `wandb.finish()` similar to how tensorboard writers are closed using `atexit` (see [here](https://github.com/facebookresearch/fairseq/blob/main/fairseq/logging/progress_bar.py#L420) ) but it doesn't work. \r\n\r\nThe current solution adds it in `fairseq_cli.train` but if there is a more elegant solution that uses `progress_bar.py` I would be happy to change\r\n",
    "head_branch": "wandb-finish",
    "is_a_fork": true,
    "comments": [
      "A better solution is to use wandb service as described in #4619, so I'm closing this PR"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd7fac254a19ac29c3ab",
    "number": 4616,
    "body": "Fix a bug that dumping the km labels without actually learning the kmeans.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (4615).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "Hi @joezhoujinjing! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234616). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd80ac254a19ac29c3ac",
    "number": 4614,
    "body": "TSIA\r\n\r\nTested En->Hk model locally. Currently, Hk->En model doesn't work due to argument mismatch. Will fix that in a follow up PR but adding the model for now",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Support En->Hk model in fairseq hub.  (#4614)\n\n* OSS ckpts for Interspeech 2022 paper\r\n\r\n* HF interface update\r\n\r\n* local test\r\n\r\n* local test\r\n\r\n* revert local test\r\n\r\n* address comments\r\n\r\n* add Hk<>En models\r\n\r\n* add Hk<>En models\r\n\r\n* add Hk<>En models\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en\r\n\r\n* add hk->en"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd81ac254a19ac29c3ad",
    "number": 4613,
    "body": "The input to adaptor layer was not padded properly so that resulting downsampled representation contained noise influenced by different batch sizes (i.e., depending on the batch max length). This fix ensures proper zeroing of padding positions of the encoder output before and after applying the convolutional downsamplings. \r\n\r\nThe latter zeroing operation is a bit redundant, but it ensures that in potential extra processing the padded information won't influence results.\r\n\r\nWe verified internally that this fix alleviates the issue of score inconsistency w.r.t. to different values of batch size.",
    "head_branch": "adaptor_pad_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "padding fix in the adaptor layer (#4613)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd82ac254a19ac29c3ae",
    "number": 4612,
    "body": "Small fix to fastbpe preprocessing script in examples/fast_noisy_channel/README.md.",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @alexandremuzio! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234612). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd83ac254a19ac29c3af",
    "number": 4611,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFormatting for fbcode.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix Linting Errors (#4611)\n\n* Update version.txt\r\n\r\n* Update create_dict_stop.sh\r\n\r\n* Update enhanced_direct_s2st_discrete_units.md\r\n\r\n* Update hubert_asr.py\r\n\r\n* Update utils.py"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd83ac254a19ac29c3b0",
    "number": 4609,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a typo in README.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nA lot of fun trying to read the Readme with the typo :p\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @YaYaB! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234609). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd84ac254a19ac29c3b1",
    "number": 4608,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd85ac254a19ac29c3b2",
    "number": 4604,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4603.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "This failure looks unrelated...",
      "Any chance this could get merged @dianaml0? It's pretty small and is parallel to what happens in, e.g., metaseq so this seems like the right thing to do here."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd86ac254a19ac29c3b3",
    "number": 4601,
    "body": "This PR completes the mention of the HuggingFace integration to the `nllb` integration.\r\n\r\nOpen for comments!",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @LysandreJik! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234601). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Hey @cbalioglu, how can I help to get this merged?",
      "> Hey @cbalioglu, how can I help to get this merged?\r\n\r\n@LysandreJik just merged!",
      "Super cool, thank you!",
      "This PR messed up the formatting of the Readme. Kindly fix it. "
    ],
    "commit_messages": [
      "Mention Hugging Face integration (#4601)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd87ac254a19ac29c3b4",
    "number": 4599,
    "body": null,
    "head_branch": "public/paden/stop-fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd88ac254a19ac29c3b5",
    "number": 4594,
    "body": "Add support for merging source and target embedding table into one table. This feature might hurt performance, but it will decrease the size of the final model.\r\n",
    "head_branch": "rasto/merge-embed",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge source and target embedding tables into one table (#4594)\n\nAdd support for merging source and target embedding table into one table. This feature might hurt performance, but it will decrease the size of the final model."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd88ac254a19ac29c3b6",
    "number": 4593,
    "body": "Update the existing S2T and TTS hub interfaces to support S2UT model plus vocoder in the HF demo. As next steps will test and update the HF pipeline. The detailed changes in this PR are\r\n\r\n1. Update CodeHifiGAN vocoder to inherit BaseFairseqModel instead of nn.Module to use the underlying hub interface. Also add relevant methods to the vocoder to load and infer using the models from S3\r\n2. Add VocoderHubInterface class to TTS hub interface to support waveform generation using CodeHifiGAN vocoders.\r\n3. Add S2UT models to xm_transformer and update S2T interface accordingly\r\n4. Tested the changes in a script in my local using the script located at\r\n/private/home/spopuri/speech_translation/fairseq_speech/test_HF_demo.py. Couldn't use google collab for testing due to GPU memory constraints",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Support direct S2ST models in HF demo (#4593)\n\n* OSS ckpts for Interspeech 2022 paper\r\n\r\n* HF interface update\r\n\r\n* local test\r\n\r\n* local test\r\n\r\n* revert local test\r\n\r\n* address comments"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd89ac254a19ac29c3b7",
    "number": 4588,
    "body": "Open source finetuned model ckpts for enhanced direct speech to speech translation paper",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "OSS ckpts for Interspeech 2022 paper (#4588)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8aac254a19ac29c3b8",
    "number": 4584,
    "body": null,
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8bac254a19ac29c3b9",
    "number": 4580,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "st2t_multilang_collater",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8cac254a19ac29c3ba",
    "number": 4579,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "st2t_ds_impl",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang ",
      "It looks good to me!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8cac254a19ac29c3bb",
    "number": 4578,
    "body": "# Before submitting\r\n\r\n- [ N ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ Y ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ Y ] Did you make sure to update the docs?\r\n- [ N ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThis PR releases code to reproduce end-to-end NLU experiments on the STOP dataset.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nSo much fun. \r\n",
    "head_branch": "public/paden/e2e-nlu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "STOP Dataset Release and Experiment Reproduction (#4578)\n\n* STOP paper release \r\n\r\nCo-authored-by: Paden Tomasello <padentomasello@devfair0417.h2.fair>\r\nCo-authored-by: Paden Tomasello <padentomasello@learnfair5258.h2.fair>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8dac254a19ac29c3bc",
    "number": 4576,
    "body": "Using the correct script name for Santali ",
    "head_branch": "nllb_fix_sat_script",
    "is_a_fork": true,
    "comments": [
      "Is this ready to be merged?",
      "@elbayadm @vedanuj \r\nThe huggingface pipeline still uses the old language code for Santali\r\n\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\r\ncheckpoint = \"facebook/nllb-200-distilled-600M\"\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\r\nmodel.eval()\r\npten_pipeline = pipeline('translation', model=model, tokenizer=tokenizer)\r\n\r\nprint(pten_pipeline(\"Hello\", src_lang='eng_Latn',tgt_lang='sat_Olck', max_length=500)) # --> Hello\r\nprint(pten_pipeline(\"Hello\", src_lang='eng_Latn',tgt_lang='sat_Beng', max_length=500)) # --> á±¥á±Ÿá±¨á±¦á±Ÿá±£\r\n```"
    ],
    "commit_messages": [
      "Fix Santali language code: sat_Beng -> sat_Olck (#4576)\n\n* fix Santali language code: sat_Beng -> sat_Olck\r\n\r\n* Olck typo"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8eac254a19ac29c3bd",
    "number": 4571,
    "body": "Fix tlayer torch JIT export exception:\r\n\"Could not cast value of type NoneType to bool\"\r\nWhen torch jit exporting, self.need_attn is None.\r\nFix #4459\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements): Issue created 1 month ago, no comment.\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests? I will if anyone comments/considers it necessary\r\n\r\n## What does this PR do?\r\nFixes #4459.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\nI ve created the Github issue 1 month ago, no comment as today: #4459\r\n\r\n## Did you have fun?\r\nnot yet",
    "head_branch": "fix-tlayer-jit-export",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd8fac254a19ac29c3be",
    "number": 4568,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nRevert the PR #4513 since that PR will break the torchscript of quantized model.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "export1",
    "is_a_fork": true,
    "comments": [
      "@dianaml0  the test looks like flaky?",
      "@frank-wei yes the breaking test is unrelated, fixing that separately. Could you just make sure that you use the pre-commit hook for this (run `flake8` and `black`) before submitting?",
      "Could you also update the PR with a link to the PR you're reverting and a quick description?",
      "> @frank-wei yes the breaking test is unrelated, fixing that separately. Could you just make sure that you use the pre-commit hook for this (run `flake8` and `black`) before submitting?\r\n\r\nyes I did.",
      "> Could you also update the PR with a link to the PR you're reverting and a quick description?\r\n\r\nUpdated."
    ],
    "commit_messages": [
      "update isort (#4568)\n\nCo-authored-by: dianaml0 <82468439+dianaml0@users.noreply.github.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd90ac254a19ac29c3bf",
    "number": 4567,
    "body": "Update the existing S2T and TTS hub interfaces to support S2UT model plus vocoder in the HF demo. As next steps will test and update the HF pipeline. The detailed changes in this PR are\r\n\r\n1. Update CodeHifiGAN vocoder to inherit BaseFairseqModel instead of nn.Module to use the underlying hub interface. Also add relevant methods to the vocoder to load and infer using the models from S3\r\n2. Add VocoderHubInterface class to TTS hub interface to support waveform generation using CodeHifiGAN vocoders.\r\n3. Add S2UT models to xm_transformer and update S2T interface accordingly\r\n4. Tested the changes in a script in my local using the script located at\r\n/private/home/spopuri/speech_translation/fairseq_speech/test_HF_demo.py. Couldn't use google collab for testing due to GPU memory constraints",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd90ac254a19ac29c3c0",
    "number": 4565,
    "body": "Update the existing S2T and TTS hub interfaces to support S2UT model plus vocoder in the HF demo. As next steps will test and update the HF pipeline. The detailed changes in this PR are\r\n1. Update CodeHifiGAN vocoder to inherit BaseFairseqModel instead of nn.Module to use the underlying hub interface. Also add relevant methods to the vocoder to load and infer using the models from S3 \r\n2. Add VocoderHubInterface class to TTS hub interface to support waveform generation using CodeHifiGAN vocoders.\r\n3. Add S2UT models to xm_transformer and update S2T interface accordingly\r\n4. Tested the changes in a script in my local using the script located at \r\n/private/home/spopuri/speech_translation/fairseq_speech/test_HF_demo.py. Couldn't use google collab for testing due to GPU memory constraints",
    "head_branch": "facebookresearch-main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd91ac254a19ac29c3c1",
    "number": 4564,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd92ac254a19ac29c3c2",
    "number": 4556,
    "body": "Fix the link to the flores-200 readme in the \"Evaluation and Generation\" section. Current link throws a 404\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "README: fix link to flores-200 (#4556)\n\nFix the link to the flores-200 readme in the \"Evaluation and Generation\" section"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd93ac254a19ac29c3c3",
    "number": 4554,
    "body": "## What does this PR do?\r\nRemoves unused cluster.data_dir and updates INSTALL.md for fairscale to use the branch needed for MoE checkpoint loading when num_experts < num_gpus",
    "head_branch": "nllb-oss-update",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #4554 from facebookresearch/nllb-oss-update\n\n[nllb] remove unused train cluster.data_dir, update fairscale INSTALL instructions"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd94ac254a19ac29c3c4",
    "number": 4551,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a typo in the README: `Backtransled` -> `Backtranslated`\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "nllb-readme-typo-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Backtransled -> Backtranslated (#4551)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd94ac254a19ac29c3c5",
    "number": 4550,
    "body": "## What does this PR do?\r\nFixes #4549  (issue).\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @st-vincent1! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234550). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [
      "closes #4549 (#4550)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd95ac254a19ac29c3c6",
    "number": 4545,
    "body": "@myleott @alexeib amazing work once again!\r\nWould love to collaborate with you guys to build a Lightning App (a new class available in Lightning) so people can your work as a template for ML systems they might want to build.\r\n(here's an example of a [Lightning App with OpenAI Clip](https://lightning.ai/app/7pmQNIDxAE-InVideo%20Search) to show how to build a meaningful ML system end-to-end)\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @williamFalcon! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234545). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Hey @williamFalcon, thanks for the proposal! I am already in touch with Thomas (Chaton) and once we have an early release of fairseq v2, we can definitely consider having a demo Lightning App. You can check out our v2 announcement in #4493.\r\n\r\nOn the organizational side I recently transitioned from PyTorch to FAIR and right now leading our fairseq modernization effort. @alexeib is also pretty much involved in the project. Unfortunately @myleott is not at Meta anymore, but he is obviously more than welcome to contribute to the project anytime he wants."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd96ac254a19ac29c3c7",
    "number": 4544,
    "body": "â€¦ epoch\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "st2t_fix_randomresampling",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang @kahne ",
      "Can you sync it with main branch?",
      "done @kahne !"
    ],
    "commit_messages": [
      "fix multi modality dataset error with resampling datasets after first epoch (#4544)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd97ac254a19ac29c3c8",
    "number": 4543,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "st2t_fixmultilingual_langs",
    "is_a_fork": true,
    "comments": [
      "cc @yuntang @kahne ",
      "@mgaido91  Thanks. Can you sync the code the lastest HEAD of main?"
    ],
    "commit_messages": [
      "fix multilingual training with tags only on target side (#4543)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd98ac254a19ac29c3c9",
    "number": 4542,
    "body": "Commits\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @prishasatwani! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234542). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd98ac254a19ac29c3ca",
    "number": 4540,
    "body": "Hi,\r\n\r\nthis PR adds an check if the `target_model` module really exists in the encoder of a data2vec model.\r\n\r\nWith newer pretrained data2vec models, this check is really important because it led to an error as reported in #4534.",
    "head_branch": "fix-data2vec-target-model-check",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd99ac254a19ac29c3cb",
    "number": 4539,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFailing build. This fixes some steps, still some failure in the test.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [
      "Merging to unblock other PRs. Additional errors that need to be fixed, but this fixes some."
    ],
    "commit_messages": [
      "Update xformers install, other small fixes (#4539)\n\n* Update build.yml\r\nfix logger error\r\n\r\n* formatting fix\r\n\r\nCo-authored-by: Diana Liskovich <dianaml@devfair0471.h2.fair>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9aac254a19ac29c3cc",
    "number": 4524,
    "body": "this prevents ooms for code that leaks memory in data loading workers",
    "head_branch": "persist_workers_flag",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "turn persistent workers off by default (#4524)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9bac254a19ac29c3cd",
    "number": 4522,
    "body": "Flashlight beam search decoding and text utilities have been moved to [Flashlight Text](https://github.com/flashlight/text) after Flashlight v0.3.2. Specify that users installing the fairseq bindings must use v0.3.2 (we've been receiving a few issues about this).",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "[docs] Update Flashlight Bindings Docs (#4522)\n\n* Update README.md\r\n\r\n* Update README.md"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9cac254a19ac29c3ce",
    "number": 4521,
    "body": "\r\n## What does this PR do?\r\nThis PR fixes a trivial typo in HuBERT preprocessing guideline.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes. We spent a month of debugging HuBERT training as the loss was quickly going to zero. The culprit was that we accidentally created the cluster .dict.txt file as EMPTY due to this typo (the filed changed in this PR)\r\nAs a result, fairseq-train would load an empty dictionary, and the k-means clusters would all be replaced with 3 (UNK).\r\nThis loosely fits within my definition of fun.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @Viliana99! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234521). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9cac254a19ac29c3cf",
    "number": 4520,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n- Enables loading model checkpoint trained with AMP into FP32 and vice versa.\r\n- Saves the state of AMP gradient scaler with checkpoint.\r\n\r\nThe first change makes it possible to stop training in AMP if loss explodes, continue training in FP32 and switch back to AMP if needed.\r\nSecond change is mostly to continue with same value of gradient scaling rather than start from amp-init-scale every time.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "amp_checkpoint_save_changes",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9dac254a19ac29c3d0",
    "number": 4519,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nDecorator to function to enable hot-reload for debugging. \r\n    It allows you to debug a function without having to reload all heavy models, dataset loading and \r\n        preprocessing, and allow faster debugging.\r\n    If you want to change model or dataset loading, consider relaunching your code\r\n\r\n    -----------------------------------\r\n\r\n    This will run the decorated function func:\r\n        -- if func run successful:\r\n            It will pause, allow user to edit code, and prompt user to:\r\n                Press enter to re-run the function with updated code\r\n                Type \"done\" to finish the function, return output\r\n                Type \"disable\" to stop pausing this function and let code continue without pause\r\n                Ctril + C to terminate\r\n        -- if func raise error:\r\n            it will prompt user to \r\n                1. Edit code, and press enter to retry \r\n                2. Ctrl + C to terminate\r\n                3. Type \"raise\" to raise that exception\r\n\r\n    -----------------------------------\r\n\r\n    * Requirements:\r\n        0. Fairseq was installed with `pip install --editable .`\r\n        1. pip install jurigged[develoop]\r\n        2. set environment HOTRELOAD_PAUSE=1 CUDA_LAUNCH_BLOCKING=1\r\n        3. Run on only 1 GPU (no distributed)\r\n\r\n   -----------------------------------\r\n\r\n    * How to use:\r\n        1. in python, import and decorate the top-level function to be re-run after code edits:\r\n            ```python\r\n            from fairseq.utils import hotreload_function\r\n            ....\r\n            @hotreload_function(\"train_step\")\r\n            def train_step(self, sample ....):\r\n                ....\r\n            ....\r\n            ```\r\n        2. in bash run scripts:\r\n            ```bash\r\n            watch_dir=<home>/fairseq-py/fairseq/tasks # directory to watch for file changes\r\n            export CUDA_VISIBLE_DEVICES=0 # single-gpu\r\n            HOTRELOAD_PAUSE=1 CUDA_LAUNCH_BLOCKING=1 python -m jurigged -w ${watch_dir} --poll 2 -v train.py ......\r\n            ```\r\n     * Then, the script will launch the training process normally, load heavy models, \r\n      data and preprocess data. The script will pause when it meets train_step(). \r\n      After running train_step() it will ask for user prompt and allow user \r\n      to edit code and editted code will take effects after train_step() is re-run.\r\n   -----------------------------------\r\n\r\n    * NOTE:\r\n        1. -w ${watch_dir} specify all the files to be watched for changes\r\n            once functions, class, ... code are changed, all instances in the process will get updated (hot-reload)\r\n    * Limitation:\r\n        * Currently distributed debugging not working\r\n        * Need to launch train.py locally (cannot submit jobs)\r\n        * JIT code may not get updated\r\n\r\n\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hot_reload_debug",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Hot_reload_debug (#4519)\n\n* Add hot reload function decorator, allow faster debugging without reloading models and data."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9eac254a19ac29c3d1",
    "number": 4516,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nNew Release, should fix #4501 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "0.12.2-release",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "v0.12.2 release (#4516)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bd9fac254a19ac29c3d2",
    "number": 4515,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nBuild breaking.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "diana_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fix format (#4515)\n\nCo-authored-by: Diana Liskovich <dianaml@devfair0471.h2.fair>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda0ac254a19ac29c3d3",
    "number": 4513,
    "body": "Summary: Reland this diff.\n\nReviewed By: erichan1\n\nDifferential Revision: D37371293\n\n",
    "head_branch": "export-D37371293",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D37371293](https://www.internalfb.com/diff/D37371293)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37371293](https://www.internalfb.com/diff/D37371293)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37371293](https://www.internalfb.com/diff/D37371293)",
      "@dianaml0  could you help to revert this PR. Thanks."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda0ac254a19ac29c3d4",
    "number": 4512,
    "body": "Summary:\nFix the NMT failed test by adding dual copy\nThis diff is based on D37371293\n\nReviewed By: erichan1\n\nDifferential Revision: D37409849\n\n",
    "head_branch": "export-D37409849",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D37409849](https://www.internalfb.com/diff/D37409849)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda1ac254a19ac29c3d5",
    "number": 4511,
    "body": "https://github.com/facebookresearch/fairseq/issues/4501\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4501 .\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml_patch",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37415937)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda2ac254a19ac29c3d6",
    "number": 4497,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "test_torch_nightly",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda3ac254a19ac29c3d7",
    "number": 4496,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #2286 by applying [the fix](https://github.com/facebookresearch/fairseq/issues/2286#issuecomment-891420884) suggested by @ryonakamura. \r\n\r\n**Causes of the issue**\r\nPyTorch throws a CUDA assert error about `sampleMultinomialOnce` when we try to sample from a distribution that contains all zero (i.e. sum > accZero, see torch implementation [here](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/MultinomialKernel.cu#L226)). \r\nWhen we try to sample the next token after the max length, `fairseq` will set the log probability to all other tokens except `self.eos` to be `-inf` (see `fairseq` implementation [here](https://github.com/facebookresearch/fairseq/blob/a0ceabc287e26f64517fadb13a54c83b71e8e469/fairseq/sequence_generator.py#L362)).\r\n\r\nThe log probability for `self.eos` is unlikely to be `-inf` from the model, therefore the multinomial sampling should always sample EOS in this case. Yet in practice, when we take the exponent of the log-probability, the probability for EOS WILL be 0 in some cases potentially due to floating point precision (making the vector to contains all zero), and this causes the CUDA assert to be thrown.\r\n\r\n\r\n[The fix](https://github.com/facebookresearch/fairseq/issues/2286#issuecomment-891420884) suggests by @ryonakamura manually set the log probability for EOS to be 1, hence `exp(1) = e > 0` which will no longer trigger the CUDA error, and will sample the EOS as expected.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "I was able to pull this branch and confirm it prevents the CUDA error without changing model predictions."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda4ac254a19ac29c3d8",
    "number": 4488,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda5ac254a19ac29c3d9",
    "number": 4487,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? #3169 discusses an error relating to using `self.args` instead of `self.cfg`\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? no api changes\r\n- [x] Did you write any new necessary tests? no api changes\r\n\r\n## What does this PR do?\r\nChanges `translation_from_pretrained_bart` task to use hydra cfg\r\nChanges imports from relative to absolute (`.translation` to `fairseq.tasks.translation`)\r\nUpdates call to `SequenceGenerator` to include new arg `max_len`\r\nFixes #3169 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n\r\nNote: Same changes in new PR that was closed by accident. See old PRs #4010 #4258",
    "head_branch": "hydrate-bart",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda5ac254a19ac29c3da",
    "number": 4486,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nHad to rerun release, created new PR. \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "0.12.1-release",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37110232)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda6ac254a19ac29c3db",
    "number": 4485,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? --> not necessary given it's a small fix\r\n- [x] Did you write any new necessary tests? --> tested on isolated local machines\r\n\r\n## What does this PR do?\r\nFixes #4484.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @jzhou316! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234485). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda7ac254a19ac29c3dc",
    "number": 4483,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "release_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37098170)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda8ac254a19ac29c3dd",
    "number": 4482,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "0.12.1-release",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37098118)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda9ac254a19ac29c3de",
    "number": 4480,
    "body": "Summary:\nas titled and depends on D36057338\nFork the inference path inside the forward function. If loaded the checkpoint file and perform the inference, we will deploy BT. Otherwise, fairseq take the position.\n\nIn summary:\nAccuracy: accuracy loss due to the fp16, the maximum diff is around 0.009. If we set it to fp32, there is no accuracy loss\nPerf: the current fairseq has similar speed as vanilla version. After the enablement, the speedup is similar to standalone BT test.\nWith batch size=64\nFor V100, the speedup reaches to 1.23x\nFor A100, the speedup reaches to 1.38x\n\nAfter enable nested tensor,\nFor V100, the speedup reaches to 2.46x\n\nReviewed By: mikekgfb\n\nDifferential Revision: D37082681\n\n",
    "head_branch": "export-D37082681",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D37082681](https://www.internalfb.com/diff/D37082681)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37082681](https://www.internalfb.com/diff/D37082681)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37082681](https://www.internalfb.com/diff/D37082681)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37082681](https://www.internalfb.com/diff/D37082681)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D37082681](https://www.internalfb.com/diff/D37082681)",
      "@dianaml0  The flow creation could be separate from this PR so I landed it first. \r\nI am not allowed to create any flow or yaml file as I met such error:\r\n new_flow -> new_flow (refusing to allow a Personal Access Token to create or update workflow `.github/workflows/build_1_13.yml` without `workflow` scope)\r\n \r\n Could you help to create a flow with new yaml file? It should be 99% the same with your current `build.yml` but changed the line 31 to:\r\n```\r\n run: pip3 install --pre torch==1.13.0.dev20220613 -f https://download.pytorch.org/whl/nightly/torch_nightly.html \r\n```",
      "This pull request has been **reverted** by 956fcf495b2d5d696ba114520363f82148a8a649."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bda9ac254a19ac29c3df",
    "number": 4476,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "0.12.0",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37072342)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdaaac254a19ac29c3e0",
    "number": 4475,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "auto_release",
    "is_a_fork": false,
    "comments": [
      "successful test run: https://github.com/facebookresearch/fairseq/actions/runs/2476987602",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37081823).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37081823).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37081823)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdabac254a19ac29c3e1",
    "number": 4473,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "circleci_status",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37052250)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdacac254a19ac29c3e2",
    "number": 4472,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "version_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37047097).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37047097)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdacac254a19ac29c3e3",
    "number": 4471,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "release_nit_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37040680)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdadac254a19ac29c3e4",
    "number": 4468,
    "body": "Summary:\nas titled\nFord the inference path inside the forward function. If loaded the checkpoint file and perform the inference, we will deploy BT. Otherwise, fairseq take the position.\n\nIn summary:\nAccuracy: accuracy loss due to the fp16, the maximum diff is around 0.009. If we set it to fp32, there is no accuracy loss\nPerf: the current fairseq has similar speed as vanilla version. After the enablement, the speedup is similar to standalone BT test.\nWith batch size=64\nFor V100, the speedup reaches to 1.23x\nFor A100, the speedup reaches to 1.38x\n\nAfter enable nested tensor,\nFor V100, the speedup reaches to 2.46x\n\nReviewed By: mikekgfb\n\nDifferential Revision: D36057338\n\n",
    "head_branch": "export-D36057338",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D36057338](https://www.internalfb.com/diff/D36057338)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D36057338](https://www.internalfb.com/diff/D36057338)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D36057338](https://www.internalfb.com/diff/D36057338)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdaeac254a19ac29c3e5",
    "number": 4467,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [âˆš] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [âˆš] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes a typo in step_lr_scheduler.py\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Note that this is technically a breaking change."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdafac254a19ac29c3e6",
    "number": 4466,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "document_release",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D37040595)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb0ac254a19ac29c3e7",
    "number": 4465,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes https://github.com/facebookresearch/fairseq/issues/4464.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb1ac254a19ac29c3e8",
    "number": 4457,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAutomates release process and allows it to be triggered directly. Heavily inspired by fairscale's release setup.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "v0.11.1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb1ac254a19ac29c3e9",
    "number": 4456,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAutomates release process and allows it to be triggered directly. Heavily inspired by fairscale's release setup.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "v0.11.1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb2ac254a19ac29c3ea",
    "number": 4455,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAutomates release process and allows it to be triggered directly. Heavily inspired by fairscale's release setup. Few improvements in followup PR.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "auto_release",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36993777).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36993777).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36993777).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36993777)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb3ac254a19ac29c3eb",
    "number": 4447,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue?\r\nSee #4416\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4416.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "denoising-omegaconf",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "switch denoising and multilingual_denoising tasks to OmegaConf (#4447)\n\nCo-authored-by: Alexander Jipa <azzhipa@amazon.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb4ac254a19ac29c3ec",
    "number": 4443,
    "body": "Summary: To fix errors introduced in D35571505\n\nReviewed By: ngimel\n\nDifferential Revision: D36726254\n\n",
    "head_branch": "export-D36726254",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D36726254](https://www.internalfb.com/diff/D36726254)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb5ac254a19ac29c3ed",
    "number": 4441,
    "body": "edit some mistypo\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @sekim-khu! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234441). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb5ac254a19ac29c3ee",
    "number": 4440,
    "body": "Alternatively, we could pin a version of omegaconf\r\n\r\n# Before submitting\r\n\r\n- [X ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements) No\r\n- [X ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)? Yes\r\n- [X ] Did you make sure to update the docs? N/A\r\n- [ X] Did you write any new necessary tests? N/A\r\n\r\n## What does this PR do?\r\nNo issue opened, but noticed when running torchbenchmark\r\n\r\n## PR review\r\n\r\n\r\n## Did you have fun?\r\nThe most fun you can have with your clothes on\r\n\r\n",
    "head_branch": "voz/fairseq_omegaconf_fix",
    "is_a_fork": true,
    "comments": [
      "Thanks @voznesenskym \r\nFacing same blocking issue when running simple torch hub examples: \r\n```\r\n  File \"/home/wtambellini/.cache/torch/hub/pytorch_fairseq_main/fairseq/dataclass/utils.py\", line 367, in __init__\r\n    self.old_is_primitive = _utils.is_primitive_type\r\nAttributeError: module 'omegaconf._utils' has no attribute 'is_primitive_type'\r\n```\r\nwith\r\n```\r\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.8/dist-packages (2.2.1)\r\n```\r\nNo CI test to catch such regression ?",
      "are you using omegaconf version from requirements.py?",
      "hum, which file ? \r\n```\r\n~/repos/fairseq (main) $ find . -iname requirements*\r\n./docs/requirements.txt\r\n```\r\n",
      "ah sorry i mean setup.py\r\nhttps://github.com/facebookresearch/fairseq/blob/main/setup.py\r\n\r\n```\r\n        install_requires=[\r\n            \"cffi\",\r\n            \"cython\",\r\n            'dataclasses; python_version<\"3.7\"',\r\n            \"hydra-core>=1.0.7,<1.1\",\r\n            \"omegaconf<2.1\",\r\n            'numpy<1.20.0; python_version<\"3.7\"',\r\n            'numpy; python_version>=\"3.7\"',\r\n            \"regex\",\r\n            \"sacrebleu>=1.4.12\",\r\n            \"torch\",\r\n            \"tqdm\",\r\n            \"bitarray\",\r\n            \"torchaudio>=0.8.0\",\r\n        ],\r\n        ```",
      "I believe I am on 2.0.6, as I ran the setup/install script specified in the README.md (`pip install --editable ./) and it passed.\r\n\r\nI am working on torchdynamo, and I found this when running torchbenchmark. \r\n\r\nFixing this moves me to the next error:\r\n\r\n```\r\nThe strict flag in the compose API is deprecated.\r\n...\r\n    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/basecontainer.py\", line 626, in _wrap_value_and_set\r\n    wrapped = _maybe_wrap(\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/omegaconf.py\", line 1070, in _maybe_wrap\r\n    return _node_wrap(\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/omegaconf.py\", line 1027, in _node_wrap\r\n    node = AnyNode(value=value, key=key, parent=parent)\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/nodes.py\", line 135, in __init__\r\n    super().__init__(\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/nodes.py\", line 29, in __init__\r\n    self._set_value(value)  # lgtm [py/init-calls-subclass]\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/nodes.py\", line 46, in _set_value\r\n    self._val = self.validate_and_convert(value)\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/nodes.py\", line 76, in validate_and_convert\r\n    return self._validate_and_convert_impl(value)\r\n  File \"/data/home/voz/miniconda3/lib/python3.9/site-packages/omegaconf/nodes.py\", line 154, in _validate_and_convert_impl\r\n    raise UnsupportedValueType(\r\nomegaconf.errors.UnsupportedValueType: Value 'Namespace' is not a supported primitive type\r\n    full_key: task\r\n    object_type=dict\r\n    ```",
      "its because the way you fixed it creates this error. that primitive type thing is monkey patching omegaconf to allow you to set a namespace object as a config value. you must have some wrong version of omegaconf if it has a different impl now for type checking",
      "> its because the way you fixed it creates this error. that primitive type thing is monkey patching omegaconf to allow you to set a namespace object as a config value. you must have some wrong version of omegaconf if it has a different impl now for type checking\r\n\r\nNot 100% - the error is valid, but I bet the name of the thing we are monkeypatching changed. Fixing PR  to get past this, I missed the enter and exit calls. \r\n\r\nIn general, I agree w/ @WilliamTambellini - if we're gonna do stuff like this (monkey patching, private APIs, etc), we need to have a CI script that ensures it all works and can install. ",
      "Downgrading to omegaconf==2.2.0\r\nseems to also resolve the issue but I still dont see any requirements.txt in the repo (except the one from the doc).\r\n",
      "@WilliamTambellini would you be able to try w/ this fix?",
      "it was always meant to be a temporary solution until the argparse config thing is phased out, which of course hasnt happened :P but maybe soon!",
      "Note: \"First-time contributors need a maintainer to approve running workflows. [Learn more.](https://docs.github.com/actions/managing-workflow-runs/approving-workflow-runs-from-public-forks)\" \r\n\r\nIf you guys have a moment, would you help me run the workflow?",
      "Thanks for the run @alexeib. CI for MacOS seems to fail with unrelated errors, while ubuntu passes. This feels like a flaky CI job, as the code should have nothing to do w/ OS.\r\n\r\nWould you be able to kick off another run for me?",
      "Master as today seems to install 2.0.6:\r\n```\r\npython3.8 -m pip install --editable ./\r\n...\r\nSuccessfully installed antlr4-python3-runtime-4.8 fairseq hydra-core-1.0.7 omegaconf-2.0.6 torch-1.11.0 torchaudio-0.11.0\r\n```",
      "Checks passed, diff stamped, would a reviewer mind merging in the PR?",
      "@alexeib has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36694085)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb6ac254a19ac29c3ef",
    "number": 4435,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdd a note for the python-dev requirement\r\n\r\n## Did you have fun?\r\nyes\r\n",
    "head_branch": "wt",
    "is_a_fork": true,
    "comments": [
      "prevents such install error:\r\n```\r\n    gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python3.6m -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-3.6/fairseq/clib/libbleu/module.o -std=c++11 -O3\r\n    fairseq/clib/libbleu/module.cpp:9:20: fatal error: Python.h: No such file or directory\r\n     #include <Python.h>\r\n```\r\n",
      "First-time contributors need a maintainer to approve running workflows ?",
      "\"First-time contributors need a maintainer to approve running workflows\" ?\r\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb7ac254a19ac29c3f0",
    "number": 4431,
    "body": "This PR adds the code for the following methods to the Non-Autoregressive Transformer:\r\n\r\n- Glancing Transformer (GLAT) from \"[Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://aclanthology.org/2021.acl-long.155.pdf)\" (Qian et al., 2021)\r\n- Connectionist Temporal Classification (CTC) from \"[End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification](https://aclanthology.org/D18-1336.pdf)\" (LibovickÃ½ & Helcl, 2018)\r\n- Deep Supervision (DS) from \"[Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision](https://arxiv.org/abs/2110.07515)\" (Huang et al., 2021)\r\n\r\nImportant to note is that this code still has one more dependency on the C++ code from [torch_imputer](https://github.com/rosinality/imputer-pytorch) that is currently not integrated in this PR. We leave it up to the fairseq team to decide how they want to include the `best_alignment` method for `fairseq/models/nat/nonautoregressive_transformer.py` (l. 171). Both, building a pip package and importing it or directly copying over the code from the respective repository would work. It is used for getting Viterbi-aligned target tokens when using CTC + GLAT jointly.\r\n\r\nMain flags for training using any of the above methods are:\r\n\r\n- GLAT: `--use-glat`\r\n- CTC: `--use-ctc-decoder --ctc-src-upsample-scale 2`\r\n- DS: `--use-deep-supervision`\r\n\r\nThese are also supported jointly. Once this PR has been integrated, we'll work on getting a follow-up PR up for the required inference speed improvements i.e. Shortlists and Average Attention (see below paper). As these are not specific to non-autoregressive models, we decided to keep them separate.\r\n\r\nIf anyone using this code finds it helpful, please consider citing our [associated paper](https://arxiv.org/abs/2205.10577):\r\n\r\n> **Abstract:** *Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. Consequently, their translation quality still tends to be inferior to their autoregressive counterparts due to several issues involving output token interdependence. In this work, we take a step back and revisit several techniques that have been proposed for improving non-autoregressive translation models and compare their combined translation quality and speed implications under third-party testing environments. We provide novel insights for establishing strong baselines using length prediction or CTC-based architecture variants and contribute standardized BLEU, chrF++, and TER scores using sacreBLEU on four translation tasks, which crucially have been missing as inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7 BLEU points. Our open-sourced code is integrated into fairseq for reproducibility.*\r\n\r\n```bibtex\r\n@misc{schmidt2022nat,\r\n  url = {https://arxiv.org/abs/2205.10577}, \r\n  author = {Schmidt, Robin M. and Pires, Telmo and Peitz, Stephan and LÃ¶Ã¶f, Jonas},\r\n  title = {Non-Autoregressive Neural Machine Translation: A Call for Clarity},\r\n  publisher = {arXiv},\r\n  year = {2022}\r\n}\r\n```",
    "head_branch": "nat-glat-ctc",
    "is_a_fork": true,
    "comments": [
      "Hi, thanks for this great integration of NAT codes. Could you please provide an example to show how to train a `GLAT` model?\r\n\r\n",
      "Sure, as written above, the main flag for that is `--use-glat` which will enable the glancing sampling. Given that you ran `fairseq-preprocess` and you have your data correctly set up in a folder `data-bin` you should be able to run a training run for GLAT with:\r\n\r\n```python\r\nfairseq-train data-bin --log-format simple --log-interval 100 --max-tokens 8192 --activation-fn gelu --adam-betas '(0.9, 0.98)' --apply-bert-init --arch nonautoregressive_transformer --clip-norm 5.0 --criterion nat_loss --decoder-learned-pos --dropout 0.1 --encoder-learned-pos --eval-bleu --eval-bleu-args '{\"iter_decode_max_iter\": 0, \"iter_decode_eos_penalty\": 0, \"iter_decode_with_beam\": 1}' --eval-bleu-detok moses --eval-bleu-print-samples --eval-bleu-remove-bpe --fp16 --label-smoothing 0 --length-loss-factor 0.1 --lr 0.001 --lr-scheduler inverse_sqrt --max-update 200000 --min-lr 1e-09 --noise full_mask --optimizer adam --pred-length-offset --share-all-embeddings --task translation_lev --use-glat --warmup-init-lr 1e-07 --warmup-updates 10000 --weight-decay 0.01 --share-all-embeddings\r\n```\r\n\r\nSimilarly, for vanilla CTC:\r\n\r\n```python\r\nfairseq-train data-bin --log-format simple --log-interval 100 --max-tokens 8192 --adam-betas '(0.9, 0.98)' --arch nonautoregressive_transformer --clip-norm 5.0 --criterion nat_loss --ctc-src-upsample-scale 2 --decoder-learned-pos --dropout 0.1 --encoder-learned-pos --eval-bleu --eval-bleu-args '{\"iter_decode_max_iter\": 0, \"iter_decode_eos_penalty\": 0, \"iter_decode_with_beam\": 1}' --eval-bleu-detok moses --eval-bleu-print-samples --eval-bleu-remove-bpe --fp16 --label-smoothing 0 --lr 0.001 --lr-scheduler inverse_sqrt --max-update 200000 --min-lr 1e-09 --noise full_mask --optimizer adam --share-all-embeddings --task translation_lev --use-ctc-decoder --warmup-init-lr 1e-07 --warmup-updates 10000 --weight-decay 0.01 --share-all-embeddings\r\n```\r\n\r\nAs you can see, the main flags to enable the methods are passed and can also be combined for CTC + GLAT (given that the C++ code is added as stated above):\r\n\r\n```python\r\nfairseq-train data-bin --log-format simple --log-interval 100 --max-tokens 8192 --adam-betas '(0.9, 0.98)' --arch nonautoregressive_transformer --clip-norm 5.0 --criterion nat_loss --ctc-src-upsample-scale 2 --decoder-learned-pos --dropout 0.1 --encoder-learned-pos --eval-bleu --eval-bleu-args '{\"iter_decode_max_iter\": 0, \"iter_decode_eos_penalty\": 0, \"iter_decode_with_beam\": 1}' --eval-bleu-detok moses --eval-bleu-print-samples --eval-bleu-remove-bpe --fp16 --label-smoothing 0 --lr 0.001 --lr-scheduler inverse_sqrt --max-update 200000 --min-lr 1e-09 --noise full_mask --optimizer adam --share-all-embeddings --task translation_lev --use-ctc-decoder --use-glat --warmup-init-lr 1e-07 --warmup-updates 10000 --weight-decay 0.01 --share-all-embeddings\r\n```\r\n\r\n\r\nFor some of the hyperparameter choices, please see the paper (above is for WMT'14 EN-DE) ! \r\n\r\nOf course, `max-tokens` and `lr` are a little specific to our setup (number of GPUs, batch size) and might need some tuning to most effectively utilise your available GPU resources. My guess would be that you need to reduce both of them since we train on multiple A100's and as a result our batch size is quite large. \r\n\r\nLet me know in case you run into any issues, I needed to strip a few internal flags so hopefully I didn't miss anything! ",
      "Dear Robin, thanks for your help, I have successfully finished the training process, could you kindly provide the test script?",
      "Sure, given that you have averaged your checkpoints and saved it in a file e.g. `ckpts_last_5.pt` running inference on the test set works with the following command:\r\n\r\n```\r\nfairseq-generate data-bin --path ckpts_last_5.pt --iter-decode-max-iter 0 --iter-decode-eos-penalty 0 --iter-decode-with-beam 1 --batch-size 128 --beam 1 --remove-bpe  --task translation_lev --gen-subset test \r\n```\r\n\r\nThis will generate hypothesis for the test set that you will need to score with `sacrebleu` something like this should work to generate `BLEU`, `chrF++`, and case-sensitive `TER` metrics for `sacrebleu==2.0.0`:\r\n\r\n```\r\nsacrebleu -i  test.hyp -t wmt14/full -l en-de -m bleu chrf ter --chrf-word-order 2 --ter-case-sensitive\r\n```\r\n\r\nNote that `EN-DE` and `DE-EN` use `wmt14/full` while `EN-RO` and `RO-EN` use `wmt16`.\r\n",
      "Sincerely thanksï¼",
      "No problem at all, please let me know in case you run into any issues! ",
      "How to implement --nbest?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb8ac254a19ac29c3f1",
    "number": 4418,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes failing build error.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "format_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36325742)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb9ac254a19ac29c3f2",
    "number": 4413,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes [#4412](https://github.com/pytorch/fairseq/issues/4412).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "jadekim",
    "is_a_fork": true,
    "comments": [
      "Hi @JadeKim042386! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234413). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdb9ac254a19ac29c3f3",
    "number": 4404,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dev",
    "is_a_fork": true,
    "comments": [
      "Hi @wyj-source! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234404). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbaac254a19ac29c3f4",
    "number": 4402,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-3-1",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36208103)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbbac254a19ac29c3f5",
    "number": 4401,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nBlocksparse attention no longer accepts masks.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-3",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36208195)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbcac254a19ac29c3f6",
    "number": 4387,
    "body": "# Before submitting\r\n\r\n- [ -] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n          Proposed in Github issue #4386, not yet reviewed/approved\r\n- [ - ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n          Yes, not sure if code lints though.\r\n- [ x ] Did you make sure to update the docs?\r\n           Added an appropriate feature flag, this should be sufficient.\r\n- [ x] Did you write any new necessary tests?\r\n           Yes, I added tests for the standalone correlation code.\r\n\r\n## What does this PR do?\r\nImplements #4386.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n          No doubt\r\n",
    "head_branch": "repr_compare_fc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbdac254a19ac29c3f7",
    "number": 4380,
    "body": "Fixes #4379",
    "head_branch": "fix-probably-meant-fstring",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36454259).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36454259).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36454259)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbeac254a19ac29c3f8",
    "number": 4378,
    "body": "Added pip install librosa in Additional Requirements\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4321.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @Bhavay192! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234378). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbeac254a19ac29c3f9",
    "number": 4373,
    "body": "The paths for conformer models pointed to a s3 bucket. Changing them to https to make the models downloadable.\r\n\r\n\r\n\r\n## What does this PR do?\r\nFixes #4372 \r\n\r\n## PR review\r\n@alexeib \r\n\r\n\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [
      "@sravyapopuri388 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35825311).",
      "Thanks for the PR. Importing it internally to merge it."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdbfac254a19ac29c3fa",
    "number": 4367,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nStep goes from 0 to `max_len + 1` (`for step in range(max_len + 1)`).\r\nHowever, we check `step < max_len` instead of `step < max_len + 1`\r\n\r\nThis PR is about fixing this issue.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @hoangcuong2011! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234367). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc0ac254a19ac29c3fb",
    "number": 4366,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4301.\r\n\r\n## PR review\r\nAdding T5-style pretraining task, described in https://arxiv.org/pdf/1910.10683.pdf.\r\nInspired by the [HuggingFace implementation](https://github.com/huggingface/transformers/blob/e1c153cbaa2f4dc6fa10aec8e3afb38c1b437947/examples/flax/language-modeling/run_t5_mlm_flax.py#L286).\r\n\r\n## Did you have fun?\r\nA ton!\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "add span_masked_lm task (#4366)\n\nCo-authored-by: Alexander Jipa <azzhipa@amazon.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc1ac254a19ac29c3fc",
    "number": 4359,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc2ac254a19ac29c3fd",
    "number": 4358,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs? -> will update that after initial discussion over the PR\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nPropose a shell-script recipe for speech-to-speech translation.\r\nThe speech-to-speech translation examples combined numerous components from the whole fairseq group. As noted that there are several steps (e.g., hubert unit extraction, language preparation, multi-tasking, etc.), it would be easier to reproduce the results with a recipe with all the required steps.\r\n\r\nThis PR serves as an example for building a s2st model from scratch basically following the doc, but with some additional helper scripts for data pre/post processing.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Note\r\nI found some legacy codes in the commit, will remove them shortly",
    "head_branch": "recipe",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc3ac254a19ac29c3fe",
    "number": 4351,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4302 (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nI had fun when I figured out why torchrun was failing :)\r\n",
    "head_branch": "dev/colin/infer-device-id-for-torchrun",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35784181)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc3ac254a19ac29c3ff",
    "number": 4349,
    "body": "# Before submitting\r\n\r\nCorrects some download links for new Wav2Vec2-Conformer checkpoints\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update README.md (#4349)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc4ac254a19ac29c400",
    "number": 4344,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4300\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nBig time!\r\n\r\nNote:\r\nI had to update `black` because of [this known issue](https://github.com/psf/black/issues/2964):\r\n```\r\nblack....................................................................Failed\r\n- hook id: black\r\n- exit code: 1\r\nTraceback (most recent call last):\r\n  File \"/Users/azzhipa/.cache/pre-commit/repoxt83whf2/py_env-python3.8/bin/black\", line 8, in <module>\r\n    sys.exit(patched_main())\r\n  File \"/Users/azzhipa/.cache/pre-commit/repoxt83whf2/py_env-python3.8/lib/python3.8/site-packages/black/__init__.py\", line 1423, in patched_main\r\n    patch_click()\r\n  File \"/Users/azzhipa/.cache/pre-commit/repoxt83whf2/py_env-python3.8/lib/python3.8/site-packages/black/__init__.py\", line 1409, in patch_click\r\n    from click import _unicodefun\r\nImportError: cannot import name '_unicodefun' from 'click' (/Users/azzhipa/.cache/pre-commit/repoxt83whf2/py_env-python3.8/lib/python3.8/site-packages/click/__init__.py)\r\n```\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "cc: @dianaml0 ",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35691648).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35691648)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc5ac254a19ac29c401",
    "number": 4334,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes typo\r\n\r\n## PR review\r\n\r\n\r\n## Did you have fun?\r\n",
    "head_branch": "finetune-typo",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35503972)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc6ac254a19ac29c402",
    "number": 4331,
    "body": null,
    "head_branch": "patch-4",
    "is_a_fork": true,
    "comments": [
      "@todpole3 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35423131)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc7ac254a19ac29c403",
    "number": 4330,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @lavibash! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234330). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc7ac254a19ac29c404",
    "number": 4316,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "wav2vec2_8k",
    "is_a_fork": true,
    "comments": [
      "Hi @lardontis! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234316). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc9ac254a19ac29c405",
    "number": 4315,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "8k_exp",
    "is_a_fork": true,
    "comments": [
      "Hi @lardontis! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234315). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdc9ac254a19ac29c406",
    "number": 4314,
    "body": "adding for generating biographies paper",
    "head_branch": "adding_womenbios",
    "is_a_fork": false,
    "comments": [
      "@huihuifan has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35205567).",
      "@huihuifan has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35205567)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdcaac254a19ac29c407",
    "number": 4313,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml_format_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35200613)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdcbac254a19ac29c408",
    "number": 4311,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nEnables logging of params and metrics with Aim. Aim is an open-source experiment tracker - https://github.com/aimhubio/aim\r\n\r\n\r\n1. Added two arguments to CommonConfig:\r\n- aim_repo: defines Aim repository location, can be set to remote URL as well(i.e. `aim://<ip>:<port>`)\r\n- aim_run_hash: defines run hash. If skipped, run will be created or continued based on `save_dir` argument. If there is an existing run which has the same `save_dir`, it will be reopened/continued, otherwise a new run will be created.\r\n\r\n2. Implemented AimProgressBarWrapper class to handle logging\r\n",
    "head_branch": "feature/add-aim-logging",
    "is_a_fork": true,
    "comments": [
      "Hi @gorarakelyan! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234311). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Just for some context here, our internal research project at FAIR needed to use Aim given they had logging functionality that was not available in either Tensorboard or WAndB. I reached out to the Aim team and they very quickly sent out this to PR integrating with fairseq.",
      "@ArmenAg has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35177412).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35177412).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35177412)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdccac254a19ac29c409",
    "number": 4310,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix issue with `black` causing build error.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "format_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35151101)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdcdac254a19ac29c40a",
    "number": 4305,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nFixes an unreported bug in examples.speech_to_text.data_utils.get_zip_manifest(..) (line 122) that occurs when a filename in a zip file contains non-ASCII characters (e.g., \"Ã¼\"). This occurs because the code computes the length of the filename in characters, not bytes.\r\n\r\nAlso contains formatting changes applied by black.\r\n\r\n",
    "head_branch": "get_zip_manifest_utf8_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdceac254a19ac29c40b",
    "number": 4291,
    "body": "The PR contains the biased beam search implementation based on the paper [Re-Translation Strategies For Long Form, Simultaneous, Spoken Language Translation](https://arxiv.org/abs/1912.03393)\r\n\r\n- [x ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4286",
    "head_branch": "biased-beam-search",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdceac254a19ac29c40c",
    "number": 4290,
    "body": "The PR contains the implementation of biased beam search based on the paper [Re-Translation Strategies For Long Form, Simultaneous, Spoken Language Translation](https://arxiv.org/abs/1912.03393)\r\n\r\n# Before submitting\r\n\r\n- [ x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).  [4286](https://github.com/pytorch/fairseq/issues/4286)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "biased_beam_search",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdcfac254a19ac29c40d",
    "number": 4289,
    "body": "# Before submitting\r\n\r\n- [ x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).[4286](https://github.com/pytorch/fairseq/issues/4286)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "baised_beam_search",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd0ac254a19ac29c40e",
    "number": 4288,
    "body": "# Before submitting\r\n\r\n- [ x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x] Did you make sure to update the docs?\r\n- [ x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue). [4286](https://github.com/pytorch/fairseq/issues/4286)\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "biased-beam-search",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd1ac254a19ac29c40f",
    "number": 4287,
    "body": "â€¦eous translation\r\n\r\n# Before submitting\r\n\r\n- [ x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements) yes\r\n- [ x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)? \r\n- [ x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue). https://github.com/pytorch/fairseq/issues/4286\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "biased-beam-search",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd2ac254a19ac29c410",
    "number": 4285,
    "body": "## What does this PR do?\r\n\r\nImplements multi-task classification prediction with the optional ability to ignore left out values.  To test both criterion and task should be set to `multi_sentence_prediction`.  `--num-classes = [3,3]` to the classes per prediction target, if has empty classes + 1, total number of targets `--number-of-targets = 2` and the string value of the value to ignore (not the index): `--ignore-str = -1`",
    "head_branch": "multi_sent_prediction",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd3ac254a19ac29c411",
    "number": 4278,
    "body": null,
    "head_branch": "sequence_loss_nick",
    "is_a_fork": true,
    "comments": [
      "Hi @noa! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234278). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd3ac254a19ac29c412",
    "number": 4276,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix download path download_glue_data.py to NYU official GLUE-baseline repository ( resolve #3771 ) \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nðŸ‘ðŸ¼ \r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @Sunkyoung! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234276). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd4ac254a19ac29c413",
    "number": 4264,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nUpdates the [`get_config_from_yaml`](https://github.com/pytorch/fairseq/blob/0f078de343d985e0cba6a5c1dc8a6394698c95c7/fairseq/data/audio/data_cfg.py#L12-L27) function used by the [`speech_to_text`](https://github.com/pytorch/fairseq/blob/main/fairseq/tasks/speech_to_text.py) task to use [`omegaconf`](https://github.com/omry/omegaconf):\r\n\r\n- This is the tool used by Hydra to manage configurations, which is currently used in Fairseq.\r\n- It has useful features such as [environment variable interpolation](https://omegaconf.readthedocs.io/en/2.0_branch/usage.html#environment-variable-interpolation) by using the following format: `{env: MY_ENV_VARIABLE}`. *\r\n\r\n---\r\n\r\n\\* This will need to be updated if ever Fairseq moves to omegaconf 2.1 -> https://github.com/omry/omegaconf/issues/573\r\nhttps://github.com/pytorch/fairseq/blob/0f078de343d985e0cba6a5c1dc8a6394698c95c7/setup.py#L216",
    "head_branch": "s2t-data-omegaconf",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd5ac254a19ac29c414",
    "number": 4263,
    "body": "# Before submitting\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nIt allows scoring validation outputs in [`speech_to_text`](https://github.com/pytorch/fairseq/blob/main/fairseq/tasks/speech_to_text.py) with WER, BLEU or any other [`scoring` metric](https://github.com/pytorch/fairseq/tree/main/fairseq/scoring).\r\n",
    "head_branch": "s2t-score-eval",
    "is_a_fork": true,
    "comments": [
      "Sorry for the mess with closing/opening the PR, I just wanted to change the branch name in my fork ðŸ™ "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd6ac254a19ac29c415",
    "number": 4258,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? #3169 discusses an error relating to using `self.args` instead of `self.cfg`\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? no api changes\r\n- [x] Did you write any new necessary tests? no api changes\r\n\r\n## What does this PR do?\r\nChanges `translation_from_pretrained_bart` task to use hydra cfg\r\nChanges imports from relative to absolute (`.translation` to `fairseq.tasks.translation`)\r\nUpdates call to `SequenceGenerator` to include new arg `max_len`\r\nFixes #3169 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n\r\nNote: New PR from `hydrate-bart` branch instead of `main`. See old PR #4010 ",
    "head_branch": "hydrate-bart",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd7ac254a19ac29c416",
    "number": 4249,
    "body": "## Summary\r\nOur mission at Meta Open Source is to empower communities through open source, and we believe that it means building a welcoming and safe environment for all. As a part of this work, we are adding this banner in support for Ukraine during this crisis.\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "@dmitryvinn-fb has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34635479)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd7ac254a19ac29c417",
    "number": 4247,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3892 #4001 #3880.\r\n\r\n1. For speech translation, the `fixed-pre-decision-*` models modifies the `p_choose` function, but the training [code](https://github.com/pytorch/fairseq/blob/e55e094b969df89d43e68b797707ee8d3bac1c5a/examples/simultaneous_translation/modules/monotonic_multihead_attention.py#L289) directly calls `p_choose_from_qk`, ignoring this change. This causes the speech models to underperform.\r\n2. bug in padding mask for fixed_predecision\r\n3. in `monotonic_attention_process_infer`, `p_choose_i` is selected by gathering p_choose at dimension 1 (source dimension), so the index tensor `monotonic_step` should have the shape `head x 1` instead to fit the intended usage of gather. The original dimension `1 x head` is incorrect.\r\n4. For speech, the latency loss's source lengths is incorrect, since the model may downsample the source.\r\n5. The latency loss used the same weight for avg and var.\r\n6. `-float(\"inf\")` causes nan in probability for expected alignment, changing to `-1e4` or `-1e8` fixes this.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_mma",
    "is_a_fork": true,
    "comments": [
      "Hello @kahne @xutaima, I made some fixes for prior issues and others mentioned above. Could you take a look at them and verify?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd8ac254a19ac29c418",
    "number": 4243,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n\r\nhttps://groups.google.com/g/fairseq-users/c/YoSm5J2To1A\r\n\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4242\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "add_target_dataset_eos_then_pad",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34538164)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdd9ac254a19ac29c419",
    "number": 4237,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dev/LRW",
    "is_a_fork": true,
    "comments": [
      "Hi @PeiyuChen1005! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234237). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddaac254a19ac29c41a",
    "number": 4232,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @mrm8488! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234232). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddbac254a19ac29c41b",
    "number": 4230,
    "body": "Currently loading the data2vec audio checkpoints: https://github.com/pytorch/fairseq/tree/main/examples/data2vec#speech results in a missing parameter value of the omega config.\r\n\r\nThis PR is solves the problem by adding the parameter. Not sure if this is the correct way to deal with it though. \r\n\r\ncc @alexeib ",
    "head_branch": "loading_data2vec_checkpoitns",
    "is_a_fork": true,
    "comments": [
      "ahh its because i migrated checkpoints from a different branch that had a bunch of debug options. it is better to remove this option from the checkpoint (new checkpoints wont have it)",
      "> ahh its because i migrated checkpoints from a different branch that had a bunch of debug options. it is better to remove this option from the checkpoint (new checkpoints wont have it)\r\n\r\nWe ported all the checkpoints to Hugging Face Transformers' now already (should be up by the end of this week). So this shouldn't delay the process really ;-) ",
      "@alexeib i got the same bug,why loading the data2vec audio checkpoints results in a missing parameter value of the omega config.what can i fix it."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddbac254a19ac29c41c",
    "number": 4221,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4058 \r\nWhile using the library the following warnings are shown which sometimes hinder the workflow. The warnings are \r\n\r\n`<USER_PATH>/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\r\n  beams_buf = indices_buf // vocab_size`\r\n\r\n`<USER_PATH>/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\r\n  unfin_idx = bbsz_idx // beam_size`\r\n\r\nThe methodology was simple, instead of using the `//`, it was replaced by `torch.div(arg1, arg2, rounding_mode='trunc')` and the variable alues do not change for both before and after, just the warning is resolved. \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\nYes, I did! Thanks!\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @choprahetarth! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234221). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "@soumith \r\nHi! I came across your profile while searching about Pytorch Maintainers. \r\nAs you can see that this is my first PR in this repository and I was unable to request a reviewer which I can add usually. \r\nSorry to ask you for this, but it would be really great if you could guide me what exactly should I do, since I do believe this PR can help PyTorch (I am working on an internal project, that uses fairseq, which is benefitting from this PR's change). \r\n\r\nReally looking forward to your reply\r\nThanks and Regards\r\n",
      "I'd expect someone like @alexeib or @sravyapopuri388 can help?",
      "@alexeib Can you please help me with this pull request? ",
      "@alexeib has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34538147).",
      "It seems the changes are only compatible with pytorch version 1.8 or later. Many of us are still using pytorch 1.7, so is it possible to revert these changes? cc @alexeib ",
      "@sravyapopuri388 if you want I can add an IF condition for the changes to take place only if the pytorch version is >=1.8.",
      "That would be helpful. Thanks @choprahetarth "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddcac254a19ac29c41d",
    "number": 4218,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-1",
    "is_a_fork": false,
    "comments": [
      "\"GitHub rate limited us while adding a commit status of success: API rate limit exceeded for user ID 6422482\" but looks like both circleci runs succeeded if you click on the details ",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36681610)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdddac254a19ac29c41e",
    "number": 4215,
    "body": "## What does this PR do?\r\nWhen running on a GPU, `has_pads` is a GPU Tensor. (Unless we're running with XLA.) However it is implicitly used as a Python bool for control flow; this has the unfortunate effect of recomputing it for every layer when Python implicitly calls `bool()`. The big issue is that this triggers a sync and D2H copy, even though the value doesn't change. This PR just forces evaluation at the start so we don't unnecessarily sync after each layer. I discovered this when looking at FAMBench XLMR; it's not a huge change, but it saves a couple percent which isn't bad for such a modest delta.\r\n\r\n## Did you have fun?\r\nThis one was quite tricky to actually run down, but very satisfying once I did. I'm working on better source tracking for the PyTorch profiler, so hopefully the experience will improve for users trying to understand these sorts of things in their own models.\r\n\r\nCC @Mortimerp9 @myleott",
    "head_branch": "materialize_bool",
    "is_a_fork": true,
    "comments": [
      "Hi @robieta! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234215). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddeac254a19ac29c41f",
    "number": 4212,
    "body": "Fix issue #4209 #4210 ",
    "head_branch": "xglm_class_name_and_pad_length_fix",
    "is_a_fork": true,
    "comments": [
      "@todpole3 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34208212).",
      "Thanks for the quick PR! I should also mention I had to upgrade these dependencies in my local system to get the XGLM example to work:\r\n\r\n- Sentencepiece: 0.1.85 => 0.1.96\r\n- Numpy: 1.20.3 => 1.22.2\r\n\r\nMaybe that can be mentioned on the XGLM readme?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bddfac254a19ac29c420",
    "number": 4206,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nBuild wheels was broken.\r\n\r\n- [x] get build wheels to work successfully\r\n- [x] update the fairseq version\r\n- [x] Bump up the python version\r\n- [x] skip musllinux builds\r\nfollowup improvements incoming in another PR\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "v0.11.0",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36880560).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D36880560)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde0ac254a19ac29c421",
    "number": 4205,
    "body": "Summary:\n# Before submitting\n\n## What does this PR do?\nOpen sources a recent paper from FAIR: https://arxiv.org/abs/2107.06955\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2440\n\nDifferential Revision: D31566331\n\nPulled By: ArmenAg\n\n",
    "head_branch": "export-D31566331",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D31566331](https://www.internalfb.com/diff/D31566331)",
      "@ArmenAg Hello, I'm really interested in this paper! Is threre any plan to release the model and the code for HTLM? Thank you in advance!",
      "Hi @ArmenAg! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@meta.com](mailto:cla@meta.com?subject=CLA%20for%20facebookresearch%2Ffairseq%20%234205). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde1ac254a19ac29c422",
    "number": 4203,
    "body": "Is it possible to remove this unused argument \"target_list\"?\r\nor are there any special reasons so we have to keep it?",
    "head_branch": "unused_arguments_remove",
    "is_a_fork": true,
    "comments": [
      "Hi @glynpu! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234203). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde1ac254a19ac29c423",
    "number": 4197,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, **doc improvements**)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFix a typo in the commandline in the document\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde2ac254a19ac29c424",
    "number": 4189,
    "body": "The script with relative paths raises an error of \"No such file or directory: data-bin/wikitext-103/dict.txt\" in `fairseq-hydra-train`.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, **doc improvements**)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nThe script with relative paths raises an error of \"No such file or directory: data-bin/wikitext-103/dict.txt\" in `fairseq-hydra-train`.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Thanks, this saved me some time!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde3ac254a19ac29c425",
    "number": 4184,
    "body": "# Before submitting\r\n\r\n- [] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [] Did you make sure to update the docs?\r\n- [] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdded incremental TTS example on LJSpeech\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "incremental-TTS",
    "is_a_fork": true,
    "comments": [
      "Hi @dannigt! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234184). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde4ac254a19ac29c426",
    "number": 4180,
    "body": "Best metric is now only logged for the first of all the validation subsets\r\n\r\n# Before submitting\r\n\r\n- [x ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n  https://groups.google.com/g/fairseq-users/c/7nk3rJmvlg8\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4162\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dont_track_best_test_loss",
    "is_a_fork": true,
    "comments": [
      "Hi @shalymin-amzn! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234180). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "@alexeib CLA is now sorted, could you please advise regarding the tests?",
      "@alexeib has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D34365416)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde5ac254a19ac29c427",
    "number": 4174,
    "body": "# Before submitting\r\n\r\n- [No] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [Yes, comments around changes and help for the new flag] Did you make sure to update the docs?\r\n- [Yes, the wikitext-103 language modelling example under scalar quant noise can now be used with the `--arch` hf_gpt2 ] Did you write any new necessary tests?\r\n\r\n```\r\nBASE_DIR=<BASE_DIR>\r\nMAX_TOKENS=1024\r\nCUDA_VISIBLE_DEVICES=0,1 fairseq-train \\\r\n  --task language_modeling $BASE_DIR/data-bin/wikitext-103 \\\r\n  --save-dir $BASE_DIR/hf_gpt2_wikitext-103/checkpoints \\\r\n  --arch hf_gpt2 \\\r\n  --dropout 0.1 \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' \\\r\n  --weight-decay 0.01 --clip-norm 0.0 \\\r\n  --lr 0.0005 --lr-scheduler inverse_sqrt \\\r\n  --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n  --tokens-per-sample 512 --sample-break-mode none \\\r\n  --max-tokens $MAX_TOKENS --update-freq 16 \\\r\n  --fp16 \\\r\n  --max-update 5000000 \\\r\n  --tensorboard-logdir $BASE_DIR/hf_gpt2_wikitext-103/tensorboard \\\r\n  --no-epoch-checkpoints \\\r\n  --quant-noise-scalar 0.5\r\n```\r\n\r\nAlso, as an experiment we tried setting up 2 (almost) identical `transformer_lm` and `hf_gpt2` tiny models and trained them on the same dataset for 100k epochs.\r\n \r\nWe see that the models with noise have slightly worse validation losses than those without noise, and also that they take slightly longer to train: \r\n\r\n![tlm_w_wo_noise](https://user-images.githubusercontent.com/34727495/151883091-5ac599be-2c2f-4132-9259-96f78bad09f6.png)\r\n\r\n![hf_gpt2_w_wo_noise](https://user-images.githubusercontent.com/34727495/151883106-4beac6dc-0120-4b90-bdcd-e92fdeb509bf.png)\r\n\r\n## What does this PR do?\r\nThis PR adds support for (scalar) quantization aware training of the `hf_gpt2` model (which is a wrapper around the hugginface `GPT2LMHead` model ).\r\n\r\nWe add the `--quant-noise-scalar` flag as an argument to the `hf_gpt2` model (as is typically used for scalar noise in fairseq models). The model then gets quantized during the `<fairseq_task>.build_model()` phase.\r\n\r\nThe quantization replaces (in-place) existing modules with their quantized counterparts. To that end, we introduce a new quantized module `qconv1dhf` which replaces the `conv1d` modules in the hf_gpt2 model (similar to what `qlinear` does for the `nn.Linear` module).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nOfcourse !\r\n",
    "head_branch": "fairseq_qat",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde6ac254a19ac29c428",
    "number": 4172,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [
      "@todpole3 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33911169)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde6ac254a19ac29c429",
    "number": 4163,
    "body": "Best metric is now only logged for the first of all the validation subsets\r\n\r\n# Before submitting\r\n\r\n- [x ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n  * https://groups.google.com/g/fairseq-users/c/7nk3rJmvlg8\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #4162\r\n\r\n[2022-01-18 05:41:02,006][fairseq_cli.train][INFO] - begin validation on \"test\" subset\r\n[2022-01-18 05:43:26,995][test][INFO] -\r\n {\"epoch\": 150, \"test_loss\": \"24.118\", \"test_ntokens\": \"674.778\", \"test_nsentences\": \"13.3364\",\r\n  \"test_nll_loss\": \"0.477\", \"test_uer\": \"7.599\", \"test_wer\": \"22.706\", \"test_raw_wer\": \"22.706\",\r\n  \"test_wps\": \"1999.9\", \"test_wpb\": \"674.8\", \"test_bsz\": \"13.3\", \"test_num_updates\": \"20000\",\r\n  \"test_best_wer\": \"14.46\"}\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dont_track_best_test_loss",
    "is_a_fork": true,
    "comments": [
      "Hi @ishalyminov! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234163). Thanks!",
      "looks good to me, thanks! if you sign the CLA then I can merge it",
      "also plz black-format your changes. thanks!",
      "Done formatting. Please bear with me for a little while regarding the CLA",
      "@alexeib PR moved here: https://github.com/pytorch/fairseq/pull/4180 - new github username, CLA on that one should be updated in a centralized way, communication sent to cla@fb.com."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde7ac254a19ac29c42a",
    "number": 4158,
    "body": "1. Add XGLM pre-training data format explanation\r\n2. Add back pointer to pre-print",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "@todpole3 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33825440)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde8ac254a19ac29c42b",
    "number": 4155,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue [4148](https://github.com/pytorch/fairseq/issues/4148)) .\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @codecaution! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234155). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bde9ac254a19ac29c42c",
    "number": 4154,
    "body": "1. Add XGLM downstream task evaluation examples\r\n2. Add bibtex citation of XGLM arXiv paper\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@todpole3 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33748846)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdeaac254a19ac29c42d",
    "number": 4138,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n    I tried, but it's been in triage for a while:\r\n    https://github.com/pytorch/fairseq/issues/3962\r\n\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n\r\n- [ ] Did you make sure to update the docs?\r\n    I couldn't find any existing manifest instructions to update, or a particularly good place to do so\r\n\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nIn this pull request, offset and number of frames can be optionally added to the path column of a manifest for .wav files, following the precedent of a similar feature for .zip files.\r\n\r\nPreviously, fairseq assumed .wav files would always be read in their entirety. This assumes that audio is already segmented prior to training, which can be very time- and space-consuming, especially when a data set's segmentation isn't fixed. It also opens up opportunities to quickly re-segment audio files which are longer than `max_sample_size` (which are otherwise truncated by fairseq).\r\n\r\nExample manifest:\r\n_(Note: To preserve backward compatibility, the final column the manifest does not affect how files are loaded.)_\r\n\r\n```\r\n/audio/files\r\n/path/to/audio1.wav:0:16000    16000              # (new format) audio1.wav, frames 0 through 16000\r\n/path/to/audio1.wav:32000:16000    16000          # (new format) audio1.wav, frames 32000 through 48000\r\n/path/to/audio2.wav    16000                      # (existing format, bw compatible) audio2.wav, frames 0 through end\r\n/path/to/collection1.zip:5000:10000    64000      # (existing format, bw compatible) collection1.zip, bytes 5000 through 10000, frames 0 through end  \r\n```\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nâœ”\r\n",
    "head_branch": "data-load-offsets-and-lengths",
    "is_a_fork": true,
    "comments": [
      "Hi @rcgale! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234138). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "Anyone get a chance to review this? I'm using these changes for the baseline model in a shared task / challenge, and it would be really nice to share code referencing official fairseq repo rather than my fork of it"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdeaac254a19ac29c42e",
    "number": 4135,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "modified_for_logging",
    "is_a_fork": true,
    "comments": [
      "Hi @Nima-sheikh! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234135). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdebac254a19ac29c42f",
    "number": 4132,
    "body": "Add \"import LegacyFairseqTask\" to tutorial script : 3.registering a new task .\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdecac254a19ac29c430",
    "number": 4129,
    "body": "# Before submitting\r\n\r\n- [ x ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ x ] Did you make sure to update the docs?\r\n- [ x ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nUpdate commands, checkpoints and contact info.\r\n\r\n## PR review\r\n\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "cc @koustuvsinha ",
      "@shruti-bh has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33556233)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdedac254a19ac29c431",
    "number": 4092,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "moe",
    "is_a_fork": true,
    "comments": [
      "Hi @Raphael-Hao! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234092). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdedac254a19ac29c432",
    "number": 4091,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdded some additional documentation to make the code example more clear\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdeeac254a19ac29c433",
    "number": 4081,
    "body": "# Before submitting\r\n\r\n- [No ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [Yes] Did you make sure to update the docs?\r\n- [No ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).When training with multiple GPU, the value of meters[\"_bleu_sys_len\"].sum  and meters[\"_bleu_ref_len\"].sum is float, while compute_blue function requires an integer input\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "compute-bleu-fixes",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdefac254a19ac29c434",
    "number": 4071,
    "body": "The num_bad_epochs state is required to resume consistent training in\r\nReduceLROnPlateau LR Scheduler.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdds the `num_bad_epochs` to the `ReduceLROnPlateauLRSchedule` 's state_dict to resume consistent training.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-lr_sched-state_dict",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf0ac254a19ac29c435",
    "number": 4069,
    "body": "Draft summary.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? Will do!\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs? WIll do!\r\n- [ ] Did you write any new necessary tests? Will do!\r\n\r\n## What does this PR do?\r\n\r\nAdds Comet experiment management logger.\r\n\r\n## PR review\r\nWill bring up in a discussion.\r\n\r\n## Did you have fun?\r\n\r\nYes, thank you :) ",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf1ac254a19ac29c436",
    "number": 4059,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf2ac254a19ac29c437",
    "number": 4058,
    "body": "Fixes warning: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Failures look unrelated."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf2ac254a19ac29c438",
    "number": 4054,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "loading_frm_hf_hub",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf3ac254a19ac29c439",
    "number": 4053,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "pytorch-main",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf4ac254a19ac29c43a",
    "number": 4052,
    "body": "Follow FSDP tutorial, fairseq crashed when set --optimizer=cpu_adam, because new deepspeed's create_adam_optimizer add an argument 'should_log'.\r\n\r\nissue: https://github.com/pytorch/fairseq/issues/3810#issue-975829307\r\n\r\noriginal error message:\r\nTypeError:\r\ncreate_adam(): incompatible function arguments. The following argument types are supported:\r\n    1. (arg0: int, arg1: float, arg2: float, arg3: float, arg4: float, arg5: float, arg6: bool, arg7: bool) -> int",
    "head_branch": "fix_crash_cpu_adam",
    "is_a_fork": true,
    "comments": [
      "Hi @alayamanas! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234052). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf5ac254a19ac29c43b",
    "number": 4043,
    "body": "This PR fixes #4035 \r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @ftshijt! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234043). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@arbabu123 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D32741046).",
      "Are there any other issues related to the PR? I'm happy to modify the PR if you have any other suggestions.",
      "@ftshijt sorry I missed this. it looks good. i will merge this soon. thanks",
      "Hi @arbabu123 , may I know why the PR is closed? In fact, we have the dependency of fairseq in our toolkit, and this mismatch breaks our CI tests for a while, so we can only depend on the older version. We really want to keep updated with the latest repo, it would be very helpful to fix the issue",
      "@ftshijt sorry i didn't understand the issue. The fix has been landed and merged to master. What's the mismatch that you are talking about? What is the error that you are getting?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf6ac254a19ac29c43c",
    "number": 4025,
    "body": "If the model is traced when self._future_mask was already initialized,\r\nit would not be initialized during the trace, which results in an\r\nincorrect ONNX model.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nFixes #4026",
    "head_branch": "buffered-future-mask-onnx-trace",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf6ac254a19ac29c43d",
    "number": 4019,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf7ac254a19ac29c43e",
    "number": 4014,
    "body": "## What does this PR do?\r\nFixes typo in args\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @rajarsheem! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234014). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf8ac254a19ac29c43f",
    "number": 4010,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? #3169 discusses an error relating to using `self.args` instead of `self.cfg`\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? no api changes\r\n- [x] Did you write any new necessary tests? no api changes\r\n\r\n## What does this PR do?\r\nChanges `translation_from_pretrained_bart` task to use hydra cfg\r\nChanges imports from relative to absolute (`.translation` to `fairseq.tasks.translation`)\r\nUpdates call to `SequenceGenerator` to include new arg `max_len`\r\nFixes #3169 \r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Closing in favour of identical PR from `hydrate-bart` branch instead of `main` #4258  "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdf9ac254a19ac29c440",
    "number": 4009,
    "body": "Summary: add wmt21 models and scripts\n\nReviewed By: huihuifan\n\nDifferential Revision: D32311009\n\n",
    "head_branch": "export-D32311009",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D32311009](https://www.internalfb.com/diff/D32311009)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D32311009](https://www.internalfb.com/diff/D32311009)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D32311009](https://www.internalfb.com/diff/D32311009)",
      "This pull request has been merged in pytorch/fairseq@47c58f0858b5484a18f39549845790267cffee1a."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfaac254a19ac29c441",
    "number": 4007,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\nAs discussed offline with @kahne this is a proposal of how an integration with the :hugs: Hub could look like.\r\nThe main purpose of this integration is to be able to better demo fairseq models on the :hugs: Hub for inference.\r\nI.e. so that [`fairseqs` speech synthesis models](https://github.com/pytorch/fairseq/tree/main/fairseq/models/text_to_speech) could be easily loaded from the :hugs: hub.\r\n\r\nThis is just a rough first draft and I would be very happy any suggestions how this PR could be improved :-)\r\n\r\n## PR review\r\n@kahne \r\n\r\n",
    "head_branch": "add_load_from_hf_func",
    "is_a_fork": true,
    "comments": [
      "@kahne has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D32697723).",
      "As an example, I've uploaded the [`fastspeech2` model weights](https://github.com/pytorch/fairseq/blob/main/examples/speech_synthesis/docs/ljspeech_example.md#results) to the hub [here](https://huggingface.co/patrickvonplaten/fairseq-fastspeech2).\r\n\r\nThose weights can now be easily downloaded and cached via:\r\n\r\n```python\r\nfrom fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf\r\n\r\nmodel = load_model_ensemble_and_task_from_hf(\"patrickvonplaten/fairseq-fastspeech2\")\r\n```",
      "@kahne - this can be closed as it's already merged no? :-)",
      "> @kahne - this can be closed as it's already merged no? :-)\r\n\r\nYes, sounds good :)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfbac254a19ac29c442",
    "number": 4004,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "xliu1231",
    "is_a_fork": true,
    "comments": [
      "Hi @xliu1231! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%234004). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfbac254a19ac29c443",
    "number": 3998,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nEnables audio padding rather than cropping for HuBERT finetuning and decoding (it shouldn't crop the audio during CTC finetuning, otherwise we may lose audio corresponding to some part of the transcript). Also fixes the issue that the decoding results of HuBERT  models are non-deterministic across different runs of inference with exactly the same setting, by disabling random_crop during inference (also disable it for the fine-tuning stage, as there seems no point doing wave crop for fine-tuning, which is also consistent with wav2vec2).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix2",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 Could you also please consider this fix?",
      "@hirofumi0810 could you please do a review for this?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfcac254a19ac29c444",
    "number": 3996,
    "body": "- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nSlowMo is being moved to [Fairscale](https://fairscale.readthedocs.io/en/latest/). This commit updates the implementation of SlowMo to the Fairscale version. It also adds tests for SlowMo.\r\nNote: This PR is currently for review. It will be merged at a later date once SlowMo has been updated to Fairscale. SlowMo is being merged to Fairscale as part of [a PR](https://github.com/facebookresearch/fairscale/pull/378). So, once that PR is merged to Fairscale, this PR on Fairseq will be ready for merge\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fairscale_slowmo",
    "is_a_fork": true,
    "comments": [
      "Thanks for the review! I'll wait for [SlowMo to be merged into Fairscale](https://github.com/facebookresearch/fairscale/pull/378) before going ahead with merging this PR.",
      "@vtantia has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D32280163).",
      "@vtantia merged this pull request in pytorch/fairseq@3a5838c320c5b7afc3a6fba5736bca22503ef804."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfdac254a19ac29c445",
    "number": 3992,
    "body": "Summary:\n## What does this PR do?\nAdd a readme pointing to the separate repo for the Textless NLP milestone https://github.com/fairinternal/fairseq-py/issues/2: speech resynthesis\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/2537\n\nReviewed By: adiyoss\n\nDifferential Revision: D32068424\n\nPulled By: wnhsu\n\n",
    "head_branch": "export-D32068424",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D32068424](https://phabricator.intern.facebook.com/D32068424)",
      "@wnhsu merged this pull request in pytorch/fairseq@c6360917ff93acebecfe09b28461166006b0c653."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdfeac254a19ac29c446",
    "number": 3989,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?\r\n- [X] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes issue where fairseq training hangs when using multiple workers. When one of the workers gets a OOM error, it tries to recover with a `try ... except` statement, but the other workers keep waiting for it, and they get out of sync.\r\n",
    "head_branch": "oom_fix",
    "is_a_fork": true,
    "comments": [
      "I am hitting the same issue on distributed jobs.\r\n\r\nWith your change, now my job ends when I hit an OOM. But I thought the correct behavior is to handle the OOM gracefully and continue training. Did I not understand the correct behavior properly?",
      "I agree that would be preferable behavior @tberckmann, but I'd rather my jobs terminate than enter an infinite loop. That would be a great follow-up PR.",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdffac254a19ac29c447",
    "number": 3984,
    "body": "## What does this PR do?\r\nUpdates the bibtex to cite the mtedx dataset in the speech-to-text example, now that mtedx has been published (Interspeech 2021)",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "@kahne has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D32102153).",
      "@kahne merged this pull request in pytorch/fairseq@d792b793a777bf660d2aaeb095c2381af189e626."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bdffac254a19ac29c448",
    "number": 3975,
    "body": "## What does this PR do?\r\nWhen translating using a model from hub_utils in the production setting, if skip_invalid_size_inputs is set to True, then the output list of translations may be different in length from the input list of strings.\r\n\r\nThese changes enforce empty responses in case of failure instead of no responses which will lead to mismatching the input sentences from the hyp outputs when using batching. ",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be00ac254a19ac29c449",
    "number": 3966,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "Hi @khalidkahloot! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233966). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "What is needed to be done? Please tell me!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be01ac254a19ac29c44a",
    "number": 3961,
    "body": "added support for python 3.9\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdded support for python 3.9 in setup.py\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be02ac254a19ac29c44b",
    "number": 3953,
    "body": "hasattr should not default to True if field doesn't exist\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "diana_fix",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31591929)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be02ac254a19ac29c44c",
    "number": 3943,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nDocs improved.\r\nThere was some issues with reproducing NAT papers results.\r\n\r\nhttps://github.com/pytorch/fairseq/issues/1231#issuecomment-543000324\r\nhttps://github.com/pytorch/fairseq/issues/2187#issuecomment-819246459\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "nat_iterative_refinement_note",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be03ac254a19ac29c44d",
    "number": 3940,
    "body": "# Before submitting\r\n\r\n- [-] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3939\r\nFixes #3941\r\n\r\n## PR review\r\nAwaiting review.\r\n\r\n## Did you have fun?\r\nOf course!\r\n",
    "head_branch": "joint_s2t_fixes",
    "is_a_fork": true,
    "comments": [
      "@kahne has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D32102157).",
      "@kahne merged this pull request in pytorch/fairseq@e69a7c1d8a7ca09435f6406c4ef6aaa63c9a093c."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be04ac254a19ac29c44e",
    "number": 3937,
    "body": "Make label_rate be of type float in Hubert pretraining to support decimal label rate (e.g. 33.3Hz, otherwise verify_label_lengths() will give warnings if the undelying label rate is 33.3Hz)\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix5",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "@dianaml0 Any chances to merge this?",
      "@freewym I'm just rerunning the checks and if everything passes I'll merge! Any chance you could update the PR description?",
      "Thanks for your reply. I think my current description is shown as \"make label_rate be of type float in Hubert pretraining to support decimal label rate (e.g. 33.3Hz, otherwise verify_label_lengths() will give warnings if the undelying label rate is 33.3Hz)\"\r\n\r\nWhat do you want me to change?",
      "> Thanks for your reply. I think my current description is shown as \"make label_rate be of type float in Hubert pretraining to support decimal label rate (e.g. 33.3Hz, otherwise verify_label_lengths() will give warnings if the undelying label rate is 33.3Hz)\"\r\n> \r\n> What do you want me to change?\r\n\r\n@freewym no worries, I updated the title and description",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "@dianaml0 Do we need to run another round of test? I notice that the lint error has been fixed in main.",
      "> @dianaml0 Do we need to run another round of test? I notice that the lint error has been fixed in main.\r\n\r\n@freewym yes thanks, just updated the branch so tests should rerun",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "@dianaml0 Failed on python 3.8... Probably just a bad luck and needs a re-run",
      "@freewym just triggered a re-run",
      "@dianaml0  The internal tests failed. Not sure if it's related.",
      "@freewym I'm rerunning the internal tests now as well",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "> @freewym I'm rerunning the internal tests now as well\n\n@dianaml0 how was the tests going?",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119).",
      "@dianaml0 has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31489119)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be05ac254a19ac29c44f",
    "number": 3934,
    "body": "# Before submitting\r\n\r\n- [ X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ X] Did you make sure to update the docs?\r\n\r\n## What does this PR do?\r\nFixes #3870",
    "head_branch": "gslm_fix_tts_code_dict",
    "is_a_fork": true,
    "comments": [
      "@wnhsu I am not able to add you as reviewer. Can you please review the change? Thanks!",
      "@wnhsu has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31503773)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be06ac254a19ac29c450",
    "number": 3926,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "effward/investigate-load-model-ensemble",
    "is_a_fork": true,
    "comments": [
      "Hi @effward! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233926). Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be07ac254a19ac29c451",
    "number": 3922,
    "body": "Loading xlmr doesn't work because trying to pull from master branch, which has been changed to main. \r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31415771)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be07ac254a19ac29c452",
    "number": 3921,
    "body": "Bug introduced in https://github.com/pytorch/fairseq/commit/d974c709bf57cf494738a824a1597e1886bebb7a I believe.",
    "head_branch": "jmp84-patch-2",
    "is_a_fork": false,
    "comments": [
      "@jmp84 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31296530).",
      "It may not be a bug.\r\n\r\nI followed the instructions in [S2T Example: Speech Translation (ST) on MuST-C](https://github.com/pytorch/fairseq/blob/main/examples/speech_to_text/docs/mustc_example.md), and run this command:\r\n```\r\npython examples/speech_to_text/prep_mustc_data.py \\\r\n  --data-root ${MUSTC_ROOT} --task asr \\\r\n  --vocab-type unigram --vocab-size 5000\r\n```\r\n\r\nIn the latest version, I got errors:\r\n```\r\nFetching split train...\r\nExtracting log mel filter bank features...\r\n  0%|                                                                                                                              | 0/229703 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/qwang/.tools/fairseq/examples/speech_to_text/prep_mustc_data.py\", line 294, in <module>\r\n    main()\r\n  File \"/home/qwang/.tools/fairseq/examples/speech_to_text/prep_mustc_data.py\", line 290, in main\r\n    process(args)\r\n  File \"/home/qwang/.tools/fairseq/examples/speech_to_text/prep_mustc_data.py\", line 141, in process\r\n    features = extract_fbank_features(\r\n  File \"/home/qwang/.tools/fairseq/examples/speech_to_text/data_utils.py\", line 90, in extract_fbank_features\r\n    features = _get_torchaudio_fbank(_waveform, sample_rate, n_mel_bins)\r\n  File \"/home/qwang/.tools/fairseq/fairseq/data/audio/audio_utils.py\", line 153, in _get_torchaudio_fbank\r\n    features = ta_kaldi.fbank(\r\n  File \"/home/qwang/.miniconda/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py\", line 561, in fbank\r\n    waveform, window_shift, window_size, padded_window_size = _get_waveform_and_window_properties(\r\n  File \"/home/qwang/.miniconda/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py\", line 139, in _get_waveform_and_window_properties\r\n    waveform = waveform[channel, :]  # size (n)\r\nIndexError: too many indices for tensor of dimension 1\r\n```\r\n\r\nBy replacing `_waveform = _waveform[0].numpy()` with `_waveform = _waveform.numpy()` (simply reverting this commit), the error disappear!\r\n\r\n### Environment:\r\nPytorch=1.10.1, py3.9_cuda11.3_cudnn8.2.0_0\r\nfairseq: 1b61bbad327d2bf32502b3b9a770b57714cc43dc (install from source)\r\ntorchaudio: 0.10.1+cu113\r\n\r\n\r\n**I did not install pykaldi,** and found the [following lines](https://github.com/pytorch/fairseq/blob/1b61bbad327d2bf32502b3b9a770b57714cc43dc/examples/speech_to_text/data_utils.py#L88):\r\n```\r\n    features = _get_kaldi_fbank(_waveform, sample_rate, n_mel_bins)\r\n    if features is None:\r\n        features = _get_torchaudio_fbank(_waveform, sample_rate, n_mel_bins)\r\n```\r\n\r\nIn my settings, my code calls the function `_get_torchaudio_fbank` which fits with `_waveform = _waveform.numpy()`. \r\n\r\nThis commit use  `_waveform = _waveform[0].numpy()`, and it may work with `_get_kaldi_fbank ` I guess.\r\n\r\nCan anyone recheck this commit and test with the two cases torchaudio fbank and kaldi fbank please?\r\n",
      "@voidmagic the discussion in https://github.com/pytorch/fairseq/pull/3887 looks relevant. cc @kahne ",
      "@voidmagic Thanks very much for reporting! As discussed in the relevant PR that @jmp84 pointed out, this is a bug and we will include the fix in our next batch of bug fixes (coming next Monday)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be08ac254a19ac29c453",
    "number": 3917,
    "body": "Fixes builds of lightconv and dynamicconv layers.",
    "head_branch": "lightconv_fix",
    "is_a_fork": true,
    "comments": [
      "Hi @taras-sereda! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233917). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be09ac254a19ac29c454",
    "number": 3912,
    "body": "\"optimizer\" and \"extra_state\" are not required in inference state.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\n\r\nThis PR makes `_upgrade_state_dict` more robust.\r\n\r\nIn inference step, we can export a small `.pt` file with the following script.\r\n```\r\nmodel = torch.load(\"checkpoint_best.pt\")\r\nnew_model = {\"model\": model[\"model\"], \"args\": model[\"args\"]}\r\ntorch.save(new_model, \"checkpoint_for_inference.pt\")\r\n```\r\n\r\nthen fairseq-generate with the small model file.\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-27",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0aac254a19ac29c455",
    "number": 3909,
    "body": "<img width=\"1062\" alt=\"Screen Shot 2021-09-24 at 11 01 50 PM\" src=\"https://user-images.githubusercontent.com/1160355/134755798-04b22ff7-dea5-485c-8a93-d57da01ad258.png\">\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0bac254a19ac29c456",
    "number": 3905,
    "body": "Regarding this [code line 62](https://github.com/pytorch/fairseq/blob/fcca32258c8e8bcc9f9890bf4714fa2f96b6b3e1/fairseq/models/bart/hub_interface.py#L62) in the `encode` function in BART hub_interface, in many cases (e.g. using a monolingual vocabulary reduced from an existing multilingual one), an OOV token should be aligned with `<unk>` index, rather than always being added as a new token type into the vocabulary. \r\n\r\nRecent code: https://github.com/pytorch/fairseq/blob/main/fairseq/models/bart/hub_interface.py\r\n\r\n```python\r\n    def encode(\r\n        self, sentence: str, *addl_sentences, no_separator=True\r\n    ) -> torch.LongTensor:\r\n        tokens = self.bpe.encode(sentence)\r\n        if len(tokens.split(\" \")) > min(self.max_positions) - 2:\r\n            tokens = \" \".join(tokens.split(\" \")[: min(self.max_positions) - 2])\r\n        bpe_sentence = \"<s> \" + tokens + \" </s>\"\r\n        for s in addl_sentences:\r\n            bpe_sentence += \" </s>\" if not no_separator else \"\"\r\n            bpe_sentence += \" \" + self.bpe.encode(s) + \" </s>\"\r\n        tokens = self.task.source_dictionary.encode_line(bpe_sentence, append_eos=False) # Always add OOV token as new type\r\n        return tokens.long()\r\n```\r\n\r\nSuggest to be as follows (https://github.com/datquocnguyen/fairseq/blob/main/fairseq/models/bart/hub_interface.py):\r\n\r\n```python\r\n    def encode(\r\n        self, \r\n        sentence: str, \r\n        *addl_sentences, \r\n        no_separator=True,\r\n        add_if_not_exist=True # Add an extra option\r\n    ) -> torch.LongTensor:\r\n        tokens = self.bpe.encode(sentence)\r\n        if len(tokens.split(\" \")) > min(self.max_positions) - 2:\r\n            tokens = \" \".join(tokens.split(\" \")[: min(self.max_positions) - 2])\r\n        bpe_sentence = \"<s> \" + tokens + \" </s>\"\r\n        for s in addl_sentences:\r\n            bpe_sentence += \" </s>\" if not no_separator else \"\"\r\n            bpe_sentence += \" \" + self.bpe.encode(s) + \" </s>\"\r\n        tokens = self.task.source_dictionary.encode_line(\r\n            bpe_sentence, append_eos=False, add_if_not_exist=add_if_not_exist\r\n        )\r\n        return tokens.long()\r\n```\r\n\r\nWith this suggested code, in the case mentioned above, encoding should be `fairseq_model.encode(sentence, add_if_not_exist=False)`\r\n\r\nFor mBART and the like, it still encodes and adds extra token types into the vocabulary (e.g. training for new languages) as before: `fairseq_model.encode(sentence)`\r\n\r\n\r\n",
    "head_branch": "main",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0bac254a19ac29c457",
    "number": 3900,
    "body": "One instance of thresholdsrc uses $min_phones, the other does not. \r\nTherefore any word that shows up less than 2 times does not end up in dict.txt, causing any sentence containing said words to be rejected. \r\nThis causes trouble especially for small datasets.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nFixes #3901\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.\r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @cdleong! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233900). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0cac254a19ac29c458",
    "number": 3895,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements) \r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?\r\n- [ ] Did you write any new necessary tests?\r\n\r\nSummery:\r\n- Using OmegaConf to convert checkpoint[\"cfg\"] dict in speech-recognition's infer\r\n- Updated to f34abcf2\r\n\r\n## What does this PR do?\r\nFixes #2838,  resolves #3488 \r\n\r\nThe `cfg` in the [checkpoint = torch.load(args.kenlm_model, map_location=\"cpu\")](https://github.com/pytorch/fairseq/blob/9549e7f76994095c92441b81c615a169dc21f478/examples/speech_recognition/w2l_decoder.py#L381) is a dict. However, it is treated as a OmegaConf. Just followed lines 322-334 of fairseq.checkpoint_utils to fix it",
    "head_branch": "bugfix/FairSeqLM-torch-load-2838",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0dac254a19ac29c459",
    "number": 3894,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3892.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@xutaima Can you also have a look at this PR, please? I guess it solves a bug related to inference time.",
      "Hi @George0828Zhang , I was looking at your PR and realised that while your commit passes `incremental_state` variable to ```p_choose_from_qk``` function [[here](https://github.com/pytorch/fairseq/pull/3894/commits/87270e61088346133594665407789a8adcead879#diff-3a984eb58cec961ea177f2d1f9c62e28f621051b28a8ab7c41ba2ffb2d7a5de2R134)]  , the variable is still not used inside ```p_choose_from_qk```. Isn't that mean basically it changes nothing or do I miss something here? @xutaima can you let us know what you think about this PR request in general?\r\n\r\n",
      "> Hi @George0828Zhang , I was looking at your PR and realised that while your commit passes `incremental_state` variable to `p_choose_from_qk` function [[here](https://github.com/pytorch/fairseq/pull/3894/commits/87270e61088346133594665407789a8adcead879#diff-3a984eb58cec961ea177f2d1f9c62e28f621051b28a8ab7c41ba2ffb2d7a5de2R134)] , the variable is still not used inside `p_choose_from_qk`. Isn't that mean basically it changes nothing or do I miss something here? @xutaima can you let us know what you think about this PR request in general?\r\n\r\nWell, incremental_state is only used by waitk policy. The waitk's [p_choose_from_qk](https://github.com/George0828Zhang/fairseq/blob/4804e73d86c99ab82d479c9168a7be864dbd6702/examples/simultaneous_translation/modules/monotonic_multihead_attention.py#L479) will pass the incremental_state to be used at [here](https://github.com/pytorch/fairseq/blob/fcca32258c8e8bcc9f9890bf4714fa2f96b6b3e1/examples/simultaneous_translation/utils/p_choose_strategy.py#L19). For MMA that change does nothing."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0eac254a19ac29c45a",
    "number": 3891,
    "body": "# Before submitting\r\n\r\n- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [Y] Did you make sure to update the docs?   \r\n- [N] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nAdd codes for language-agnostic constraint for SwAV loss, which can be used to mine pseudo-parallel data for unsupervised machine translation.\r\n\r\nThe codes are stored in `examples/swav_project`. There is no modification to the original codebase on master.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "swav_umt",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0fac254a19ac29c45b",
    "number": 3887,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3882\r\nFixes #3884 \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-prep-mustc",
    "is_a_fork": true,
    "comments": [
      "Hi @kahne, #3882 was **partially** closed in #3921. However, I think the solution I propose here is cleaner, and it also considers some problems with `--use-audio-input`(#3882 & #3884).\r\nAre you planning to merge it some day? Thank you.",
      "@kahne has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D33152073).",
      "> Hi @kahne, #3882 was **partially** closed in #3921. However, I think the solution I propose here is cleaner, and it also considers some problems with `--use-audio-input`(#3882 & #3884). Are you planning to merge it some day? Thank you.\r\n\r\nThanks! I've imported it for internal code review.",
      "Hi, @kahne , this commit introduces a small bug [#98ebe4f\r\n](https://github.com/pytorch/fairseq/commit/98ebe4f1ada75d006717d84f9d603519d8ff5579)\r\n\r\nthe following line:https://github.com/pytorch/fairseq/blob/87d0ede93c06f564b7b985c1a9a0489ae946313d/examples/speech_to_text/data_utils.py#L86\r\nshould be replacedï¼š\r\n```\r\n _waveform = _waveform.numpy()\r\n```\r\nbecause you have altered [line83](https://github.com/pytorch/fairseq/blob/87d0ede93c06f564b7b985c1a9a0489ae946313d/examples/speech_to_text/data_utils.py#L83) in that commit",
      "Yes, you're right. That line was changed by #3921, and it should have been reverted when introducing the changes of the current PR.",
      "Thanks! I will revert the change."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be0fac254a19ac29c45c",
    "number": 3883,
    "body": "Sacrebleu 2 is not compatible with fairseq (see here: https://github.com/pytorch/fairseq/blob/f6abcc2a67328bee8b15c596bb626ce2d720aae6/fairseq/scoring/tokenizer.py#L36)\r\nAnd here: https://github.com/facebookresearch/vizseq/issues/29#issuecomment-609444573 \r\nSo we should just fix the dependency to a version which works.\r\nGenerally doing \">=\" and assuming future versions don't break anything is not a good idea.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @ImanHosseini! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233883). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be10ac254a19ac29c45d",
    "number": 3879,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dianaml0-patch-1",
    "is_a_fork": false,
    "comments": [
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D30969142).",
      "@dianaml0 merged this pull request in pytorch/fairseq@f6abcc2a67328bee8b15c596bb626ce2d720aae6."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be11ac254a19ac29c45e",
    "number": 3873,
    "body": "Add optimized library implementation to accelerator MoE module.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nTo accelerate MoE execution in fairseq if tutel package is installed.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "moe-benchmark",
    "is_a_fork": true,
    "comments": [
      "This seems to be breaking when running on multiple processes (gpus).\r\n\r\n```\r\n  File \"/private/home/namangoyal/src/fairseq_gshard/fairseq-py/fairseq/modules/moe/moe_layer.py\", line 165, in forward\r\n    l_aux, self.metadata, C, E, indices_, locations_, gates_ = self.gate(reshaped_input, reshaped_input_padding_mask)\r\n  File \"/private/home/namangoyal/.conda/envs/fairseq-20210102-py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/private/home/namangoyal/src/fairseq_gshard/fairseq-py/fairseq/modules/moe/top2gate.py\", line 247, in forward\r\n    return top2gating(\r\n  File \"/private/home/namangoyal/src/fairseq_gshard/fairseq-py/fairseq/modules/moe/top2gate.py\", line 131, in top2gating\r\n    locations1 = fused_cumsum_sub_one(mask1)\r\n  File \"/private/home/namangoyal/src/tutel/tutel/jit_kernels/gating.py\", line 79, in fast_cumsum_sub_one\r\n    return get_cumsum_kernel(data.size(0), data.size(1))(data)\r\n  File \"/private/home/namangoyal/src/tutel/tutel/jit_kernels/gating.py\", line 70, in optimized_cumsum\r\n    base_kernel(mask1.to(torch.int32), locations1)\r\n  File \"/private/home/namangoyal/src/tutel/tutel/impls/jit_compiler.py\", line 44, in func\r\n    tutel_custom_kernel.invoke(inputs, __ctx__ * 256 + key)\r\nRuntimeError: (0) == (return_code) INTERNAL ASSERT FAILED at \"/private/home/namangoyal/src/tutel/tutel/custom/custom_kernel.cpp\":95, please report a bug to PyTorch. return_codeCHECK_EQ fails.\r\n```\r\n\r\nI think, this could be due to some weird interaction of how Tutel compiles and loads custom cuda kernels and maybe spawned processes.  \r\n\r\nI tried Tutel's example also on multi gpus with pytorch distribuetd launch script, that seems to work. One difference I think is, pytorch launch script forks the new processes vs fairseq spawns them. Maybe thats creating a problem?\r\nI also created a `spawn version` of Tutel's helloworld example, and even that doesnt work. I can share that if that can be helpful in debugging?\r\n\r\n",
      "Thanks, @ngoyal2707 for reporting this issue. From tutel's team, we also use pytorch distribuetd launch to test this pull requests, and they are verified running well using both pytorch's ddp backend and fully-sharded backend.\r\n\r\nSo mostly likely it is an environmental issue. I see the assertion failure you encountered happens at: https://github.com/microsoft/tutel/blob/v0.1.x/tutel/custom/custom_kernel.cpp#L95\r\nIt indicates that the JIT compiler doesn't get a source code content that matches its expected format.\r\n\r\nMay I ask whether you change some source codes and install Tutel from source? If so, the changes may be not suitable and break the following procedure.\r\n\r\nIf you don't change anything or just install tutel via `pip install https://..`, can you help to show the standard output of the following shell commands in your environment?\r\n```sh\r\n(Command)\r\n$ ls /tmp/*.cu\r\n```\r\n",
      "1) pytorch distributed launch script works for me also, I am saying that fairseq's way of launching doesn't work for me.\r\nCan you please share the cmd you used to test fairseq integration? \r\n\r\n2) The aassertion fails here for me:\r\n```\r\nhttps://github.com/microsoft/tutel/blob/v0.1.x/tutel/custom/custom_kernel.cpp#L92 \r\n```\r\nthe trace says 95 cause I have bunch of debugging print statements in that code locally.\r\n\r\n3) I tried installing without any changes and it still failed.\r\n\r\n4) I do see all the *.cu files and the fatbins also in the tmp dir, they just dont load with spawned processes:\r\n````\r\n/tmp/0-0.cu  /tmp/1-0.cu  /tmp/10-0.cu  /tmp/12-0.cu  /tmp/14-0.cu  /tmp/16-0.cu  /tmp/18-0.cu  /tmp/2-0.cu  /tmp/20-0.cu  /tmp/22-0.cu  /tmp/24-0.cu  /tmp/26-0.cu  /tmp/28-0.cu  /tmp/3-0.cu  /tmp/30-0.cu  /tmp/32-0.cu  /tmp/34-0.cu  /tmp/36-0.cu  /tmp/5-0.cu  /tmp/7-0.cu  /tmp/9-0.cu\r\n/tmp/0-1.cu  /tmp/1-1.cu  /tmp/11-0.cu  /tmp/13-0.cu  /tmp/15-0.cu  /tmp/17-0.cu  /tmp/19-0.cu  /tmp/2-1.cu  /tmp/21-0.cu  /tmp/23-0.cu  /tmp/25-0.cu  /tmp/27-0.cu  /tmp/29-0.cu  /tmp/3-1.cu  /tmp/31-0.cu  /tmp/33-0.cu  /tmp/35-0.cu  /tmp/4-0.cu   /tmp/6-0.cu \r\n```\r\n",
      "@ngoyal2707 The outputs make sense.\r\nThe format of these JIT codes are: `<code_id>-<gpu-id>.cu`, and they are created with different names to avoid being overwritten by different child processes.\r\n\r\nSeems like the fairseq's launch doesn't deal with `LOCAL_RANK` at https://github.com/microsoft/tutel/blob/v0.1.x/tutel/impls/jit_compiler.py#L20.\r\n\r\nSo multiple child executors from fairseq's launch are conflicts writing into the same file target.\r\nCan you have a quick try on change that line into `GPU-ID` values for different executors to avoid the conflict? Those values should also be smaller than 256.\r\nIf that works, the root cause is found, and I'll think about how to fix it in a gentle way.\r\n",
      "Ahhh perfect, now I got it! thanks",
      "@ngoyal2707 BTW, may I ask what command you use to launch the model using **fairseq's way**? We want to see the runtime differences of that way and it would help us to come up with a gentle fix. Thanks!",
      "@ghostplant thanks for the awesome stuffs in Tutel. Curious if you considered using NVRTC instead of invoking nvcc. I'm asking because in some environment with VMs and so on don't have access to run nvcc. I quickly tried and functions in gating .py can be compiled with NVRTC with minor tweak but the ones in sparse.py have issues associated with __half2 doesn't seem to be supported by NVRTC. Wonder if you have any experience.",
      "@jspark1105 A good suggestion. In fact, our first version was based on NVRTC initially, but we soon find that **same kernels compiled by NVRTC is less optimized than using NVCC**. NVRTC's optimization options that NVIDIA exposes are very limited.\r\n\r\nWe create a temporary branch named `jit_with_nvrtc`: https://github.com/microsoft/tutel/tree/jit_with_nvrtc\r\nIf you are interested, you can install it from source and see whether the hello-world example is a little slower. (about 3% slower if using NVRTC for JIT compilation?)\r\nBy setting environment variable NO_NVRTC=0 or 1 (by default it is set to 0), the kernel will be compiled with NVRTC or NVCC.\r\n\r\nOne advantage of using NVRTC is that the JIT compiler no longer create any temp files on disk and of course there will be no file conflicting issues.\r\n\r\nIf you support using NVRTC and don't see noticeable kernel perf. regression, we'll create a Tutel version v0.1.1 to uses NVRTC by default.",
      "Thanks so much @ghostplant. I tried the branch but not passing a simple correctness check I wrote. If I comment out \"#define USE_NVRTC\" it goes through. I'll dig more and share minimum reproducer but wonder if you have any guess.",
      "@jspisak Not quite sure, Is your check script compatible with NVRTC? Have you tried latest v0.1.x or jit_with_nvrtc branch? Macro `USE_NVRTC` were replaced by setting environment variable `NO_NVRTC=0/1` which allows to switch both methods at runtime.",
      "@jspark1105 One possible guess: can you remove some fast-but-low-precision compiler option at: https://github.com/microsoft/tutel/blob/v0.1.x/tutel/custom/custom_kernel.cpp#L61\r\n\r\nLike `--use_fast_math`, which is to use faster instruction to get similar but not precise result.",
      "@ghostplant following up nvrtc, found removing --restrict option from option resolves the issue.",
      "@jspark1105 Thanks, just a little weird since this keyword is expected to work the same way of `__restrict__` and it works as expected in my environment. Do you know how this NVRTC's `--restrict` option impacts on your check correctness?"
    ],
    "commit_messages": [
      "Add Tutel boost for Fairseq MoE acceleration (#3873)\n\nSigned-off-by: Wei CUI <weicu@microsoft.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be12ac254a19ac29c45f",
    "number": 3867,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n#3866\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "Fix_w2v_to_hubert",
    "is_a_fork": true,
    "comments": [
      "Hi @yuseungwoo! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233867). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be13ac254a19ac29c460",
    "number": 3862,
    "body": "Summary: We resolved a bug for missing \"_metadata\" attribute for pytorch models during checkpoing saving and loading using forced state[\"model\"][\"_metadata\"], but it's not an efficient solution due to expensive model.state_dict() invocation. This diff offers an alternative solution.\n\nDifferential Revision: D30857147\n\n",
    "head_branch": "export-D30857147",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D30857147](https://phabricator.intern.facebook.com/D30857147)",
      "This pull request has been merged in pytorch/fairseq@32b31173aa30e9b1555c4048917e8aa9f6227e18."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be13ac254a19ac29c461",
    "number": 3861,
    "body": "Summary: backout fairseq changes. fix with a suggested, more optimal changes in checkopint utils.\n\nDifferential Revision: D30886481\n\n",
    "head_branch": "export-D30886481",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D30886481](https://phabricator.intern.facebook.com/D30886481)",
      "This pull request has been merged in pytorch/fairseq@e679327497702f52e4c6e5c2ab29b4d576c44ec4."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be14ac254a19ac29c462",
    "number": 3855,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nTurning `train_wall` metric precision from sec to msec.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "train_wall_precision",
    "is_a_fork": true,
    "comments": [
      "Hi @ghostplant! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233855). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be15ac254a19ac29c463",
    "number": 3848,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3847.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be16ac254a19ac29c464",
    "number": 3835,
    "body": "Fixes Import error in `data_utils.py`\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n## What does this PR do?\r\nFixes #3809\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be17ac254a19ac29c465",
    "number": 3833,
    "body": "Fix fucntion name error of convert_to_mono in audio_utils.py to match its usage in data_utils.py\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nMinor bug fix for Speech-to-text datapipeline.\r\nFix fucntion name error of function convert_to_mono in data/audio/audio_utils.py file to match the function name inside examples/speech_to_text/data_utils.py file.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be18ac254a19ac29c466",
    "number": 3826,
    "body": "# Before submitting\r\n\r\n- [No] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [No need] Did you make sure to update the docs?   \r\n- [No need] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAdd `--eval-bleu-tokenizer` to support Chinese and Japanese in training time bleu evaluation.  \r\n\r\n```python\r\neval_bleu_tokenizer: Optional[str] = field(\r\n    default='13a',\r\n    metadata={\"help\": \"Tokenization method to use for SacreBLEU. Specific `zh` for Chinese and `ja-mecab` for Japanese . Choices: {none,zh,13a,char,intl,ja-mecab}\"}\r\n)\r\n```\r\n \r\nTranslation task supports eval-bleu using sacrebleu. But when target language is something like Chinese or Japanese, which is not space split, the default tokenizer `13a` can't tokenize sentences correctly. \r\n\r\n When scoring Chinese and Japanese, `sacrebleu` need `--tokenize` to specify `zh` or `ja-mecab`. So I add `--eval-bleu-tokenizer` just like the [sacrebleu](https://github.com/mjpost/sacrebleu/)'s argument\r\n```\r\n--tokenize {none,zh,13a,char,intl,ja-mecab}, -tok {none,zh,13a,char,intl,ja-mecab}\r\n                        Tokenization method to use for BLEU. If not provided, defaults to `zh` for Chinese, `ja-mecab` for Japanese and `13a` (mteval) otherwise.\r\n```\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be18ac254a19ac29c467",
    "number": 3818,
    "body": "## What does this PR do?\r\nFixes #3817 \r\n\r\nThis PR always clears https://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/fairseq/logging/progress_bar.py#L314\r\n\r\nafter training is done, also in the case when they were not used (something to take into account)\r\n",
    "head_branch": "tblogging",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be19ac254a19ac29c468",
    "number": 3813,
    "body": "# Before submitting\r\n\r\n- [ May be  no ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ Yes ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ Yes ] Did you make sure to update the docs?   \r\n- [ No, but i test it on the colab with a actual scene ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n**Fix a bug about translation task of computing BLEU during the validation time.\r\nthe sacrebleu don't have compute_bleu() method, but the class BLEU in sacrebleu.metrics have this method.**\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding **ðŸ™ƒ**\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @hannlp! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233813). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1aac254a19ac29c469",
    "number": 3796,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3794 \r\nSpeeds up ngram blocking using a pure PyTorch implementation **compatible with Torchscript**.\r\n\r\n> fairseq-generate \\\r\n>     ./data-bin/wmt16_en_de_bpe32k/ \\\r\n>     --path model/wmt16.en-de.joined-dict.transformer/model.pt \\\r\n>     --beam 4 --remove-bpe --lenpen 0.6 --batch-size 256 --no-repeat-ngram-size 3 \\\r\n>     --gen-subset test --fp16\r\n\r\nold pytorch implementation / v100: 194.8s (15.41 sentences/s, 437.92 tokens/s)\r\ncudakernel  implementation / v100: 12.6s (237.80 sentences/s, 6756.76 tokens/s)\r\nnew pytorch  implementation (this PR) / v100: 12.9s (232.09 sentences/s, 6594.54 tokens/s)\r\n\r\nAll versions generate the same result:\r\n> BLEU4 = 29.18, 60.7/35.1/22.7/15.2 (BP=0.996, ratio=0.996, syslen=62835, reflen=63078)\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nThis was particularly fun -- there were some operations not compatible with TS that made things a bit more complex :)\r\n",
    "head_branch": "efficient_ngram_blocking",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1bac254a19ac29c46a",
    "number": 3793,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This file contains Speech to text recognition system for large size audio of length more than 15 Min. as the session gets crashed even after using 25GB RAM and high-speed GPU hence this contains a way to get this rid of by making chunks of larger file audio using librosa audio library ",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1cac254a19ac29c46b",
    "number": 3792,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @pushkal1234! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233792). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1dac254a19ac29c46c",
    "number": 3791,
    "body": "Make the current TransformerEncoderBase.forward_scriptable compatible with the current opset supported by onnx, so that the encoder can be exported to onnx for fast cpu inference\r\n\r\n## What does this PR do?\r\nFixes #3738.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "johnxin66-optional-src_lengths-calculation",
    "is_a_fork": true,
    "comments": [
      "Hi @johnxin66! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233791). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1dac254a19ac29c46d",
    "number": 3781,
    "body": "## What does this PR do?\r\nFixes OOM which happens from TPUs due to dynamic batching exceed the max a single core can work with.\r\n",
    "head_branch": "w2v2-tpu-update",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D30327091).",
      "@alexeib merged this pull request in pytorch/fairseq@cb747010c47a017e71285556afa9acce0ec62786."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1eac254a19ac29c46e",
    "number": 3778,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix #3338.\r\nIf the translation model works so bad, the `max(totals)` becomes 0. \r\nWhen this case, if the args `best-checkpoint-metric bleu` givens the training system produces an error. \r\nI try to fix this problem by logging zero bleu when max(totals) `<= 0` case. \r\n\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-issue-3338",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be1fac254a19ac29c46f",
    "number": 3773,
    "body": "â€¦verride the defaults\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3761.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-gix5",
    "is_a_fork": true,
    "comments": [
      "@kahne has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D30310383)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be20ac254a19ac29c470",
    "number": 3770,
    "body": "# Before submitting\r\n\r\n- [ âœ“] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ âœ“] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [âœ“ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (2583).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-shorten",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be21ac254a19ac29c471",
    "number": 3768,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/3623\r\n\r\n## PR review    \r\nFixes the example script for multilingual translation using iwslt17 data. The https://wit3.fbk.eu/archive/2017-01-trnted/* links now throw a 404, and the [WIT3 website page](https://wit3.fbk.eu/2017-01) now points to a Google Drive link that has all 2017-01-trnted data in one tgz file. So this PR:\r\n- switches download to use Google Drive, using `gdown`\r\n- extracts the relevant SRC-TGT files before preprocessing\r\n\r\nCould confirm that `bash prepare-iwslt17-multilingual.sh` runs without errors using this fix. It is also idempotent.",
    "head_branch": "fix_iwslt17",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be22ac254a19ac29c472",
    "number": 3766,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "multilingual_translation/batchensemble_v2",
    "is_a_fork": true,
    "comments": [
      "Hi @eadwu! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233766). Thanks!",
      "Sorry did not mean to open this here."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be22ac254a19ac29c473",
    "number": 3755,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nWhen --dataset-impl=raw and --task=language_modeling, fairseq-train cause the following error\r\n```FileNotFoundError: Dataset not found: valid (./input/data-bin/valid)```\r\n\r\nCurrently, make_dataset generates valid.None-None.\r\nThe PR fix that.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-output-text-file",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be23ac254a19ac29c474",
    "number": 3742,
    "body": "Updated example config file for tpu to include mask parameters.\r\nNoted currently cli bug in README\r\n\r\n## What does this PR do?\r\nFixes # (3741) B.\r\n",
    "head_branch": "w2v2-task-fix",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29938257)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be24ac254a19ac29c475",
    "number": 3728,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "eval-bleu",
    "is_a_fork": true,
    "comments": [
      "Hi @Csinclair0! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233728). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be25ac254a19ac29c476",
    "number": 3727,
    "body": "Previously combining FSDP with `--update-freq` would result in significant memory usage because full-size gradients would be accumulated on each GPU. We can instead skip the `no_sync` context manager in this case. The tradeoff is more communication (we do reduce-scatter on each backward), but the memory savings are likely to be worth it in most cases.",
    "head_branch": "fsdp-update-freq",
    "is_a_fork": false,
    "comments": [
      "@myleott has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29824021).",
      "@myleott merged this pull request in pytorch/fairseq@698961dc0d2eb17d15e9665f6f31fd4c1c4b58c7."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be26ac254a19ac29c477",
    "number": 3726,
    "body": "The `positions` argument is passed in\r\n`LearnedPositionalEmbedding.forward()`, while it is ignored in\r\n`SinusoidalPositionalEmbedding.forward()`.\r\n\r\nThe method has been fixed to use the argument when passed.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAbove-mentioned.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "pass_pe_positions_arg",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be26ac254a19ac29c478",
    "number": 3722,
    "body": "One of the changes in Hydra 1.1 is that the default composition order is changing.\r\nThis is documented [here](https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order).\r\nIn Hydra 1.1, a config is overriding values introduced by the defaults list while in Hydra 1.0 - the defaults list was overriding the values in the config.\r\n\r\nfairseq is currently depending on the previous behavior:\r\nThe class `FairseqConfig` defines config values, and it's expecting them to be overridden by the defaults list.\r\nThis result in a different config being created when running `fairseq_cli/hydra_train.py` with Hydra 1.0 and with 1.1.\r\n\r\nHydra 1.1 introduced the `_self_` keyword in the defaults list to control the composition order. In order to achieve the behavior of Hydra 1.0, `_self_` should be added as the first item in the defaults list.\r\n\r\nTo allow for a smoother migration, Hydra 1.0 is ignoring `_self_` starting from 1.0.7 (previous versions will issue an error).\r\n\r\nThis diff adds `_self_` as the first item in the defaults list the fairseq config, and introduce a dependency a Hydra 1.0 version that is equal or newer to 1.0.7.\r\n\r\n\r\n### Testing:\r\nI ensured that the following yield the same composed config:\r\nDefault config with Hydra 1.0.6, 1.0.7 and 1.1.0\r\n\r\n`examples/wav2vec/config/finetuning/base_10h.yaml` with Hydra 1.0.6, 1.0.7 and 1.1.0.\r\n\r\nThis can be achieved by outputing the generated config using `--cfg job` and compating the outputs.",
    "head_branch": "hydra-1.1-compatibility",
    "is_a_fork": true,
    "comments": [
      "will this have any effect on external configs (--config-dir + --config-name)? if not lgtm",
      "Not supposed to.\r\nAs I said, I tested examples/wav2vec/config/finetuning/base_10h.yaml and it's composing the same config.",
      "@alexeib, can we get this merged soon?",
      "sure, can you just make diff from this? merging happens through fbcode, not here",
      "@jieru-hu has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29917677).",
      "@jieru-hu merged this pull request in pytorch/fairseq@53802e781291b63e656c89818c38bfc49ff0f108.",
      "Hi! [This line](https://github.com/facebookresearch/fairseq/blob/acd9a53607d1e5c64604e88fc9601d0ee56fd6f1/setup.py#L203) could be changed to `hydra-core>=1.0.7,<=1.1`, right?\r\nAlso, `omegaconf` requirement should then be changed to `omegaconf<=2.1` to satisfy [this requirement](https://github.com/facebookresearch/hydra/blob/79b39bed17eadae1ea98d30c520c4686fedab530/requirements/requirements.txt#L1) from `hydra`.",
      "@omry Could you please confirm if fairseq works with hydra-core==1.1?",
      "@rsxdalv, Nope. I am no longer involved with either."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be27ac254a19ac29c479",
    "number": 3708,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dev_tpu_mp-powersgd",
    "is_a_fork": true,
    "comments": [
      "Hi @iankouls-aws! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233708). Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be28ac254a19ac29c47a",
    "number": 3675,
    "body": "## What does this PR do?\r\nSome downstream users reported that errors when passing Namespace to load_checkpoint().\r\n\r\nA recent change made the assumption that the passed object is dict like (dict or DictConfig) that have a get function.\r\nThis changes that and make sure the mocked config have checkpoint.save_dir to allow the test to run.",
    "head_branch": "hydra_1.1_fix",
    "is_a_fork": true,
    "comments": [
      "@lematt1991 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29564805).",
      "@lematt1991 merged this pull request in pytorch/fairseq@dd106d9534b22e7db859a6b87ffd7780c38341f8."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be29ac254a19ac29c47b",
    "number": 3660,
    "body": "This is conflicting with a recent change on fairscale:\r\n\r\nhttps://github.com/facebookresearch/fairscale/pull/731",
    "head_branch": "remove-numpy-version-clamp",
    "is_a_fork": true,
    "comments": [
      "As of numpy 1.20, the minimum supported Python version is 3.7.\r\nClosing, there is a discussion to have about fairseq minimum Python version."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2aac254a19ac29c47c",
    "number": 3659,
    "body": "## What does this PR do?\r\nFixes compatibility with Hydra 1.1.\r\nThe result is compatible with both Hydra 1.0 and Hydra 1.1, and will allow a smoother migration to Hydra 1.1.\r\n\r\nAt this point I am not yet removing the restriction on the Hydra version from setup.py:\r\n1. It depends on some Hydra 1.1 changes that are not yet released (It will be compatible with 1.1.1).\r\n2. Upgrading will result in deprecation warnings, and fixing them will break compatibility with Hydra 1.0.\r\n\r\nThere will be some followup to make the code fully compatible with 1.1 once Hydra 1.1 is the default version in fbcode.",
    "head_branch": "hydra_1.1_compatibility",
    "is_a_fork": true,
    "comments": [
      "@lematt1991 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29498036).",
      "@lematt1991 merged this pull request in pytorch/fairseq@9bee82e4a7b73249a88f2e2d286e991493ef13c2."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2aac254a19ac29c47d",
    "number": 3651,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "2021_06_24_03",
    "is_a_fork": true,
    "comments": [
      "Hi @Csinclair0! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233651). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2bac254a19ac29c47e",
    "number": 3650,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nAllows nonblocking checkpointing, and ensure the nonblocking transfers are finished before the optimizer step can be taken.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2cac254a19ac29c47f",
    "number": 3648,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2dac254a19ac29c480",
    "number": 3637,
    "body": "Work in progress.\r\n\r\nTODO:\r\n\r\n1) Remove some traces that aren't very useful.\r\n2) Prefix each trace with fairseq::\r\n3) Cleanup support for new torch profiler API\r\n(https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/)\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "profile-data",
    "is_a_fork": false,
    "comments": [
      "Hi @msbaines! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233637). Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2eac254a19ac29c481",
    "number": 3627,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3371.\r\n\r\nCurrently, the output from Wav2Vec 2.0 decoding does not contain word-level start/end times, which can be useful for certain applications of ASR. Based on the discussion [here](https://github.com/flashlight/flashlight/issues/618), they could be computed based on the output from the Flashlight decoder. For the KenLM decoder, we could first obtain the frame number corresponding to each non-blank token. Next, the timestamp of each character could be computed as `segment_start + frame_no/total_frames * segment_duration`. Finally, the start and end time of each word could be calculated based on the timestamp of the word boundary characters. In order to enable this, the frame number of each non-blank character is returned as a result of KenLM decoding. This is similar to the `timesteps` output from the [ctcdecode](https://github.com/parlance/ctcdecode#outputs-from-the-decode-method) library.\r\n\r\n## PR review    \r\n@alexeib\r\n",
    "head_branch": "timestamp",
    "is_a_fork": true,
    "comments": [
      "thanks - could you also add it to decoders here, which are soon to replace the old ones you improved:\r\n\r\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_recognition/new/decoders",
      "Done! By the way, I only added this to the KenLM decoder. Do you think the same approach would work for FairseqLMDecoder?",
      "> Done! By the way, I only added this to the KenLM decoder. Do you think the same approach would work for FairseqLMDecoder?\r\n\r\nyes, it should work for all decoders",
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29282488).",
      "@alexeib merged this pull request in pytorch/fairseq@3c4a8e41559fa50b6c907fbefa1dab55d57bda5c.",
      "Hey @Nithin-Holla,\r\n\r\nHow to interpret these timesteps?\r\n\r\nAre the timesteps in seconds?\r\n",
      "@harveenchadha These timesteps are similar to the ones returned by the [ctcdecode](https://github.com/parlance/ctcdecode#outputs-from-the-decode-method) library. They indicate the frame number corresponding to each of the predicted characters. Suppose you have the start time and duration of an audio segment, one simple way to convert these timesteps into seconds that I can think of is `segment_start + timestep/total_frames * segment_duration`.",
      "Hey @Nithin-Holla \r\nHow can we generate timestamps for the Viterbi decoder?\r\nand can this method be used to interpret timestamps in seconds for that also?",
      "@hpaliwal1225 I haven't checked it for the Viterbi decoder. But if the expression `viterbi_path[b].tolist()` is similar to `result.tokens` in the KenLM decoder, i.e., token predictions for every frame, then this approach should work for it too. ",
      "@Nithin-Holla , thanks for this awesome contribution. \r\n\r\nCould you share a working code example? I've been trying to work through some of the code in the speech recognition page but realize I'm in over my head. In particular, I'm not sure why I need to download and pre-process the entire LibriSpeech corpus to run inference on a tiny wav file.\r\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_recognition\r\n\r\nIf you could share any code/example for running inference (with timings) on a small wav file, that'd be really helpful. Thanks much!",
      "> @harveenchadha These timesteps are similar to the ones returned by the [ctcdecode](https://github.com/parlance/ctcdecode#outputs-from-the-decode-method) library. They indicate the frame number corresponding to each of the predicted characters. Suppose you have the start time and duration of an audio segment, one simple way to convert these timesteps into seconds that I can think of is `segment_start + timestep/total_frames * segment_duration`.\r\n\r\nthis segment_start + timestep/total_frames * segment_duration` only works when file duration is less than 60s. what to do if file is greater than 60s?\r\n",
      "Hi guys, could anyone please explain to me where I can get the values for `segment_start`, `segment_duration` and `total_frames` in the decoder? I can't find these values anywhere. Maybe I need to search elsewhere?",
      "I figured out how to get the word-level timestamps from the fairseq `W2lDecoder` subclasses' outputs. In [examples/speech_recognition/w2l_decoder.py](https://github.com/facebookresearch/fairseq/blob/main/examples/speech_recognition/w2l_decoder.py), I added the following code:\r\n1. in `W2lDecoder.__init__()`: `self.symbols = tgt_dict.symbols # symbols (usually chars) understood by the ASR model, that are predicted in the emission matrix.`\r\n2. `W2lKenLMDecoder` already had `get_timesteps()` method implemented, but I extended the functionality to all other decoders by adding the `get_timesteps()` function to the parents class `W2lDecoder` and also created a `get_symbols()` method:\r\n```python\r\ndef get_timesteps(self, token_idxs: List[int]) -> List[int]:\r\n        \"\"\"Returns frame numbers corresponding to every non-blank token.\r\n\r\n        Parameters\r\n        ----------\r\n        token_idxs : List[int]\r\n            IDs of decoded tokens (including blank tokens), i.e. list of tokens spanning all frames of the emission matrix.\r\n\r\n        Returns\r\n        -------\r\n        List[int]\r\n            Frame numbers corresponding to every non-blank token.\r\n        \"\"\"\r\n        timesteps = []\r\n        for i, token_idx in enumerate(token_idxs):\r\n            if token_idx == self.blank:\r\n                continue\r\n            if i == 0 or token_idx != token_idxs[i-1]:\r\n                timesteps.append(i)\r\n                \r\n        return timesteps\r\n\r\n    def get_symbols(self, token_idxs: List[int]) -> List[int]:\r\n        \"\"\"Returns characters corresponding to every non-blank token.\r\n\r\n        Parameters\r\n        ----------\r\n        token_idxs : List[int]\r\n            IDs of non-blank tokens.\r\n\r\n        Returns\r\n        -------\r\n        List[int]\r\n            Character corresponding to every non-blank token.\r\n        \"\"\"\r\n        chars = []\r\n        for token_idx in token_idxs:\r\n            chars.append(self.symbols[token_idx])\r\n\r\n        return chars\r\n```\r\n3. I slightly modified the code of the `decode()` method of each decoder to ensure the character symbols, timesteps corresponding to each symbol and list of words is returned in the `hypos` object.\r\n- The new `decode()` method for `W2lViterbiDecoder`:\r\n```python\r\n    def decode(self, emissions):\r\n        B, T, N = emissions.size()\r\n        hypos = []\r\n        if self.asg_transitions is None:\r\n            transitions = torch.FloatTensor(N, N).zero_()\r\n        else:\r\n            transitions = torch.FloatTensor(self.asg_transitions).view(N, N)\r\n        viterbi_path = torch.IntTensor(B, T)\r\n        workspace = torch.ByteTensor(CpuViterbiPath.get_workspace_size(B, T, N))\r\n        CpuViterbiPath.compute(\r\n            B,\r\n            T,\r\n            N,\r\n            get_data_ptr_as_bytes(emissions),\r\n            get_data_ptr_as_bytes(transitions),\r\n            get_data_ptr_as_bytes(viterbi_path),\r\n            get_data_ptr_as_bytes(workspace),\r\n        )\r\n\r\n        for b in range(B):\r\n            tokens = self.get_tokens(viterbi_path[b].tolist()).tolist()\r\n            hypos.append(\r\n                [\r\n                    {\r\n                        \"tokens\": tokens,  # non-blank token idxs.\r\n                        \"symbols\": self.get_symbols(\r\n                            tokens\r\n                        ),  # characters (symbols) corresponding to non-blank token idxs.\r\n                        \"score\": 0,\r\n                        \"timesteps\": self.get_timesteps(\r\n                            viterbi_path[b].tolist()\r\n                        ),  # frame numbers of non-blank tokens.\r\n                        \"words\": post_process(\r\n                            self.tgt_dict.string(tokens), \"letter\"\r\n                        ).split(\r\n                            \" \"\r\n                        ),  # the transcript as a list of words.\r\n                    }\r\n                ]\r\n            )\r\n\r\n        return hypos\r\n```\r\n- The new `decode()` method for `W2lKenLMDecoder`:\r\n```python\r\ndef decode(self, emissions):\r\n        B, T, N = emissions.size()\r\n        hypos = []\r\n        for b in range(B):\r\n            emissions_ptr = emissions.data_ptr() + 4 * b * emissions.stride(0)\r\n            results = self.decoder.decode(emissions_ptr, T, N)\r\n\r\n            nbest_results = results[: self.nbest]\r\n            hypos.append(\r\n                [\r\n                    {\r\n                        \"tokens\": tokens,  # non-blank token idxs.\r\n                        \"symbols\": self.get_symbols(\r\n                            tokens\r\n                        ),  # characters (symbols) corresponding to non-blank token idxs.\r\n                        \"score\": result.score,\r\n                        \"timesteps\": self.get_timesteps(\r\n                            result.tokens\r\n                        ),  # frame numbers of non-blank tokens.\r\n                        \"words\": [\r\n                            self.word_dict.get_entry(x) for x in result.words if x >= 0\r\n                        ],  # the transcript as a list of words. Empty if lexicon-free decoding.\r\n                    }\r\n                    for result in nbest_results\r\n                    if (\r\n                        tokens := self.get_tokens(result.tokens).tolist()\r\n                    )  # tokens is a local variable for the list comprehension.\r\n                ]\r\n            )\r\n        return hypos\r\n```\r\n- The new `decode()` method for `W2lFairseqLMDecoder`:\r\n```python\r\ndef decode(self, emissions):\r\n        B, T, N = emissions.size()\r\n        hypos = []\r\n\r\n        def idx_to_word(idx):\r\n            if self.unit_lm:\r\n                return self.idx_to_wrd[idx]\r\n            else:\r\n                return self.word_dict[idx]\r\n\r\n        def make_hypo(result):\r\n            hypo = {\r\n                        \"tokens\": self.get_tokens(result.tokens).tolist(),  # non-blank token idxs.\r\n                        \"score\": result.score\r\n                    }\r\n            if self.lexicon:\r\n                hypo[\"words\"] = [idx_to_word(x) for x in result.words if x >= 0] # the transcript as a list of words.\r\n            hypo[\"symbols\"] = self.get_symbols(hypo[\"tokens\"]) # characters (symbols) corresponding to non-blank token idxs.\r\n            hypo[\"timesteps\"] = self.get_timesteps(result.tokens) # frame numbers of non-blank tokens.\r\n\r\n            return hypo\r\n\r\n        for b in range(B):\r\n            emissions_ptr = emissions.data_ptr() + 4 * b * emissions.stride(0)\r\n            results = self.decoder.decode(emissions_ptr, T, N)\r\n\r\n            nbest_results = results[: self.nbest]\r\n            hypos.append([make_hypo(result) for result in nbest_results])\r\n            self.lm.empty_cache()\r\n\r\n        return hypos\r\n```\r\nI then postprocess the results in my own custom script to get the word-level time alignments (in seconds) for each hypothesis:\r\n1. For results of beam search-based decoders (`W2lKenLMDecoder`, `W2lFairseqLMDecoder`) I use the following function to process the result of `decode()`:\r\n```python\r\ndef beam_search_decode_fairseq(hypos, emission_mx, audio_lens, num_hyps, time_aligns):\r\n\"\"\"Process the results of a W2lDecoder object from fairseq.\r\n\r\n    Args:\r\n        hypos (Union[List[Dict], List[List[Dict]]]):\r\n            List of results for each audio file returned by a W2lDecoder object. If the number of hypotheses to return (W2lDecoder.nbest) is 1, hypos will be a list of just the best hypotheses dicts.\r\n             If W2lDecoder.nbest > 1, hypos will be a list of lists, where for each audio file there will be N best hypotheses dicts.\r\n        emission_mx (torch.tensor(B,T,N)):\r\n            The batched emission matrix outputted by the w2v2 acoustic model trained in fairseq.\r\n        audio_lens (List[int]):\r\n            The lengths of the original audio files in the batch, measured in number of samples.\r\n        num_hyps (int):\r\n            The number of best hypotheses to return per audio file.\r\n        time_aligns (bool):\r\n            Flag used to specify whether to calculate word-level time alignment in seconds for each hypothesis.\r\n\r\n    Returns:\r\n        transcripts (Union[List[Dict], List[List[Dict]]]):\r\n            List of processed results for each audio file. If W2lDecoder.nbest = 1, transcripts will be a list of just the best hypotheses dicts.\r\n             If W2lDecoder.nbest > 1, transcripts will be a list of lists, where for each audio file there will be N best hypotheses dicts.\r\n            A hypothesis dict has the following fields:\r\n                'pred_txt': (str) the transcript hypothesis itself.\r\n                'timestamps_word': (List[Dict]) List of word Dict objects, one for each word in the transcript, with the following fields:\r\n                    'word': the word itself.\r\n                    'start_time': the start time of the word in seconds in the corresponding audio file.\r\n                    'end_time': the end time of the word in seconds in the corresponding audio file.\r\n    \"\"\"\r\n    transcripts = []\r\n    for i in range(emission_mx.size(dim=0)):\r\n        # if the batch_size is > 1, use the maximum original audio length in the batch, as all other audio files are padded to the max length during preprocessing.\r\n        audio_len = audio_lens[i] if emission_mx.size(dim=0) == 1 else max(audio_lens)\r\n        if num_hyps > 1:\r\n            all_results = []\r\n            for hyp in hypos[i]:\r\n                hyp_dict = dict()\r\n                if hyp['words']:\r\n                    # 'words' field is not empty if using a lexicon.\r\n                    transcript = ' '.join(hyp['words']).lower()\r\n                else:\r\n                    # 'words' field is [] if lexicon-free decoding, convert from non-blank symbols to words instead.\r\n                    tokens_str = ''.join(hyp['symbols'])\r\n                    transcript = ' '.join(tokens_str.split('|')).strip().lower()\r\n                hyp_dict['pred_txt'] = transcript\r\n                if time_aligns:\r\n                    word_times = get_word_time_alignments_fairseq(audio_len, emission_mx.size(dim=1), 16000, hyp['symbols'], hyp['timesteps'])\r\n                    timestamps_word = normalize_timestamp_output_w2v2(hyp_dict['pred_txt'].split(' '), word_times)\r\n                    hyp_dict['timestamps_word'] = timestamps_word\r\n                # add a hypothesis dict\r\n                all_results.append(hyp_dict)\r\n                \r\n            transcripts.append(all_results)\r\n        else:\r\n            hyp_dict = dict()\r\n            # append the decoded phrase (as a list of words) from the prediction of the first beam [0] (most likely transcript).\r\n            if hypos[i][0]['words']:\r\n                # 'words' field is not empty if using a lexicon.\r\n                transcript = ' '.join(hypos[i][0]['words']).lower()\r\n            else:\r\n                # 'words' field is [] if lexicon-free decoding, convert from non-blank symbols to words instead.\r\n                tokens_str = ''.join(hypos[i][0]['symbols'])\r\n                transcript = ' '.join(tokens_str.split('|')).strip().lower()\r\n            hyp_dict['pred_txt'] = transcript\r\n            if time_aligns:\r\n                word_times = get_word_time_alignments_fairseq(audio_len, emission_mx.size(dim=1), 16000, hypos[i][0]['symbols'], hypos[i][0]['timesteps'])\r\n                timestamps_word = normalize_timestamp_output_w2v2(hyp_dict['pred_txt'].split(' '), word_times)\r\n                hyp_dict['timestamps_word'] = timestamps_word\r\n            # add a hypothesis dict\r\n            transcripts.append(hyp_dict)\r\n\r\n    return transcripts\r\n```\r\n2. For Viterbi decoder (`W2lViterbiDecoder`) I use the following code to process the result of `decode()`:\r\n```python\r\ntranscripts = []\r\n        for i in range(emission_mx.size(dim=0)):\r\n            # if the batch_size is > 1, use the maximum original audio length in the batch, as all other audio files are padded to the max length during preprocessing.\r\n            audio_len = audio_lens[i] if emission_mx.size(dim=0) == 1 else max(audio_lens)\r\n            hyp_dict = dict()\r\n            # append the decoded phrase (as a list of words) from the prediction of the first beam [0] (most likely transcript).\r\n            transcript = ' '.join(hypos[i][0]['words']).lower()\r\n            hyp_dict['pred_txt'] = transcript\r\n            if self.time_aligns:\r\n                word_times = get_word_time_alignments_fairseq(audio_len, emission_mx.size(dim=1), 16000, hypos[i][0]['symbols'], hypos[i][0]['timesteps'])\r\n                timestamps_word = normalize_timestamp_output_w2v2(hyp_dict['pred_txt'].split(' '), word_times)\r\n                hyp_dict['timestamps_word'] = timestamps_word\r\n            # add a hypothesis dict\r\n            transcripts.append(hyp_dict)\r\n\r\n        return transcripts\r\n```\r\n**Most importantly**, `get_word_time_alignments_fairseq()` is where I calculate the word-level time alignments:\r\n```python\r\ndef get_word_time_alignments_fairseq(audio_len, num_frames, sample_rate, symbols, timesteps):\r\n    \"\"\"Get word time alignments information for a hypothesis transcript input by converting from timesteps to seconds.\r\n    Args:\r\n        audio_len (int):\r\n            The length of audio file in number of samples.\r\n        num_frames (int):\r\n            The number of frames in the ASR acoustic model emission matrix.\r\n        sample_rate (int):\r\n            The sample rate of the loaded audio file.\r\n        symbols (List[str]):\r\n            Decoded list of characters corresponding to the non-blank tokens returned by the decoder.\r\n        timesteps (List[int]):\r\n            Frame numbers corresponding to the non-blank tokens/symbols.\r\n\r\n    Returns:\r\n        word_times (List[Tuple[float, float]]):\r\n            List of tuples of start_time and stop_time in seconds for word in the transcript.\r\n    \"\"\"\r\n    # list of times in seconds in the corresponding audio file for the the non-blank tokens/symbols.\r\n    timestamps = []\r\n    # get the timestep in seconds corresponding to each non-blank token.\r\n    for frame_num in timesteps:\r\n        timestamp = frame_num * (audio_len / (num_frames * sample_rate))\r\n        timestamps.append(timestamp)\r\n\r\n    # NOTE: algorithm only works if the first and last symbols are '|', so add them in if that's not the case.\r\n    frame_offset = 0\r\n    if symbols[0] != '|':\r\n        symbols.insert(0, '|')\r\n        # if adding a symbol at index 0, all symbols will have their frame idx increased by 1, so an offset of -1 is created.\r\n        frame_offset = -1\r\n    if symbols[-1] != '|':\r\n        symbols.append('|')\r\n\r\n    word_boundary_idxs = [] # tuples of word start and stop indices.\r\n    # get the indices of all word-boundary tokens (|).\r\n    wb_tokens_idxs = [i for i in range(len(symbols)) if symbols[i] == '|']\r\n\r\n    # create tuples for each word that contains the indices of its start symbol and end symbol.\r\n    tup = [] # initialise the first tuple of word start character and word end character indices.\r\n    # loop through the indices of the '|' tokens and find the indices of the word-boundary symbols/characters that are the start and end characters of each word.\r\n    for wb_tokens_idx in wb_tokens_idxs:\r\n        try:\r\n            if symbols[wb_tokens_idx-1] != '|' and tup:\r\n                # there is a start index in tuple, but no end index yet.\r\n                # end index has been found.\r\n                if wb_tokens_idx-1 == tup[0]:\r\n                    # word is composed of only one character, add the index of this '|' token as the end character index for the word.\r\n                    tup.append(wb_tokens_idx)\r\n                else:\r\n                    # word is composed of more than one character.\r\n                    tup.append(wb_tokens_idx-1) # add an end character index for the word.\r\n                # add the tuple as complete word to the list of word start and end index tuples.\r\n                word_boundary_idxs.append(tup)\r\n                tup = [] # reset the tuple.\r\n                # continue onto the next if statement as this '|' token may be the boundary between two words.\r\n            if symbols[wb_tokens_idx+1] != '|':\r\n                # start character of new word reached.\r\n                tup.append(wb_tokens_idx+1) # add a start character index for the word.\r\n        except IndexError:\r\n            continue\r\n    \r\n    # create tuples of start and stop times for each word\r\n    word_times = [(timestamps[start_idx + frame_offset], timestamps[end_idx + frame_offset]) for start_idx, end_idx in word_boundary_idxs]\r\n\r\n    return word_times\r\n```\r\nAnd `normalize_timestamp_output_w2v2()` is just a utility function to create a Dict containing time alignment information for each word:\r\n```python\r\ndef normalize_timestamp_output_w2v2(words, word_time_tuples):\r\n    \"\"\"Get word Dict objects with time information for each word in the hypothesis transcript.\r\n\r\n    Args:\r\n        words (List[str]):\r\n            List of words in the transcript.\r\n        word_time_tuples (List[Tuple[float,float]]):\r\n            List of tuples of start_time and stop_time in seconds for word in the transcript.\r\n\r\n    Returns:\r\n        values (List[Dict]):\r\n            List of dict objects where each dict has the following fields:\r\n                'word': (str) the word itself.\r\n                'start_time': (float) the start time in seconds of the word in the corresponding audio file.\r\n                'end_time': (float) the end time in seconds of the word in the corresponding audio file.\r\n    \"\"\"\r\n    values = []\r\n    for word, (word_start, word_end) in zip(words, word_time_tuples):\r\n        vals_dict = dict()\r\n        vals_dict['word'] = word\r\n        vals_dict['start_time'] = word_start\r\n        vals_dict['end_time'] = word_end\r\n        values.append(vals_dict)\r\n    \r\n    return values\r\n```\r\n**The formula I use to calculate the time in seconds in the corresponding audio for each non-blank symbol in the transcript is the following:**<br />\r\n        timestamp = frame_num * (audio_len / (num_frames * sample_rate))<br />\r\nwhere:\r\n- frame_num = the timestep of the symbol, as returned in the 'timesteps' field of Wl2Decoder.decode() outputs.\r\n- audio_len = the number of samples in the loaded audio file corresponding to the transcript (if using batched w2v2 acoustic model inference, will be zero padded to the length of the longest loaded audio file in the batch). \r\n- num_frames = the number of frames in the emission matrix returned by the w2v2 acoustic model inference for that audio file (if using batched inference, the number of frames for each audio file will be the same as in this case all loaded audio files are padded to the length of the longest audio file in the batch).\r\n- sample_rate = sample rate of loaded audio files (usually 16000 Hz)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2eac254a19ac29c482",
    "number": 3610,
    "body": "In EpochBatchIterator, first_batch() freezes batches in order to\r\ngenerate the dummy_batch. We then freeze batches again in the call\r\nto next_epoch_itr(). We can avoid the second freeze and reduce\r\ntime to first iteration by about 50% in cases where we have a\r\ncallable batch_sampler.\r\n\r\nBefore:\r\n\r\n![Screen Shot 2021-06-10 at 5 08 22 PM](https://user-images.githubusercontent.com/35972327/121613200-d2366600-ca10-11eb-9d1d-bafc2403766a.png)\r\n\r\nAfter:\r\n\r\n![Screen Shot 2021-06-10 at 5 07 54 PM](https://user-images.githubusercontent.com/35972327/121613224-dfebeb80-ca10-11eb-9d5a-07be9440db77.png)\r\n\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "avoid-double-freeze",
    "is_a_fork": false,
    "comments": [
      "ping",
      "@msbaines has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D29105845).",
      "@msbaines merged this pull request in pytorch/fairseq@cd5775f30184baa414a354d9f06b747344a8ba74."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be2fac254a19ac29c483",
    "number": 3609,
    "body": "## What does this PR do?\r\n\r\nIn upcoming PyTorch 1.9 / torchaudio 0.9 release, torchaudio support TorchScript-able wav2vec2.0 model definitions.\r\nThis PR adds the illustration fo how to convert the models to from `fairseq` and `transformers` into deployable package.\r\n\r\ncc @myleott @alexeib \r\n",
    "head_branch": "fairseq-pr",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be30ac254a19ac29c484",
    "number": 3607,
    "body": "# Before submitting\r\n\r\n- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [no need] Did you make sure to update the docs?   \r\n- [no need] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix that `--keep-inference-langtok` does't work with fairseq-generate and fairseq-interactive in translation_multi_simple_epoch task.\r\n\r\n`--keep-inference-langtok` is a argument of `TranslationMultiSimpleEpochTask`, it is mainly used to keep language tokens in inference output for analysis or debugging\r\n\r\nIn the original implementation https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/translation_multi_simple_epoch.py#L199-L217, following code will be executed\r\n\r\n```python\r\nif not getattr(args, \"keep_inference_langtok\", False)\r\n```\r\n\r\nBut the variable `args` comes from `cfg.generation` (see https://github.com/pytorch/fairseq/blob/master/fairseq_cli/interactive.py#L169 and https://github.com/pytorch/fairseq/blob/master/fairseq_cli/generate.py#L168) which don't have `keep_inference_langtok` argument. So the if condition judgment always satisfies, thus result in `keep_inference_langtok` dose't work with fairseq-generate and fairseq-interactive\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be31ac254a19ac29c485",
    "number": 3606,
    "body": "## What does this PR do?\r\nFixes second part of #3604 correcting the dropout arg.\r\n",
    "head_branch": "translation_fix_b",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be32ac254a19ac29c486",
    "number": 3605,
    "body": "Fixes #3604 correcting the path to src and the dropout arg.\r\n\r\n",
    "head_branch": "translation_fix",
    "is_a_fork": true,
    "comments": [
      "Hi @cronopioelectronico! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233605). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be33ac254a19ac29c487",
    "number": 3601,
    "body": "## What does this PR do?\r\nFor HubertPretrainingTask, added dictionaries to the task state to enable the serialization of the dictionaries (thus removing the need to load from the disk after training)\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n\r\n### Test Plan\r\nTo verify the success, run the Hubert Pretraining pipeline, load a checkpoint model, and verify that the \"dictionaries\" key is present in the state within the model.\r\n\r\nSpecifically, \r\n```\r\nPYTHONPATH=. python /path/to/fairseq/fairseq_cli/hydra_train.py -m \\\r\n        --config-dir ${fairseq_dir}/examples/hubert/config/pretrain \\\r\n        --config-name hubert_base_librispeech \\\r\n        hydra/launcher=submitit_local \\\r\n        hydra.launcher.gpus_per_node=2 \\\r\n        hydra.launcher.cpus_per_task=8 \\\r\n        hydra.launcher.mem_gb=384 \\\r\n        task.data=${tsv_dir} \\\r\n        task.label_dir=${km_dir} \\\r\n        task.labels=[\"km\"] \\\r\n        +data=iter1 \\\r\n        optimization.max_update=250 \\\r\n        hydra.sweep.dir=${exp_dir} \\\r\n        hydra.run.dir=${exp_dir} > ${exp_dir}/log.out 2> ${exp_dir}/log.err &\r\n```\r\nThen, at the location of the model, load the model using `pytorch.load`, and verifying that \"dictionaries\" is a key under the `task_state` key of the model.\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "hi @neeyanthkvk \r\ncan you test this PR further by also doing fine-tuning? \r\n\r\nWe overload the use of `dictionaries` in pre-training and fine-tuning: during pre-training it is a set of dictionaries of clustering indices, while during fine-tuning it's a single dictionary containing letter labels for CTC loss. I'm worried that now you store dictionaries in the task_state, it may prevents loading the dictionary used for fine-tuning (`dict.ltr.txt`) in the fine-tuning stage.\r\n\r\nIf fine-tuning fails due to the above reason, we may want to have distinguish pre-training/fine-tuning dictionary and labels explicitly, and change `load_dataset()` in the HubertPretrainingTask",
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "@neeyanthkvk has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28995537).",
      "@neeyanthkvk merged this pull request in pytorch/fairseq@afc77bdf4bb51453ce76f1572ef2ee6ddcda8eeb."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be33ac254a19ac29c488",
    "number": 3593,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_dummy_masked",
    "is_a_fork": false,
    "comments": [
      "> Looks great. Could you please add a test. tests/test_valid_subset_checks.py may be useful as an example.\r\n\r\n@msbaines just added a test!",
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28992614).",
      "@dianaml0 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28992614).",
      "@dianaml0 merged this pull request in pytorch/fairseq@50158da3a7b293f2d2fa06a23e90c160b92f54ce."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be34ac254a19ac29c489",
    "number": 3592,
    "body": "In file_io.py, there is a logging message that happens in the global\r\nscope. This logging message can be invoked before calling\r\nlogging.basicConfig() in fairseq_cli/train.py resulting in that\r\ncall becoming a no-op. This was causing the loglevel to remain at\r\nWARNING.\r\n\r\nFix is to call logging.basicConfig() before import-ing any fairseq\r\nlibraries that may do logging in global scope.\r\n\r\nVerified that I logging.info messages are now visible after applying\r\nthis PR.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-logging",
    "is_a_fork": false,
    "comments": [
      "From pytorch docs:\r\n\r\n\r\n> The functions debug(), info(), warning(), error() and critical() will call basicConfig() automatically if no handlers are defined for the root logger.\r\n> \r\n> This function does nothing if the root logger already has handlers configured, unless the keyword argument force is set to True.\r\n\r\n\r\nhttps://docs.python.org/3/library/logging.html#logging.basicConfig",
      "@msbaines has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28900871).",
      "@msbaines merged this pull request in pytorch/fairseq@45d8fefaa6871afbb747e5e65ba58b8f9fda37fe."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be35ac254a19ac29c48a",
    "number": 3576,
    "body": "For fp16 case, we set the value to -6e4 instead of -1e8 to prevent overflow error.\r\n\r\n\r\n## What does this PR do?\r\nFixes #3575 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be36ac254a19ac29c48b",
    "number": 3572,
    "body": "This PR is a copy of https://github.com/pytorch/fairseq/pull/3228 for Hubert. Wav2Vec2 had an issue with the padding mask which was solved in https://github.com/pytorch/fairseq/pull/3228. Hubert seems to have the same problem as Wav2vec2 (see: https://github.com/pytorch/fairseq/issues/3227) so that the padding mask should be corrected accordingly.\r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/3227 for Hubert\r\n\r\n## PR review    \r\n@hikushalhere\r\n@alexeib \r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "correct_padding_mask_hubert",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28723688).",
      "Hey @alexeib,\r\n\r\nIs this PR merged actually? :-)",
      "hmm it might have gotten lost. lets see if i can rebase this and merge"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be37ac254a19ac29c48c",
    "number": 3569,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Fixed by f8a7c93440cd925f70979a6082c18f830b39e44b"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be38ac254a19ac29c48d",
    "number": 3568,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [z] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [z] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nfixes the issue of `need_weights` is never used in the wav2vec2 model.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix3",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be38ac254a19ac29c48e",
    "number": 3566,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "wav2vec2_st_oss",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be39ac254a19ac29c48f",
    "number": 3561,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [X] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nfix the error when using pyarrow for raw_audio_dataset, Currently there is an error `TypeError: join() argument must be str, bytes, or os.PathLike object, not 'StringScalar'`. In this PR I just convert the type to string in `__getitem__()` for `os.path.join()`\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix3",
    "is_a_fork": true,
    "comments": [
      "@kahne has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28594620).",
      "Thanks for PR! We will merge it soon.",
      "@kahne merged this pull request in pytorch/fairseq@f9edd9f9b919b5fe77255296c69e052e4a930b2b."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be3aac254a19ac29c490",
    "number": 3560,
    "body": "To avoid the creation of a cuda:0 context, I needed to make sure that the `BackgroundConsumer` thread had its cuda device context set to the correct GPU.\r\n",
    "head_branch": "enable-pin-memory",
    "is_a_fork": false,
    "comments": [
      "> For my learning, what is cuda:0 context?\r\n\r\nIn torch, CUDA GPU0 is referred to as device=\"cuda:0\" so I really meant a CUDA context for GPU 0. pin_memory will create a context for the current CUDA device which defaults to GPU 0. To avoid this, you can change the default device via `torch.cuda.set_device()`. The problem is the the default device is thread-specific, so new threads fallback to GPU 0 unless you call `torch.cuda.set_device()` on a new thread after creation.\r\n\r\nSome more context:\r\n\r\nhttps://github.com/pytorch/pytorch/issues/58626\r\nhttps://github.com/pytorch/pytorch/issues/32424",
      "@msbaines has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28573071).",
      "@msbaines merged this pull request in pytorch/fairseq@a1fea2eb0e5a68c9f91b18a344056675332181a3."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be3bac254a19ac29c491",
    "number": 3557,
    "body": "###  this PR fixes the Error \"nn.utils.rnn.pack_padded_sequence: RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor\" when using berard architecture\r\nthe **lengths** parameter of pack_padded_sequence()  _\"must be in cpu if provided as a tensor\" even when using GPU (CUDA) _  as mentioned in the documentation [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) and in issue pytorch/pytorch#43227.\r\n \r\n\r\n\r\n ",
    "head_branch": "fbranch",
    "is_a_fork": true,
    "comments": [
      "The issue was solved by @myleott in  pytorch/pytorch#43227 i think that this PR can be aproved by you.",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be3cac254a19ac29c492",
    "number": 3556,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "clumba",
    "is_a_fork": true,
    "comments": [
      "Hi @dreamermary! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233556). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be41ac254a19ac29c493",
    "number": 3553,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "local",
    "is_a_fork": true,
    "comments": [
      "Hi @shalei120! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233553). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be42ac254a19ac29c494",
    "number": 3552,
    "body": "Fixes the following crash:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/private/home/msb/.conda/envs/fairseq-20210102-pt181/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\r\n    fn(i, *args)\r\n  File \"/private/home/msb/code/fairseq/fairseq/distributed/utils.py\", line 328, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/private/home/msb/code/fairseq/fairseq_cli/train.py\", line 117, in main\r\n    data_utils.raise_if_valid_subsets_unintentionally_ignored(cfg)\r\n  File \"/private/home/msb/code/fairseq/fairseq/data/data_utils.py\", line 584, in raise_if_valid_subsets_unintentionally_ignored\r\n    other_paths = _find_extra_valid_paths(train_cfg.task.data)\r\nAttributeError: 'Namespace' object has no attribute 'data'\r\n```\r\n",
    "head_branch": "fix-dummy-tasks",
    "is_a_fork": false,
    "comments": [
      "> Sorry, thanks. Regression test?\r\n\r\nDone.",
      "ping",
      "@msbaines has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28667773).",
      "@msbaines has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28667773).",
      "@msbaines merged this pull request in pytorch/fairseq@9497ae3cfb04bb6ec4735758bbe8dc767276932c."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be43ac254a19ac29c495",
    "number": 3547,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "local",
    "is_a_fork": true,
    "comments": [
      "Hi @shalei120! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233547). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be44ac254a19ac29c496",
    "number": 3524,
    "body": "First commit adding some extra pretraining. \r\n",
    "head_branch": "denoising",
    "is_a_fork": true,
    "comments": [
      "Hi @Csinclair0! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233524). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be44ac254a19ac29c497",
    "number": 3514,
    "body": "This leads to the following error:\r\n\r\n```\r\nValueError: unsupported format character 'k' (0x6b) at index 95\r\n```\r\n\r\nResolves #3491\r\n\r\n### Test:\r\n\r\n`fairseq-train --help`\r\n\r\nProduces:\r\n\r\n```\r\n...\r\n  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN\r\n                        when used with --keep-interval-updates, skips deleting any checkpoints with update X where X % keep_interval_updates_pattern == 0\r\n...\r\n```\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n#3491 \r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "escape-%-in-help",
    "is_a_fork": false,
    "comments": [
      "@lematt1991 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28037028).",
      "@lematt1991 merged this pull request in pytorch/fairseq@8b861beae282ec5dd5051686440948a3f893c3ec."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be45ac254a19ac29c498",
    "number": 3505,
    "body": "# Before submitting\r\n\r\n- [n/a ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x ] Did you make sure to update the docs?   \r\n- [n/a ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nScope of changes is restricted to docs only. Changes are in agreement with Standard American English. \r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nJust trying to help wherever I can.",
    "head_branch": "Typo-corrections",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be46ac254a19ac29c499",
    "number": 3502,
    "body": "## What does this PR do?\r\nThis PR updates some outdated code from the Hugging Face Transformers library to the new, better format.\r\n\r\n## PR review    \r\n@alexeib \r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "update_hf_example",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28140574).",
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D28140574).",
      "@alexeib merged this pull request in pytorch/fairseq@366974d9817138d1618693f021ea1690f9e53f33."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be47ac254a19ac29c49a",
    "number": 3499,
    "body": "",
    "head_branch": "aim_logger_integration",
    "is_a_fork": true,
    "comments": [
      "Hi @Khazhak! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233499). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be48ac254a19ac29c49b",
    "number": 3495,
    "body": "Summary: Avoid creating size-0 tensor \"filler\" in case src_len is the same as key_padding_mask_size or prev_key_padding_mask_size\n\nReviewed By: jackm321\n\nDifferential Revision: D27897778\n\n",
    "head_branch": "export-D27897778",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D27897778](https://phabricator.intern.facebook.com/D27897778)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be48ac254a19ac29c49c",
    "number": 3477,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nUpdate GLUE Downloader URL, it just doc improvements :)\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @dongs0104! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233477). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be49ac254a19ac29c49d",
    "number": 3460,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3282\r\nAdd support for `torch.cuda.amp`\r\nAMP can be enabled by `--amp`, instead of using `--fp16` for the already present full fp16 support.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "add_torch_amp",
    "is_a_fork": true,
    "comments": [
      "@myleott has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D27932253).",
      "@myleott merged this pull request in pytorch/fairseq@237184e5222b347475456f4a44f31a510c64ca35.",
      "I'm using the updated code, but I still get the following error when I try to experiment with point_generator.\r\n\r\nepoch 001:   0%|                                                                                                | 0/9869 [00:00<?, ?it/s]2021-06-11 22:21:59 | INFO | fairseq.trainer | begin training epoch 1\r\n2021-06-11 22:21:59 | INFO | fairseq_cli.train | Start iterating over samples\r\nTraceback (most recent call last):                                                                                                       \r\n  File \"/home/fairseq_repo/train.py\", line 14, in <module>\r\n    cli_main()\r\n  File \"/home/fairseq_repo/fairseq_cli/train.py\", line 507, in cli_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/home/fairseq_repo/fairseq/distributed/utils.py\", line 369, in call_main\r\n    main(cfg, **kwargs)\r\n  File \"/home/fairseq_repo/fairseq_cli/train.py\", line 180, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/home/mabavisani/anaconda3/envs/fairseqrepo/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fairseq_repo/fairseq_cli/train.py\", line 291, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/home/mabavisani/anaconda3/envs/fairseqrepo/lib/python3.6/contextlib.py\", line 52, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/fairseq_repo/fairseq/trainer.py\", line 710, in train_step\r\n    ignore_grad=is_dummy_batch,\r\n  File \"/home/fairseq_repo/fairseq/tasks/fairseq_task.py\", line 501, in train_step\r\n    with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\r\nAttributeError: module 'torch.cuda.amp' has no attribute 'autocast'\r\n\r\n",
      "@mahdiabavisani what version of pytorch are you using?\r\nI think this might be due to an older version of pytorch.",
      "@gaganbahga : Thanks. Yes, upgrading `pytorch` to `1.8` (from `1.5`) solved the issue.",
      "@gaganbahga I updated my pytorch version to 1.8 but im still having the same error. What else can i do?",
      "@gaganbahga I updated pytorch to 1.8 and also reinstalled fairseq and it works now... "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4aac254a19ac29c49e",
    "number": 3436,
    "body": "Summary: Remove residue of changes not meant to be landed in D27372372\n\nReviewed By: myleott\n\nDifferential Revision: D27543742\n\n",
    "head_branch": "export-D27543742",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D27543742](https://phabricator.intern.facebook.com/D27543742)",
      "This pull request has been merged in pytorch/fairseq@aa5f0119a383e013e56ae5d88e4a7aff0e67f0f9."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4bac254a19ac29c49f",
    "number": 3431,
    "body": "minor typo in README\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes typo in the README\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4bac254a19ac29c4a0",
    "number": 3430,
    "body": "Solves [this](https://github.com/pytorch/fairseq/issues/3415) issue.\r\nCC: @sarapapi.\r\n\r\n\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements) [Yes](https://github.com/pytorch/fairseq/commit/6e91e226441fc3c68adf91bdef9f39d9d9dc2c9c#r48976767)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n[This](https://github.com/pytorch/fairseq/commit/6e91e226441fc3c68adf91bdef9f39d9d9dc2c9c#r48976767) change causes an [error](https://github.com/pytorch/fairseq/issues/3415) during evaluation. Evaluation is running fine with this modification\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "@mzaidi59 Thank you so much for your pull request! The same change has been updated in https://github.com/pytorch/fairseq/commit/a20dc364647c94417f493ba0b0c8d1e1834e67eb.",
      "Thanks a lot @xutaima!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4cac254a19ac29c4a1",
    "number": 3427,
    "body": "tokens logging was using the tgt_dict on src tokens\r\n\r\n## Description\r\nIn `GeneratorHubInterface.generate`, we attempt to log the stringified source tokens in `logger.info(\"S\\t{}\".format(self.string(source_tokens)))`. However `self.string` uses the `tgt_dict` for the conversion, so we're applying the `tgt_dict` to source tokens! This PR fixes that bug by using the `src_dict` on the logged source tokens.\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @raphaelmerx! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233427). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "it great thanks so much",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4dac254a19ac29c4a2",
    "number": 3416,
    "body": "Correction in the use of decoder output that now is a NamedTuple and not a dict anymore\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [
      "Thank you so much @sarapapi! I am able how to merge it, it seems that I am not authorized. @kahne could you please merge it?",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4eac254a19ac29c4a3",
    "number": 3380,
    "body": "This is a proposed quick fix for #3194 (https://github.com/pytorch/fairseq/issues/3194)\r\nOn Windows 10, creating a symlink requires super user priviledges, so I changed the code to use a copytree instead.\r\nThis seems OK since the data volume to be copied is small. LMK\r\n\r\nNo tests were added since this an installation script and my goal is to show that a simple solution to the issue exists.",
    "head_branch": "fix_win10_install",
    "is_a_fork": true,
    "comments": [
      "Hi @sierkov! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233380). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Still relevant",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4fac254a19ac29c4a4",
    "number": 3379,
    "body": "â€¦her logging messages to appear)\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes import deepspeed and logger.warning (otherwise it will block other logging messages to appear)\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix2",
    "is_a_fork": true,
    "comments": [
      "Ah, we did fix this in a concurrent PR before seeing this one, but thank you! Will close this out since it's fixed in master already."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be4fac254a19ac29c4a5",
    "number": 3374,
    "body": "Clarify that training is done on 1 GPU.",
    "head_branch": "jmp84-patch-1",
    "is_a_fork": false,
    "comments": [
      "@jmp84 has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D27183474).",
      "@jmp84 merged this pull request in pytorch/fairseq@53b781caad3d58cd7fefed80e17faf147d15da66."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be50ac254a19ac29c4a6",
    "number": 3370,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nImplements 1 and 4 bit quantization for quant noise. Haven't added a test harness yet, so just opening a draft PR. Work done in collaboration with @sergiogcharles and @lyronctk\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes ðŸ‘ ",
    "head_branch": "1-4-bit-quantization",
    "is_a_fork": true,
    "comments": [
      "Hi @wells853! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233370). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n",
      "bump"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be51ac254a19ac29c4a7",
    "number": 3366,
    "body": "I guess it should be \"preprocess\" instead of \"postprocess\".\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be52ac254a19ac29c4a8",
    "number": 3350,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nIt addresses what I believe is a typo in the wav2vec2 docs.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix/documentation_typo",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be53ac254a19ac29c4a9",
    "number": 3349,
    "body": "Some functionalities of fairseq are accessible only when building extensions and making them available as user command. Would it be helpful to update the README in some way to make that information more accessible?\r\n\r\nFor instance, I find this workflow quite useful:\r\n```\r\nâ¯ conda create -n torch-stable python==3.8 parameterized numpy scipy pytest torchaudio pytorch -c pytorch\r\nâ¯ conda activate torch-stable\r\nâ¯ pip install soundfile\r\nâ¯ git clone https://github.com/pytorch/fairseq\r\nâ¯ pip install --verbose --user --global-option=\"build_ext\" --editable ./fairseq\r\nâ¯ # fairseq-hydra-train task.data=... --config-dir ... --config-name ...\r\nâ¯ # pip uninstall fairseq\r\n```\r\n\r\nEven if this pull request is closed without merge, its suggestion remains discoverable and hopefully helpful to other users :)\r\n\r\ncc #2106",
    "head_branch": "install",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be53ac254a19ac29c4aa",
    "number": 3344,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix a bug in score.py with --sentence_bleu. Scorer must use BleuConfig.\r\n(related [commit](https://github.com/pytorch/fairseq/commit/3b27ed7996b0315f471c795cf9b7dfcc18467cbe#diff-b741b1f7738ef6a2b9c5697b09367c97b74b98f0eb3c998e03346943e40cce7c))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nðŸ˜„ \r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be54ac254a19ac29c4ab",
    "number": 3331,
    "body": "",
    "head_branch": "fsdp_cr",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be55ac254a19ac29c4ac",
    "number": 3328,
    "body": "This enables training wav2vec 2.0 models on TPUs courtesy of @taylanbil ",
    "head_branch": "enable-w2v2-tpu",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D27127542)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be56ac254a19ac29c4ad",
    "number": 3327,
    "body": "",
    "head_branch": "fsdp_release",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be57ac254a19ac29c4ae",
    "number": 3323,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n- New option to use raw audio input for S2T tasks (`--use-audio-input`). (*)\r\n- New option to prepend the target language tag for S2T tasks (`--prepend-tgt-lang-tag`).\r\n- Scripts don't generate a new vocabulary if there is one with the same properties in the data folder.\r\n- New [EuroparlST](https://ieeexplore.ieee.org/document/9054626) preprocessing script\r\n\r\n(*) CoVoST and EuroparlST need to be converted to WAV beforehand. Otherwise, SpeechToTextDataset won't be able to load their audio files.\r\n\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "s2t_waveforms",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be57ac254a19ac29c4af",
    "number": 3322,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThis PR adds two arguments to `GeneratorHubInterface`:\r\n1. `return_all_hypotheses: bool = False`: if `True` it returns all hypotheses from beam search instead of the top-1. Default is `False` for back compatibility.\r\n2. `return_scores: bool = False`: if `True` it returns the scores (log-probs) for all hypotheses as `float`.  Default is `False` for back compatibility.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "adding_sample_functions",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be58ac254a19ac29c4b0",
    "number": 3321,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes reordering when calling `BARTHubInterface.generate`. Without reordering, if we call the function with a batch of sentences, the output order is arbitrary and this is a bug. This PR fixes this.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fixing_bart_reordering",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "bump",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "@nicola-decao thanks for your PR, I can take a look. Would you be able to resolve the conflict?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be59ac254a19ac29c4b1",
    "number": 3319,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3318 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @scan3! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233319). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5aac254a19ac29c4b2",
    "number": 3316,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "robertadeepspeed2",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5bac254a19ac29c4b3",
    "number": 3307,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix a typo in gcmv_path given for config yaml generation (actual: gcvmn_cvmn_path, correct: gcmvn_path) \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @sarapapi! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233307). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@kahne merged this pull request in pytorch/fairseq@16c1a200f87a2adb6395e353345c19bbe990d1dd."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5cac254a19ac29c4b4",
    "number": 3306,
    "body": "",
    "head_branch": "plasma-latest",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5cac254a19ac29c4b5",
    "number": 3305,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n- Migrate `speech_to_text` modules to Hydra, including:\r\n  - Adapting the models and the task to Hydra.\r\n  - Creating models' default configs to define architectures.\r\n  - Creating configs in `examples/speech_to_text/config/` to make it easier to run training examples.\r\n  - Modifying the markdown documentation in `examples/speech_to_text/`accordingly to the previous point.\r\n  \r\n- Minor fixes:\r\n  - Solve a bug in BÃ©rard model related to `pack_padded_sequence`, which requires `lengths` to be in CPU (https://github.com/pytorch/pytorch/issues/43227).\r\n  - Corrected the subsets names in the Librispeech example, since`train` and `dev` do not exist in this dataset. I changed them to `train-clean-100,train-clean-360,train-other-500` and `dev-clean,dev-other`, respectively.\r\n\r\n## Future work\r\n\r\n- Inheriting fields from the same dataclass is not possible yet in Hydra. Hence, I could not reproduce the behaviour of some pre-Hydra defaults definitions like `args.decoder_embed_dim = getattr(args, \"decoder_embed_dim\", args.encoder_embed_dim)`. It might be available when Hydra upgrades to OmegaConf 2.1 (https://github.com/facebookresearch/hydra/issues/1426).\r\n- I renamed `config_yaml` to `data_config_yaml` to avoid confusion with Hydra's `config-name` and `config-dir`. However, I think we should integrate the configuration files created during preprocessing to the Hydra workflow somehow.\r\n- I've seen that Simultaneous ST modules are still in active development, so I haven't migrated that part. I've just modified `convtransformer_simul_trans.py` to remove the need for the old `convtransformer_espnet` architecture. These modules will need to be migrated in the future.\r\n- Add examples using `convtransformer` and `s2t_berard`.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nSure! ðŸ’ª \r\n",
    "head_branch": "s2t_hydra",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5dac254a19ac29c4b6",
    "number": 3287,
    "body": "",
    "head_branch": "plasma-patch",
    "is_a_fork": false,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Hi @sshleifer! \n\nThank you for your pull request. \n\nWe **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.\n\nYou currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233287). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5eac254a19ac29c4b7",
    "number": 3286,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes circular imports incurred by a recent commit\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix3",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5fac254a19ac29c4b8",
    "number": 3280,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3279.\r\n\r\nThis change modifies the output of `echo -e \"Ja, wer hat, wenn du willst, GÃ¶tter gebildet, uns zu ihnen erhoben, sie zu uns herniedergebracht, als der Dichter?\\tbard\\nZu vollenden ist nicht die Sache des SchÃ¼lers, es ist genug, wenn er sich Ã¼bt\\tstudent\" | python normalize.py | python tok.py | fairseq-interactive --constraints -s de -t en --beam 10 --batch-size 2 --buffer-size 2 --bpe fastbpe --bpe-codes ../../../models/ende30k.fastbpe.code --path ../../../models/wmt19.de-en.ffn8192.pt ../../../models/` as follows.\r\n\r\nBefore:\r\n```\r\nS-0\tJa , wer hat , wenn du will@@ st , GÃ¶@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?\r\nW-0\t1.755\tseconds\r\nC-0\tstudent\r\nH-0\t-1.1425577402114868\tYes , who , if you will , has formed go@@ ds , raised us up to them , brought them down to us , but the po@@ et student ?\r\nD-0\t-1.1425577402114868\tYes , who , if you will , has formed gods , raised us up to them , brought them down to us , but the poet student ?\r\nP-0\t-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -1.8780 -1.4195 -0.4071 -0.1347 -0.3726 -1.1306 -0.1665 -1.4588 -0.2837 -0.1722 -0.2330 -0.2840 -0.1806 -0.1432 -0.2263 -0.1395 -0.7261 -1.4593 -0.3639 -0.4030 -0.1083 -18.7577 -0.2396 -0.1837\r\nS-1\tZu voll@@ enden ist nicht die Sache des Sch@@ Ã¼l@@ ers , es ist genug , wenn er sich Ã¼bt\r\nW-1\t1.755\tseconds\r\nC-1\tb@@ ard\r\nH-1\t-1.9625756740570068\tIt is not up to the b@@ ard to complete , it is enough if he practi@@ ses\r\nD-1\t-1.9625756740570068\tIt is not up to the bard to complete , it is enough if he practises\r\nP-1\t-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -17.3978 -6.0238 -0.4888 -1.7563 -0.8708 -0.6773 -0.2027 -0.2456 -1.6366 -0.2911 -2.0235 -0.1961 -0.5538\r\n```\r\n\r\nAfter:\r\n```\r\nS-0\tJa , wer hat , wenn du will@@ st , GÃ¶@@ tter gebildet , uns zu ihnen erhoben , sie zu uns her@@ nieder@@ gebracht , als der Dich@@ ter ?\r\nW-0\t1.740\tseconds\r\nC-0\tb@@ ard\r\nH-0\t-1.2060465812683105\tYes , who , if you will , formed go@@ ds , raised us up to them , brought them down to us , but the b@@ ard ?\r\nD-0\t-1.2060465812683105\tYes , who , if you will , formed gods , raised us up to them , brought them down to us , but the bard ?\r\nP-0\t-1.8768 -0.2214 -0.4671 -1.2521 -0.2101 -0.3053 -1.2077 -0.1496 -2.2551 -0.5702 -0.1331 -0.3940 -1.0268 -0.1750 -1.4635 -0.2821 -0.1725 -0.2404 -0.3575 -0.1833 -0.1441 -0.2250 -0.1419 -0.7020 -1.5215 -0.3700 -16.8578 -2.7290 -0.3405 -0.2060\r\nS-1\tZu voll@@ enden ist nicht die Sache des Sch@@ Ã¼l@@ ers , es ist genug , wenn er sich Ã¼bt\r\nW-1\t1.740\tseconds\r\nC-1\tstudent\r\nH-1\t-0.8064212203025818\tIt is not up to the student to complete , it is enough if he practi@@ ses\r\nD-1\t-0.8064212203025818\tIt is not up to the student to complete , it is enough if he practises\r\nP-1\t-1.2630 -0.3364 -0.1634 -2.7070 -0.1734 -0.2815 -1.5556 -0.2831 -1.3885 -0.7310 -0.6367 -0.1824 -0.2386 -1.5320 -0.2728 -2.0003 -0.2163 -0.5536\r\n```\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-the-order-of-constraints",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be5fac254a19ac29c4b9",
    "number": 3276,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes the use of `prefix_allowed_tokens_fn` in generation. It was working for `fairseq==0.9.0` (see https://github.com/facebookresearch/GENRE) but with the current version is broken.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fixing_prefix_allowed_tokens_fn",
    "is_a_fork": true,
    "comments": [
      "I'm interested in seeing this land, but I was curious if it would be possible to also include a bit of documentation about this `prefix_allowed_tokens_fn` callable. I can't seem to find anything which explains what it's supposed to do or what shape it's supposed to be (`f: (Tensor, list[int]) -> int`?)",
      "> I'm interested in seeing this land, but I was curious if it would be possible to also include a bit of documentation about this `prefix_allowed_tokens_fn` callable. I can't seem to find anything which explains what it's supposed to do or what shape it's supposed to be (`f: (Tensor, list[int]) -> int`?)\r\n\r\n@erip you are right. Where do you think is the best place to write the signature of the function? here? https://github.com/pytorch/fairseq/blob/e5e8b3fee1e57a7abf35ad1a3ff223a2b7190c65/fairseq/search.py#L148",
      "@nicola-decao I think that makes good sense. There doesn't seem to be any documentation on the other search strategies, but this one is somewhat less straightforward since it's got the callback. Unless @myleott has other thoughts, I think throwing a docstring beneath the ctor would be great. ",
      "@nicola-decao if you can share a docstring here, I can update the imported version before merging",
      "> @nicola-decao if you can share a docstring here, I can update the imported version before merging\r\n\r\n@myleott Here you go:\r\n\r\nprefix_allowed_tokens_fn: `Callable[[int, torch.Tensor], List[int]]`: If provided, this function constrains the beam search to allowed tokens only at each step. If not provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id: int` and a unidimensional tensor of token ids `inputs_ids: torch.Tensor`. It has to return a `List[int]` with the allowed tokens for the next generation step conditioned on the previously generated tokens `inputs_ids` and the batch ID `batch_id`. This argument is useful for constrained generation conditioned on the prefix, as described in `Autoregressive Entity Retrieval https://arxiv.org/abs/2010.00904 and https://github.com/facebookresearch/GENRE`.",
      "@myleott Any news on this? Is there something I should do?",
      "@myleott Any news on this? I have a facebook AI project https://github.com/facebookresearch/GENRE that depends on this bug fix (for now I link people to my fork with the fix that is not ideal).",
      "@myleott @sshleifer can we please proceed on the merge here? It is really a minor change.\r\n\r\n> There is this Facebook AI project https://github.com/facebookresearch/GENRE that depends on this bug fix (for now I link people to my fork with the fix that is not ideal and may have trouble installing it).",
      "also cc @alexeib"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be60ac254a19ac29c4ba",
    "number": 3266,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes a bug in the no_overlap case when computing mask indices for wav2vec\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix4",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "@alexeib has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D35704063)."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be61ac254a19ac29c4bb",
    "number": 3264,
    "body": "# Before submitting\r\n\r\n- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [Y] Did you make sure to update the docs?   \r\n- [N] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nSmall fixes in the script and documentation for correctly reproducing the results in the corresponding paper.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_unsup_qe",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@89cd70c0f0c096bdbfcfb2ab339a9c8f23540bc0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be62ac254a19ac29c4bc",
    "number": 3257,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nfixes circular import as complained by python\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix3",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@ab560669cd9baaa4009e1fd01c970f8ffccd1ee0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be63ac254a19ac29c4bd",
    "number": 3253,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n- updates audio_utils to handle multi-channel audio as well as mono, with no change needed for existing recipes\r\n- adds speech-to-text example for Multilingual TEDx (http://openslr.org/100) data\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@kahne merged this pull request in pytorch/fairseq@da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d.",
      "@esalesky Thanks for making this PR. It looks great! I made some small edits for the code style.",
      "Sounds good, thank you very much!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be64ac254a19ac29c4be",
    "number": 3249,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3178 (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ (I did ;)\r\n",
    "head_branch": "min_max_sample_size",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be64ac254a19ac29c4bf",
    "number": 3247,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFixes #3246\r\nFixes #3248\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "Improve_Torchscript_compatibility_of_transfomer_and_transformer_pg",
    "is_a_fork": true,
    "comments": [
      "@lematt1991 merged this pull request in pytorch/fairseq@808b751597d85c098990080d21fd450877dcb242."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be65ac254a19ac29c4c0",
    "number": 3244,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2651 \r\nFixes #2618 \r\n\r\nOften we want to only speech recognize some audio files, we don't have the labels or don't want to construct the data set with labels.\r\n\r\nThis PR support \"infer\" only run.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "speech_recognition_infer_only_without_eval",
    "is_a_fork": true,
    "comments": [
      "@alexeib has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D27408168).",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n",
      "is this documented anywhere? how are you supposed to use it?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be66ac254a19ac29c4c1",
    "number": 3240,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @jpmcarrilho! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233240). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be67ac254a19ac29c4c2",
    "number": 3237,
    "body": "â€¦ith BLEU scores\r\n\r\n# Before submitting\r\n\r\n- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [no need] Did you make sure to update the docs?   \r\n- [no need] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes bugs of evaluation with BLEU score when training with multi-gpus. But no error will happend if there is no distributed training.\r\n\r\nwhen --eval-bleu is set to be `True` (default it is `False` and the best checkpoint is selected according to loss) and training with multi-gpus (when the number of gpu which participate in distributed training is greater than 1), following error will happend.\r\n\r\n```bash\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train\", line 33, in <module>\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train\", line 33, in <module>\r\nTraceback (most recent call last):\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train\", line 33, in <module>\r\n        sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 450, in cli_main\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 450, in cli_main\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 450, in cli_main\r\n        distributed_utils.call_main(cfg, main)distributed_utils.call_main(cfg, main)\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)    \r\ndistributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n\r\n      File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 143, in main\r\nmain(cfg, **kwargs)\r\n                                                                                                                                                                               main(cfg, **kwargs)rder/fairseq/fairseq_cli/train.py\", line 143, in main\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 143, in main\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n                                                                                                                                                                               valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n      File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\nvalid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 259, in train\r\nTraceback (most recent call last):\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train\", line 33, in <module>\r\n    return func(*args, **kwds)    \r\nreturn func(*args, **kwds)  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 259, in train\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 259, in train\r\n    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 345, in validate_and_save\r\n    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 345, in validate_and_save\r\n        cfg, trainer, task, epoch_itr, valid_subsets, end_of_epochsys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 345, in validate_and_save\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 450, in cli_main\r\n    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 413, in validate\r\n    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 413, in validate\r\n    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 413, in validate\r\n    trainer.valid_step(sample)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    distributed_utils.call_main(cfg, main)\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 349, in call_main\r\n    trainer.valid_step(sample)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 834, in valid_step\r\n    trainer.valid_step(sample)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    return func(*args, **kwds)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 834, in valid_step\r\n        return func(*args, **kwds)distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 834, in valid_step\r\n  File \"/data1/cordercorder/fairseq/fairseq/distributed/utils.py\", line 326, in distributed_main\r\n    main(cfg, **kwargs)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 143, in main\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 1157, in _reduce_and_log_stats\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 1157, in _reduce_and_log_stats\r\n    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 1157, in _reduce_and_log_stats\r\n    return func(*args, **kwds)\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 259, in train\r\n    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 345, in validate_and_save\r\n    self.task.reduce_metrics(logging_outputs, self.get_criterion())\r\n  File \"/data1/cordercorder/fairseq/fairseq/tasks/translation.py\", line 410, in reduce_metrics\r\n        self.task.reduce_metrics(logging_outputs, self.get_criterion())valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq/tasks/translation.py\", line 410, in reduce_metrics\r\n  File \"/data1/cordercorder/fairseq/fairseq_cli/train.py\", line 413, in validate\r\n    self.task.reduce_metrics(logging_outputs, self.get_criterion())\r\n  File \"/data1/cordercorder/fairseq/fairseq/tasks/translation.py\", line 410, in reduce_metrics\r\n    metrics.log_scalar(\"_bleu_counts\", np.array(counts))\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py\", line 480, in __array__\r\n    trainer.valid_step(sample)\r\n      File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/contextlib.py\", line 74, in inner\r\nmetrics.log_scalar(\"_bleu_counts\", np.array(counts))\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py\", line 480, in __array__\r\n        return func(*args, **kwds)metrics.log_scalar(\"_bleu_counts\", np.array(counts))\r\n\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 834, in valid_step\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py\", line 480, in __array__\r\n    return self.numpy()\r\nTypeError: can't convert cuda:2 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n    return self.numpy()\r\nTypeError: can't convert cuda:3 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n    return self.numpy()\r\nTypeError: can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\n    logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\r\n  File \"/data1/cordercorder/fairseq/fairseq/trainer.py\", line 1157, in _reduce_and_log_stats\r\n    self.task.reduce_metrics(logging_outputs, self.get_criterion())\r\n  File \"/data1/cordercorder/fairseq/fairseq/tasks/translation.py\", line 410, in reduce_metrics\r\n    metrics.log_scalar(\"_bleu_counts\", np.array(counts))\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/tensor.py\", line 480, in __array__\r\n    return self.numpy()\r\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\r\nTraceback (most recent call last):\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/distributed/launch.py\", line 261, in <module>\r\n    main()\r\n  File \"/data/cordercorder/anaconda3/envs/nmt/lib/python3.7/site-packages/torch/distributed/launch.py\", line 257, in main\r\n    cmd=cmd)\r\nsubprocess.CalledProcessError: Command '['/data/cordercorder/anaconda3/envs/nmt/bin/python', '-u', '/data/cordercorder/anaconda3/envs/nmt/bin/fairseq-train', '--local_rank=3', 'tiny_data_bin', '--distributed-world-size', '4', '--arch', 'transformer', '--share-decoder-input-output-embed', '--optimizer', 'adam', '--adam-betas', '(0.9, 0.98)', '--clip-norm', '0.0', '--lr-scheduler', 'inverse_sqrt', '--warmup-init-lr', '1e-07', '--warmup-updates', '3000', '--lr', '0.0005', '--stop-min-lr', '1e-09', '--dropout', '0.25', '--weight-decay', '0.0001', '--criterion', 'label_smoothed_cross_entropy', '--label-smoothing', '0.1', '--max-tokens', '5000', '--batch-size', '64', '--update-freq', '4', '--max-epoch', '30', '--save-dir', 'checkpoint', '--skip-invalid-size-inputs-valid-test', '--eval-bleu', '--eval-bleu-args', '{\"beam\": 5}', '--eval-bleu-remove-bpe', 'sentencepiece', '--eval-bleu-print-samples', '--eval-tokenized-bleu', '--best-checkpoint-metric', 'bleu', '--maximize-best-checkpoint-metric', '--validate-interval-updates', '1']' returned non-zero exit status 1.\r\n\r\n```\r\n\r\nThe error is cased by the fact that the numpy of version 1.20.1 does't support codes like following:\r\n```python\r\nimport torch\r\nimport numpy as np\r\na = torch.tensor(0, device=\"cuda:0\")\r\nb = np.array([a])\r\n```\r\nThe above codes will lead to error: \"TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\", but the codes run well if the numpy version is 1.18.1 or 1.17.0 (when the numpy version is below 1.20.0, it is ok, I guess). However, it seems like that the latest version of fairseq need a numpy package of version 1.20.0 or higher (issue #3203 ).\r\n\r\n### Reproduce the error\r\nDownload the source code of fairseq (commit ID: 7061a0ff83872ac491ba5963eb7fc04cb10d57c4) and run following code:\r\n```bash\r\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\r\ndata_bin_dir=tiny_data_bin\r\n\r\npython -m torch.distributed.launch --nproc_per_node=4 \\\r\n    --master_addr=\"127.0.0.1\" \\\r\n    --master_port=12345 \\\r\n    $(which fairseq-train) ${data_bin_dir} \\\r\n    --distributed-world-size 4 \\\r\n    --arch transformer \\\r\n    --share-decoder-input-output-embed \\\r\n    --optimizer adam \\\r\n    --adam-betas '(0.9, 0.98)' \\\r\n    --clip-norm 0.0 \\\r\n    --lr-scheduler inverse_sqrt \\\r\n    --warmup-init-lr 1e-07 \\\r\n    --warmup-updates 3000 \\\r\n    --lr 0.0005 \\\r\n    --stop-min-lr 1e-09 \\\r\n    --dropout 0.25 \\\r\n    --weight-decay 0.0001 \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --label-smoothing 0.1 \\\r\n    --max-tokens 5000 \\\r\n    --batch-size 64 \\\r\n    --update-freq 4 \\\r\n    --max-epoch 30 \\\r\n    --save-dir checkpoint \\\r\n    --skip-invalid-size-inputs-valid-test \\\r\n    --eval-bleu \\\r\n    --eval-bleu-args '{\"beam\": 5}' \\\r\n    --eval-bleu-remove-bpe sentencepiece \\\r\n    --eval-bleu-print-samples \\\r\n    --eval-tokenized-bleu \\\r\n    --best-checkpoint-metric bleu \\\r\n    --maximize-best-checkpoint-metric \\\r\n    --validate-interval-updates 1\r\n```\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@alexeib merged this pull request in pytorch/fairseq@09945b45d4e2608563b1b18c3bbe289bf9351529."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be68ac254a19ac29c4c3",
    "number": 3235,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [N/A] Did you make sure to update the docs?   \r\n- [N/A] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nCurrently when installing the newest source package from PyPI I get an error like so:\r\n\r\n```\r\nCollecting fairseq\r\n  Using cached fairseq-0.10.2.tar.gz (938 kB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /home/frankier/sources/datasets/.venv/bin/python3 /tmp/tmp_ujftsgi_in_process.py get_requires_for_build_wheel /tmp/tmpmn0eumq2\r\n       cwd: /tmp/pip-install-dg5d6q9y/fairseq\r\n  Complete output (31 lines):\r\n  Traceback (most recent call last):\r\n    File \"setup.py\", line 214, in <module>\r\n      do_setup(package_data)\r\n    File \"setup.py\", line 136, in do_setup\r\n      setup(\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/__init__.py\", line 152, in setup\r\n      _install_setup_requires(attrs)\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/__init__.py\", line 147, in _install_setup_requires\r\n      dist.fetch_build_eggs(dist.setup_requires)\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 60, in fetch_build_eggs\r\n      raise SetupRequirementsError(specifier_list)\r\n  setuptools.build_meta.SetupRequirementsError: ['cython', 'numpy', 'setuptools>=18.0']\r\n  \r\n  During handling of the above exception, another exception occurred:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/tmp/tmp_ujftsgi_in_process.py\", line 280, in <module>\r\n      main()\r\n    File \"/tmp/tmp_ujftsgi_in_process.py\", line 263, in main\r\n      json_out['return_val'] = hook(**hook_input['kwargs'])\r\n    File \"/tmp/tmp_ujftsgi_in_process.py\", line 114, in get_requires_for_build_wheel\r\n      return hook(config_settings)\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 149, in get_requires_for_build_wheel\r\n      return self._get_build_requires(\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 130, in _get_build_requires\r\n      self.run_setup()\r\n    File \"/tmp/pip-build-env-hag0sxvp/overlay/lib/python3.9/site-packages/setuptools/build_meta.py\", line 145, in run_setup\r\n      exec(compile(code, __file__, 'exec'), locals())\r\n    File \"setup.py\", line 217, in <module>\r\n      os.unlink(fairseq_examples)\r\n  IsADirectoryError: [Errno 21] Is a directory: 'fairseq/examples'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: /home/frankier/sources/datasets/.venv/bin/python3 /tmp/tmp_ujftsgi_in_process.py get_requires_for_build_wheel /tmp/tmpmn0eumq2 Check the logs for full command output.\r\n```\r\n\r\nI believe the reason for this is that the source package contains the examples directory because it was put there during package creation (it seems the symlink because a directory). Now, when setup.py is run again, it seems the setup.py attempts to unlink the directory, which is not possible because only symlinks can be unlinked. This PR therefore only attempts to unlink it if it is a symlink. I have not thoroughly tested whether my proposed cause is the true cause, but this should fix it in any case.\r\n\r\nNote that the source package is fetched because there is no wheel for Python 3.9, so most users will not see this because they will use the wheel.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ",
    "head_branch": "fix-remove-directory-source-package",
    "is_a_fork": true,
    "comments": [
      "@myleott "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be68ac254a19ac29c4c4",
    "number": 3231,
    "body": "More informative exception when numpy version changes to ask the user to recompile Cython files\r\n\r\n# Before submitting\r\n\r\n- [With @myleott  ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [N/A ] Did you make sure to update the docs?   \r\n- [N/A ] Did you write any new necessary tests?  \r\n\r\n\r\n## What does this PR do?\r\nRaises a more informative error to tell the user to recompile Cython files after an update to the numpy version.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "add/informative-numpy-exception",
    "is_a_fork": false,
    "comments": [
      "@mwillwork merged this pull request in pytorch/fairseq@fd7c2a8b371c2abf645f558282221eba6833f35f."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be69ac254a19ac29c4c5",
    "number": 3228,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3227\r\n\r\nAll models that do **not** make use of group norm, such as \r\n- Wav2Vec 2.0 Large (LV-60)*\r\n- Wav2Vec 2.0 Large (LV-60) + Self Training *\r\n\r\ndo need this fix IMO to able to correctly run batches through the model. Before this PR, the \r\nfollowing code snippet failed:\r\n\r\n```python\r\nimport fairseq\r\nimport torch\r\n\r\n# get model\r\nwav2vec_path = \"data/wav2vec2_vox_960h_new.pt\"\r\nmodel, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\r\n    [wav2vec_path], arg_overrides={\"data\": \"./data\"}\r\n)\r\nmodel = model[0]\r\nmodel.eval()\r\n\r\n# create single input\r\ninput_wav_0 = torch.randn((1, 2000))\r\ninput_wav_1 = torch.randn((1, 3000))\r\n\r\n# create batched input\r\nbatch_input_wav = torch.zeros((2, 3000))\r\nbatch_input_wav[0, :input_wav_0.shape[-1]] = input_wav_0\r\nbatch_input_wav[1, :input_wav_1.shape[-1]] = input_wav_1\r\n\r\n# create padding mask\r\npadding_mask = torch.zeros((2, 3000), dtype=torch.bool)\r\npadding_mask[0, input_wav_0.shape[-1]:] = True\r\n\r\n# run batch & single\r\noutput = model(source=input_wav_0, padding_mask=None)[\"encoder_out\"]\r\nbatch_output = model(source=batch_input_wav, padding_mask=padding_mask)[\"encoder_out\"]\r\n\r\n# is equal?\r\nprint(\"Is batched forward and simple forward equal?\", torch.allclose(output[:,0], batch_output[:output.shape[0], 0], atol=1e-3))\r\n```\r\nNote: It is assumed that both https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt and https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec2_vox_960h_new.pt were downloaded and stored in the folder data.\r\n\r\nAlso, see [this](https://colab.research.google.com/drive/1ASZ4lVZbKkj-dvRHDl1lo0mCcsaOERlG?usp=sharing) notebook for reproducibility.\r\n\r\nThis PR should fix the behavior and make the above code snippet / notebook run succesfully.\r\n\r\n## PR review    \r\n\r\nGently pinging @alexeib for Wav2Vec2\r\n\r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "correct_padding_wav2vec2",
    "is_a_fork": true,
    "comments": [
      "@alexeib merged this pull request in pytorch/fairseq@4fed0beca64a52aa718371dc3b2cf1fd979197a4.",
      "Will this fix resolve the issue https://github.com/pytorch/fairseq/issues/3278? Will the code snippet there be affected by this fix? I'm not sure if fairseq is required to run the code:\r\n```\r\nimport soundfile as sf\r\nimport torch\r\nfrom transformers import Wav2Vec2ForMaskedLM, Wav2Vec2Tokenizer\r\n\r\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\r\nmodel = Wav2Vec2ForMaskedLM.from_pretrained(\"facebook/wav2vec2-base-960h\")\r\nmodel.eval()\r\naudio_input, _ = sf.read(\"ionlywishtobealone.wav\")\r\ninput_values = tokenizer(audio_input, return_tensors=\"pt\").input_values\r\nlogits = model(input_values).logits\r\npredicted_ids = torch.argmax(logits, dim=-1)\r\ntranscription = tokenizer.batch_decode(predicted_ids)[0]\r\nprint(transcription)\r\nprint(input_values.shape)\r\n\r\n# pad the 1.42s audio to about 3,5,7,9s:\r\nfor n in [3,5,7,9]:\r\n  input_values = tokenizer(audio_input, return_tensors=\"pt\", padding=\"max_length\", max_length=n*10000).input_values\r\n  logits = model(input_values).logits\r\n  predicted_ids = torch.argmax(logits, dim=-1)\r\n  transcription = tokenizer.batch_decode(predicted_ids)[0]\r\n  print(input_values.shape)\r\n  print(\"padding to {}s: {}\\n\".format(n, transcription))\r\n```",
      "I just uninstalled fairseq and the code above still runs. Any ideas how I can apply the fix to the issue above, if the fix can resolve it?",
      "As far as I understand it, this fix only worked for the `lv60` models and not for the base models. The base models cannot really give the same results for padding vs. non-padding I think"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6aac254a19ac29c4c6",
    "number": 3218,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nCorrect a wrong description in the document.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @liminghao1630! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233218). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6bac254a19ac29c4c7",
    "number": 3216,
    "body": "# Before submitting\r\n\r\n- [] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @patrickvonplaten! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233216). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "looks great! ",
      "@alexeib merged this pull request in pytorch/fairseq@4f9831bf847b8595f5590faf30b2f0af6a03bac4."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6cac254a19ac29c4c8",
    "number": 3212,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes KeyError mentioned in  # (3211).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hotfix-keyerror",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@2909ee1852cdae7ad4115a1a04520b0522265dd2."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6cac254a19ac29c4c9",
    "number": 3208,
    "body": "It has the same question described in #3198\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "> Do you mind merging this with #3198? Each separate PR has to go through some internal unit tests that can take a few hours, so easier to have fewer PRs\r\n\r\nok",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6dac254a19ac29c4ca",
    "number": 3198,
    "body": "There are a lot of default values and I just need to set the input_size in theory. However, the default value of the argument padding_l is None and it will lead an error. So i think you can take the default value of the argument to 0 for preventing errors.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @JonnesLin! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233198). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6eac254a19ac29c4cb",
    "number": 3193,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix hyperlink\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-26",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@e802a30bffdad0b22ad6efc413230ca348f8f50b."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be6fac254a19ac29c4cc",
    "number": 3188,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3187 \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "dev-zxn",
    "is_a_fork": true,
    "comments": [
      "CC @myleott do you have thoughts on this?  Seems fine to me, but I don't have context around this"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be70ac254a19ac29c4cd",
    "number": 3184,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFix logger error\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-25",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@6ec7ed9920c64ae99a787c0885c543896b525df0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be70ac254a19ac29c4ce",
    "number": 3183,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFix AttributeError: 'Namespace' object has no attribute 'max_positions'\r\n\r\nhttps://github.com/pytorch/fairseq/blob/master/examples/bart/README.glue.md#3-fine-tuning-on-glue-task\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-24",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@fd624018bf3e834c09cc03695a8fa0bcaa4a10f3."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be71ac254a19ac29c4cf",
    "number": 3182,
    "body": "It seems that the current implementation uses a slightly larger label smoothing value, for a large vocabulary, it is fine, but it can be more different with a small vocabulary size. By changing these 2 lines, the computation of label smoothing loss shall be consistent with the standard.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @anoidgit! \n\nThank you for your pull request and welcome to our community. \n\n# Action Required\n\nIn order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.\n\n# Process\n\nIn order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nOnce the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233182). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@myleott merged this pull request in pytorch/fairseq@791ab7c20831a76a9196aaf0db3a2cb1cf906dde."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be72ac254a19ac29c4d0",
    "number": 3179,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3131\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "weight_init_conv_tbc",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@8629245b0329a7e704ebc7ec05b94ac094468c1b."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be73ac254a19ac29c4d1",
    "number": 3175,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3158 .\r\n\r\n## PR review    \r\nFixes bug of loading previously trained translation-task model. It is not necessary to define eval_bleu_args and eval_bleu_detok_args when we use eval_bleu=False.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "eval_bleu_args",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@51c312a30f33e3366b1bb61084d037a90aa1d4a0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be74ac254a19ac29c4d2",
    "number": 3165,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes warning \r\nlocal variable `label_dict` is not used\r\n\r\nhttps://github.com/pytorch/fairseq/blob/bfcc13e20a6cfa18fb25daaae39644f9b7872699/fairseq/tasks/sentence_prediction.py#L122-L132\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-23",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@b4843681b4d5af442febf8caba58ca9600b01656."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be74ac254a19ac29c4d3",
    "number": 3149,
    "body": "Summary: fairscale.nn.Pipe has been ported to PyTorch:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/distributed/pipeline/sync/pipe.py#L138.\r\nAs a result, modifying the pipeline transformer to use PyTorch pipe if available. This change depends on https://github.com/pytorch/pytorch/pull/50860.\r\n\r\nTest Plan:\r\n```\r\npython train.py ru_en_bin/ --arch transformer_iwslt_de_en_pipeline_parallel --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --pipeline-model-parallel --pipeline-balance '[1,3,5,3,3,1]' --pipeline-devices '[0,1,0,2,3,0]' --pipeline-chunks 16 --distributed-world-size 1 --distributed-no-spawn --disable-validation --max-epoch 1\r\n```\r\n\r\nOutput with torch pipe:\r\n```\r\n2021-01-20 16:13:35 | INFO | train | epoch 001 | loss 12.676 | nll_loss 12.331 | ppl 5151.97 | wps 5108 | ups 1.66 | wpb 3081.6 | bsz 131.6 | num_updates 380 | lr 4.75e-05 | gnorm 2.08 | train_wall 229 | wall 233\r\n2021-01-20 16:13:36 | INFO | fairseq_cli.train | done training in 233.1 seconds\r\n```\r\n\r\nOutput with fairscale pipe:\r\n```\r\n2021-01-20 14:13:59 | INFO | train | epoch 001 | loss 12.677 | nll_loss 12.331 | ppl 5152.07 | wps 5198.9 | ups 1.69 | wpb 3081.6 | bsz 131.6 | num_updates 380 | lr 4.75e-05 | gnorm 2.08 | train_wall 224 | wall 228\r\n2021-01-20 14:13:59 | INFO | fairseq_cli.train | done training in 228.0 seconds\r\n```",
    "head_branch": "torchpipe",
    "is_a_fork": true,
    "comments": [
      "Looks great to me apart from @msbaines remaining comment ",
      "@shruti-bh Addressed @msbaines's comment, could you take another look? Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be75ac254a19ac29c4d4",
    "number": 3148,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2318\r\nThe denoising task had no truncate option, which caused errors with longer sentences.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be76ac254a19ac29c4d5",
    "number": 3136,
    "body": "The bug that this pull request addresses was discussed [in this GitHub issue](https://github.com/pytorch/fairseq/issues/3085#issue-777177450), and @myleott has [asked for the pull request](https://github.com/pytorch/fairseq/issues/3085#issuecomment-754854074). \r\n\r\nAs you can see from the diff, the pull request is very simple. However, I did not run any tests (I don't have a suitable environment, I think).\r\n\r\nAlso: I did not add any tests, I did not change any documentation, I did not check linting.",
    "head_branch": "dynconv_params",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be77ac254a19ac29c4d6",
    "number": 3122,
    "body": "The normalize and encoder_embed_dim are not present in base pretraining config which leads to errors during finetuning.\r\n\r\n\r\n\r\n## What does this PR do?\r\nFixes #3005\r\n\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@4c197de87f92b0bd7e427aa3e094d05112b325a0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be78ac254a19ac29c4d7",
    "number": 3120,
    "body": "# Before submitting\r\n\r\n- [ Yes ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n\r\n\r\n## What does this PR do?\r\n\r\n- removes a minor bug in line 193 root = Path(args.data_root).absolute() / args.src_lang\r\n- args.data_root is <root>/<src_lang>, no need to add args.src_lang to the path again\r\n\r\npython examples/speech_to_text/prep_covost_data.py -d /N/slate/piyush/covost/ru -s ru -t en --vocab-type char \r\nTraceback (most recent call last):\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 280, in <module>\r\n    main()\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 276, in main\r\n    process(args)\r\n  File \"examples/speech_to_text/prep_covost_data.py\", line 195, in process\r\n    raise NotADirectoryError(f\"{root} does not exist\")\r\nNotADirectoryError: /N/slate/piyush/covost/ru/ru does not exist",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "You are supposed to use `-d /N/slate/piyush/covost` instead (see also the [instructions](https://github.com/pytorch/fairseq/blob/master/examples/speech_to_text/docs/covost_example.md#data-preparation)). ",
      "My bad than, I got confused by - \r\n\r\nparser.add_argument(\r\n        \"--data-root\", \"-d\", required=True, type=str,\r\n        help=\"data root with sub-folders for each language <root>/<src_lang>\"\r\n    ) "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be78ac254a19ac29c4d8",
    "number": 3113,
    "body": "",
    "head_branch": "fix_iwslt",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@f32de63e69aceb966b84c7c515a016ec96439125."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be79ac254a19ac29c4d9",
    "number": 3104,
    "body": "",
    "head_branch": "hubconf2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7aac254a19ac29c4da",
    "number": 3102,
    "body": "",
    "head_branch": "add_hydra_dep",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@7e5e45b483ec5896830231ebbd3ed26472bcff47."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7bac254a19ac29c4db",
    "number": 3084,
    "body": "With missing file extension in --lang-pairs option generation from 418M and 1.2B Models fails with the following error\r\n```\r\nValueError: language pair en-fr contains languages that are not in the language dictionary; langs: ['language_pairs_small_models']\r\n```\r\n\r\nHowever generation still fails after restoring file extension with following error:\r\n```\r\nRuntimeError: Error(s) in loading state_dict for TransformerModel:\r\n\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).\r\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).\r\n\tsize mismatch for decoder.output_projection.weight: copying a param with shape torch.Size([128112, 1024]) from checkpoint, the shape in current model is torch.Size([128104, 1024]).\r\n```\r\nThis could be resolved by adding --fixed-dictionary model_dict.128k.txt like in Generation for the 12B model section.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@83391858c9ca951bb243d60ba3f5f23b87e99cf8."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7cac254a19ac29c4dc",
    "number": 3070,
    "body": "There is a small typo in fairseq/modules/dynamic_convolution.py, fairseq/modules/dynamicconv_layer/dynamicconv_layer.py.\n\nShould read `efficiency` rather than `efficieny`.\n\n\nSemi-automated pull request generated by\nhttps://github.com/timgates42/meticulous/blob/master/docs/NOTE.md",
    "head_branch": "bugfix_typo_efficiency",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@81e38ed39da02b5104939cef941bd848b87f9e26."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7cac254a19ac29c4dd",
    "number": 3066,
    "body": "",
    "head_branch": "windows",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7dac254a19ac29c4de",
    "number": 3060,
    "body": "I've written custom parsers and emitters for everything from docstrings to classes and functions. However, I recently came across an issue when I was parsing/generating from the TensorFlowâ€”and now PyTorchâ€”codebases: inconsistent use of `Args:` and `Arguments:` in its docstrings. It is easy enough to extend my parsers to support both variants, however it looks like `Arguments:` is wrong anyway, as per:\n\n  - https://google.github.io/styleguide/pyguide.html#doc-function-args @ [`ddccc0f`](https://github.com/google/styleguide/blob/ddccc0f/pyguide.md)\n\n  - https://chromium.googlesource.com/chromiumos/docs/+/master/styleguide/python.md#describing-arguments-in-docstrings @ [`9fc0fc0`](https://chromium.googlesource.com/chromiumos/docs/+/9fc0fc0/styleguide/python.md)\n\n  - https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html @ [`c0ae8e3`](https://github.com/sphinx-contrib/napoleon/blob/c0ae8e3/docs/source/example_google.rst)\n\nTherefore, only `Args:` is valid. This PR replaces them throughout the codebase.\n\nPS: For related PRs, see pytorch/pytorch/pull/49736",
    "head_branch": "args-for-google-style-docstrings",
    "is_a_fork": true,
    "comments": [
      "Great that it's been merged. It shows up as this PR being closed though (red) rather than merged (purple).",
      "@myleott merged this pull request in pytorch/fairseq@e2e80c6f2dca01dd8c04b3e5b0b356abf4b429cf."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7eac254a19ac29c4df",
    "number": 3059,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n\r\nGet rid of `TransformerSentenceEncoder` from `RobertaEncoder`. This commit has passed the test in https://github.com/pytorch/fairseq/tree/master/examples/roberta\r\n\r\n`TransformerEncoder` covers all the features of `TransformerSentenceEncoder` besides segment embedding.\r\n\r\n## Futher suggection\r\n\r\nWhy not remove `TransformerSentenceEncoder` completely,\r\nsimply by adding `segment_embeddings` into `TransformerEncoder` or extends `TransformerEncoder` with additional  `segment_embeddings`.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-22",
    "is_a_fork": true,
    "comments": [
      "Thanks, this makes a lot of sense and will fix some long-standing naming confusion. I'll work on merging this.",
      "Looking forward to your update.\r\n\r\nIn my test, this commit works for Roberta and get same result in hub test . It may also work in other related cases.",
      "@myleott It looks like freeze embeddings and freeze encoder layers (all or some) are only available in TransformerSentenceEncoder, Can we add it to TransformerEncoder and therefore the Roberta/Bert code. That way people has much more flexibility in the fine-tuning stage (sometimes they just want to tune the classification head or last few encoder layers to make training faster)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be7fac254a19ac29c4e0",
    "number": 3057,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nReorganize `self.emb_layer_norm` in order to keep right order while `print(model)` \r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-21",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@48a607527a8c2435c63795cca9a05348ef6e1f9d."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be80ac254a19ac29c4e1",
    "number": 3045,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n`src_lengths` is not a required parameter in `TransformerEncoder`.\r\nIt is a dummy variable. \r\n\r\nMaybe more changes should be done to fix this issue in Class such as `Transformer`, `FairseqEncoderDecoderModel`, `BARTModel` etc.\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-20",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@a041e1ae9cd5d69af993f5da6561223ad407f5da."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be80ac254a19ac29c4e2",
    "number": 3039,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@828960f5dace4787ad81aeadca60043c907adc67."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be87ac254a19ac29c4e3",
    "number": 3036,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @Sohyo! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%233036). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be88ac254a19ac29c4e4",
    "number": 3034,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements) (An associated github issue was created https://github.com/pytorch/fairseq/issues/3035) \r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests? (not sure new tests are necessary)\r\n\r\n## What does this PR do?\r\nIntroduces the implementation of: \r\n- a recurrent encoder decoder, with either uni/bidir GRU/LSTM as encoder and decoders\r\n- an attention unit, containing all of the attentions in Luong's paper (dot, general, concat), the original Bahdanau attention, and Bahdanau's attention modified to use the mechanisms of Luong's papers.\r\n- a multilingual recurrent encoder decoder\r\n\r\n### Doesn't this already exist?\r\nNo, at the moment, the only implementation available in fairseq is an LSTM implementation with only one of Luong's attentions (no GRU, no Bahdanau, only one Luong attention). \r\n\r\n### Use cases\r\nWhen working on low resource machine translation, the transformer architecture can be too needy, in terms of data size, to perform efficiently. When that is the case, people go back to the recurrent encoder decoder with attention. It has not yet been fully implemented in fairseq, so there it is, as complete as can be in terms of modules!\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Bump\n\nOn Mon 28 Jun 2021, 14:36 stale[bot], ***@***.***> wrote:\n\n> This pull request has been automatically marked as stale. *If this pull\n> request is still relevant, please leave any comment* (for example,\n> \"bump\"), and we'll keep it open. We are sorry that we haven't been able to\n> prioritize reviewing it yet. Your contribution is very much appreciated.\n>\n> â€”\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/fairseq/pull/3034#issuecomment-869647216>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFNMROAODL5SJJE75W7FPHLTVBUDNANCNFSM4U3EJHZA>\n> .\n>\n",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Bump, and what should I do to get this PR merged?",
      "@soumith Hi, I looked for maintaners for the repo, as I couldn't find how to request a review.\r\n\r\nCould you please tell me what I need to do for this PR to be merged? That would be extremely useful, as I have no idea what more to do. Thank you for your time :) ",
      "@clefourrier thanks for you PR! Perhaps it would make sense to move this to the examples directory instead?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be89ac254a19ac29c4e5",
    "number": 3028,
    "body": "# Before submitting\r\n\r\n- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [no need] Did you make sure to update the docs?   \r\n- [no need] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nJust fixing a small typo of logging one additional bracket before starting training.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\n\r\n> If we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\nIt's really a small change, no need to discuss.\r\n\r\n## Did you have fun?\r\nSmall change although, do have fun reading the code.\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@3a597d11731c7b7949072856aa51dbf4581963b0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be89ac254a19ac29c4e6",
    "number": 3027,
    "body": "Fix the format\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "pipibjc-wmt20",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8aac254a19ac29c4e7",
    "number": 3022,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #3011.\r\n\r\n",
    "head_branch": "fix-args-copy",
    "is_a_fork": true,
    "comments": [
      "@louismartin merged this pull request in pytorch/fairseq@e8b195ac069600203da3e7d60ba29d0975dd0afd."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8bac254a19ac29c4e8",
    "number": 3018,
    "body": "## What does this PR do?\r\nFixes #3017 \r\n\r\nMy original implementation of the function utils.import_user_module supported external user modules wrapped in zip/jar files (python natively support modules in zip/jar files).\r\n\r\nThe new piece of code which checks for file existence breaks this functionality.\r\nThis trivial fix can solve the problem!",
    "head_branch": "bugfix/import_user_module",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@39e722ceabff11db00d9dd66998039236b40ae50."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8cac254a19ac29c4e9",
    "number": 2999,
    "body": "# Before submitting\r\n\r\n- [no] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [yes, the CLI flag has a help string] Did you make sure to update the docs?   \r\n- [no, but the code was successfully tested in training] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nAdds a CLI flag `--azureml-logging` to `fairseq-train` which allows fairseq to log to the default Azure ML context to improve training on Azure Machine Learning services. If `azureml-core` is not installed, it fails with a logging message like the `wandb` logging.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nAlways!\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@ac11107ed41cb06a758af850373c239309d1c961."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8dac254a19ac29c4ea",
    "number": 2992,
    "body": "# Before submitting\r\n\r\n- There is no related issue for this pull request.\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- We did not see any necessity for tests.\r\n\r\n## What does this PR do?\r\nAdd German RoBERTa model (GottBERT)\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "The line in the RoBERTa README was already present. However, while double checking it, I found a non-conformity in the style of the entry, which is resolved now. Thank you for looking into it so fast!",
      "@myleott merged this pull request in pytorch/fairseq@f3d5045a71ae463bd3f05254d7c4216801a04bc2.",
      "Hi @scheiblr , thanks for releasing the model and great to see that training is working on TPU!\r\n\r\nWould it be possible that you provide some more information about the preprocessing steps (how to preprocess data with fairseq in combination with HF Tokenizers).\r\n\r\nCould you also give some insights about the used training commands and the GCP VM (would be interested in the RAM usage, because I think you need to keep the corpus in memory, in contrast to the e.g. TensorFlow records approach that is used for BERT pre-training ).\r\n\r\nI would like to set up a kind of cheatsheet for LM pre-training for non-English languages with fairseq using TFRC TPUs so your input would be very helpful ðŸ¤— ",
      "Hi @stefan-it, I dropped you an email.",
      "Hi @scheiblr ,\r\ncould you provide the information on preprocessing and training commands in a Readme/example? I have the same questions.\r\n\r\nFor now, I did my preprocessing as was suggested in the roberta example:\r\n```\r\npython -m examples.roberta.multiprocessing_bpe_encoder \\\r\n          --encoder-json $gottbert_path/vocab.json \\\r\n          --vocab-bpe $gottbert_path/merges.txt \\\r\n          --inputs $r_dir/data/train.tok.de \\\r\n          --outputs $root_dir/train.bpe.de \\\r\n          --workers 5 \\\r\n          --keep-empty\r\n```\r\n\r\nAnd tried the following command for training. However, the architecture doesn't seem to match. Though, in your paper, you states you used the architecture RoBERTa base.\r\n```\r\nfairseq-train data-bin \\\r\n    --tensorboard-logdir tensorboard-logs \\\r\n    --task translation --source-lang de --target-lang als \\\r\n    --finetune-from-model $gottbert_path/model.pt \\\r\n    --arch roberta_base \\\r\n    --criterion label_smoothed_cross_entropy \\\r\n    --max-positions 512 \\\r\n    --dropout 0.1 --attention-dropout 0.1 \\\r\n    --label-smoothing 0.1 \\\r\n    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\r\n    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\r\n    --max-epoch 10 \\\r\n    --max-tokens 6000 --batch-size $MAX_SENTENCES --update-freq $FREQ \\\r\n    --required-batch-size-multiple 1 \\\r\n    --log-interval 100 \\\r\n    --eval-bleu --eval-bleu-args '{\"beam\": 5}' --eval-bleu-detok moses --eval-bleu-remove-bpe \\\r\n    --save-dir checkpoints \\\r\n    --keep-last-epochs 3\r\n```\r\n\r\nThat's my log output:\r\n2022-01-18 14:25:42 | INFO | fairseq_cli.train | task: translation (TranslationTask)\r\n2022-01-18 14:25:42 | INFO | fairseq_cli.train | model: roberta_base (RobertaModel)\r\n2022-01-18 14:25:42 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\r\n2022-01-18 14:25:42 | INFO | fairseq_cli.train | num. model params: 126037032 (num. trained: 126037032)\r\n2022-01-18 14:25:46 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\r\n2022-01-18 14:25:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\r\n2022-01-18 14:25:46 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     \r\n2022-01-18 14:25:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\r\n2022-01-18 14:25:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\r\n2022-01-18 14:25:46 | INFO | fairseq_cli.train | max tokens per GPU = 6000 and max sentences per GPU = 8\r\n2022-01-18 14:25:46 | INFO | fairseq.checkpoint_utils | loading pretrained model from /project/student_projects2/orangielou/data/gottbert-base/model.pt: optimizer, lr scheduler, meters, dataloader will be reset\r\nTraceback (most recent call last):\r\n  File \"/home/orangielou/fairseq/fairseq/trainer.py\", line 283, in load_checkpoint\r\n    self.get_model().load_state_dict(\r\n  File \"/home/orangielou/fairseq/fairseq/models/fairseq_model.py\", line 99, in load_state_dict\r\n    return super().load_state_dict(new_state_dict, strict)\r\n  File \"/home/orangielou/miniconda/envs/virt_env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for RobertaModel:\r\n        size mismatch for encoder.sentence_encoder.embed_tokens.weight: copying a param with shape torch.Size([52009, 768]) from checkpoint, the shape in current model is torch.Size([52008, 768]).\r\n        size mismatch for encoder.lm_head.weight: copying a param with shape torch.Size([52009, 768]) from checkpoint, the shape in current model is torch.Size([52008, 768]).\r\n        size mismatch for encoder.lm_head.bias: copying a param with shape torch.Size([52009]) from checkpoint, the shape in current model is torch.Size([52008]).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/orangielou/miniconda/envs/virt_env/bin/fairseq-train\", line 33, in <module>\r\n    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\r\n  File \"/home/orangielou/fairseq/fairseq_cli/train.py\", line 352, in cli_main\r\n    distributed_utils.call_main(args, main)\r\n  File \"/home/orangielou/fairseq/fairseq/distributed_utils.py\", line 301, in call_main\r\n    main(args, **kwargs)\r\n  File \"/home/orangielou/fairseq/fairseq_cli/train.py\", line 110, in main\r\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\r\n  File \"/home/orangielou/fairseq/fairseq/checkpoint_utils.py\", line 188, in load_checkpoint\r\n    extra_state = trainer.load_checkpoint(\r\n  File \"/home/orangielou/fairseq/fairseq/trainer.py\", line 291, in load_checkpoint\r\n    raise Exception(\r\nException: Cannot load model parameters from checkpoint /project/student_projects2/orangielou/data/gottbert-base/model.pt; please ensure that the architectures match.\r\n\r\nDo you have any suggestions what might have gone wrong or what I should do differently? Would be very much appreciated! ðŸ˜Š"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8dac254a19ac29c4eb",
    "number": 2989,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes Sequence Generator\r\n\r\nReproduce the bug:\r\n1. Background: \r\n**fairseq.utils.move_to_cuda** function support to change device.\r\n2. When you change the file **fairseq_cli/generate.py**\r\nsuppose `gpu_id = 1`\r\n- line 134 change from\r\n`model.cuda()`\r\nto\r\n`model.cuda(gpu_id)`\r\n- line 189 change from\r\n`sample = utils.move_to_cuda(sample) if use_cuda else sample` \r\nto \r\n`sample = utils.move_to_cuda(sample, gpu_id) if use_cuda else sample`\r\n2. you will get an error\r\n```\r\nfairseq/sequence_generator.py, line 382, in generate\r\n    cand_bbsz_idx = cand_beams.add(bbsz_offsets)\r\nRuntimeError: binary_op(): expected both inputs to be on same device, but input a is on cuda:1 and input b is on cuda:0\r\n\r\n```\r\n\r\nThe reason of this bug is that `bbsz_offsets` is not assigned to proper cuda device. Therefore, we need to change the line 281 and line 281 of the file **fairseq/sequence_generator.py** \r\n\r\nThe test file for this fix is the file **tests/test_sequence_generator.py**\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ \r\nvery happy ðŸ™‚",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@d5218f88275fd57825819c6dab523e30a41b6866."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8eac254a19ac29c4ec",
    "number": 2985,
    "body": "If the argument is set to \"soft\", print probability for each source\r\ntoken, like this:\r\n\r\nA-0        0.365083,0.328207,0.306710 0.442428,0.340282,0.217290\r\n0.378712,0.367315,0.253973 0.321335,0.425601,0.253064\r\n\r\nEach source token is separated from each other by a comma (,) and each\r\ntarget token is separated from each other by a space ( ).\r\n\r\nThis option is based on the Marian NMT's option.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "print_alignment_options",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be8fac254a19ac29c4ed",
    "number": 2976,
    "body": "Forget to update another model url after the fix https://github.com/pytorch/fairseq/commit/dea66cc294a18dd4d9e59aa0af8d51f951e83884. @chtran ",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "@chtran merged this pull request in pytorch/fairseq@fa802c1034c4e6a38c80e7ab545b445aabd2d314."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be90ac254a19ac29c4ee",
    "number": 2969,
    "body": "There was no min_lr argument in the config (causing \"not found\" style errors), so I've added it.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes the lack of a `min_lr` argument (resulting in errors) in the new hydra config for the CosineLRScheduler.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Thanks for the PR! We ended up doing a larger refactor, since the way min/max_lr was defined here was very confusing.\r\n\r\nThe new version is here: 4817a9142f49793ec2eedbd71fe5bd872e58e7b5 ðŸ˜„ "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be91ac254a19ac29c4ef",
    "number": 2958,
    "body": "# Before submitting\r\n\r\n- [[2720](https://github.com/pytorch/fairseq/issues/2720) ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [yes ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [yes ] Did you make sure to update the docs?   \r\n- [yes ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAdd negative constraints decoding based on positive constraints decoding.\r\nFix a liittle bug of the mismatch between line and constraints when using batch inference.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\n:)\r\n",
    "head_branch": "negative_constrained_decoding",
    "is_a_fork": true,
    "comments": [
      "I'm not involved in the approval of this or anything and have not idea how interested FB will be. One thing that seems unfortunate to me though is that you created an entirely new set of API calls in `search.py`, which are never going to be used by any other class. Was it not possible to just use the existing constrained decoding API, even if you have to slightly extend it?",
      "@myleott @mjpost  I have made some changes to the code following your suggestions above. ",
      "I think negative constrained decoding is more direct and effective than latent variable method as to diverse language generation. \r\n",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n",
      "Hi @myleott,\r\n\r\nany update on merging this into master?"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be91ac254a19ac29c4f0",
    "number": 2956,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThis PR fixes an issue with `f.tell()` when creating dictionaries. Before this bugfix, the dictionary generation had silently nondeterministic behavior which worsens with multiple workers. Please the comment in the commit for more details.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "I just realized the same behavior occurs in `binarizer.py` I fixed the implementation there as well. The overall implementation is still nondeterministic, but the probability of it being so is very, very low.",
      "@myleott merged this pull request in pytorch/fairseq@9693504a8a75bafd7bddd6caa47cc5aed6821a2b."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be92ac254a19ac29c4f1",
    "number": 2955,
    "body": "## What does this PR do?\r\nThis pull-lick fixes the following errors in cosine_lr_scheduler.py:\r\n```\r\nomegaconf.errors.ConfigAttributeError: Key 'min_lr' not in 'CosineLRScheduleConfig'\r\n\tfull_key: min_lr\r\n\treference_type=Any\r\n\tobject_type=CosineLRScheduleConfig\r\n```\r\nThis error occurs when training a wav2vec(1.0) model, for example.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be93ac254a19ac29c4f2",
    "number": 2945,
    "body": "first rebase\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @AndrewMpapalika! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232945). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be94ac254a19ac29c4f3",
    "number": 2944,
    "body": "first rebase\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @AndrewMpapalika! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232944). Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be94ac254a19ac29c4f4",
    "number": 2932,
    "body": "Mostly copied from https://github.com/facebook/react/blob/master/.github/stale.yml",
    "head_branch": "stale_bot",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@158bd0321c4b915e4bddf738f5cb9d72d192f969."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be95ac254a19ac29c4f5",
    "number": 2931,
    "body": "",
    "head_branch": "v0.10.1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be96ac254a19ac29c4f6",
    "number": 2916,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThe cosine learning rate scheduler was implemented incorrectly. It annealed to the learning rate (`--lr`) instead of the minimum learning rate (`--min-lr`). This implementation is consistent with the PyTorch [CosineAnnealingLR](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L461).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@b1b02a828fec708aea1718e5336dd941a24f4276."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be97ac254a19ac29c4f7",
    "number": 2914,
    "body": "",
    "head_branch": "v0.10.1",
    "is_a_fork": false,
    "comments": [
      "@myleott would you entertain a PR to also package a conda artifact on new tags, too? not sure what the exact process is for publishing to the pytorch conda channel, but I can do some of the legwork there.",
      "@erip yeah that sounds good.\r\n\r\nI'm actually reworking this a bit, since auto-uploading the wheels seems a bit risky right now. Instead I'm using the [`upload-artifact` action](https://github.com/fairseq/fairseq/commit/8d66c7c0b2afa612a974714248ab2daa156bce2e#diff-52d0610b43ce35e44510ae2d15d70668334378b01c1bdc774159cd3a33b71728) to produce the artifacts and then they can easily be downloaded and uploaded manually (example: https://github.com/fairseq/fairseq/runs/1419699104).\r\n\r\nAssuming conda works similarly, it'd be great to have some automated build process to build conda packages in a similar way."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be98ac254a19ac29c4f8",
    "number": 2911,
    "body": "# Before submitting\r\n\r\n- [X ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ X] Did you make sure to update the docs?   N/A\r\n- [ X] Did you write any new necessary tests?  N/A\r\n\r\n## What does this PR do?\r\nFixes #2122.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mma_criterion",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@ffa158ff0cf2aa6c104ae844bfde361f125478f6."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be98ac254a19ac29c4f9",
    "number": 2910,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n has no attribute 'args'\r\n\r\n",
    "head_branch": "patch-19",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@b889b52ae9b91a0114112d00735df56c1aa36fad."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be99ac254a19ac29c4fa",
    "number": 2906,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2899\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-wav2vec-url",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9aac254a19ac29c4fb",
    "number": 2903,
    "body": "",
    "head_branch": "metrics",
    "is_a_fork": true,
    "comments": [
      "Hi @janchorowski! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232903). Thanks!",
      "Sorry, not sure I understand what's going on in this PR. It looks like you're adding handwriting recognition? That seems pretty cool, but please first open an Issue to discuss large new features. In this case, we'd most likely want to keep the implementation self-contained in the examples/ directory (similar to examples/speech_recognition) and ideally the implementation would reproduce results from a paper. Closing for now, but we'd be excited about this if you're open to refactoring things a bit! ðŸ˜„ ",
      "Hi, \r\n\r\nyes we are working on handwriting recognition with wav2vec2.0, but this was meant to be against our own fork, until things work.\r\n\r\nSorry for spamming you, we'll send the PR with a proper intro and issue when things do work.\r\n\r\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9bac254a19ac29c4fc",
    "number": 2900,
    "body": "Updating Model Path to https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec2_vox_960h.pt\r\n\r\n# Before submitting\r\n\r\n## What does this PR do?\r\nFixes #2899.\r\n\r\n\r\n\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Hi @harveenchadha! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232900). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "We've renamed the file in the S3 bucket.  Downloads should work now."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9cac254a19ac29c4fd",
    "number": 2891,
    "body": "Simple fix. Lines 498 and 499 are the same.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @gpengzhi! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232891). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9cac254a19ac29c4fe",
    "number": 2890,
    "body": "adding smaller models",
    "head_branch": "m2m_readme_update",
    "is_a_fork": false,
    "comments": [
      "@huihuifan merged this pull request in pytorch/fairseq@b55053373fb8678361a45a1d2c1b462befd9ab1a."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9dac254a19ac29c4ff",
    "number": 2881,
    "body": "## What does this PR do?\r\n\r\nFixes:\r\n\r\n- 2x `DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working`\r\n\r\n- 1x `/fairseq/optim/adam.py:98: DeprecationWarning: invalid escape sequence \\:`\r\n\r\nThis is with py38.\r\n",
    "head_branch": "deprecation",
    "is_a_fork": true,
    "comments": [
      "Hi @stas00! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232881). Thanks!",
      "CLA has been signed",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@myleott merged this pull request in pytorch/fairseq@0d03fbedcf79b63901b8718b4c61c525464cb198."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9eac254a19ac29c500",
    "number": 2880,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mstyp/scribblelens",
    "is_a_fork": true,
    "comments": [
      "Hi @janchorowski! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232880). Thanks!",
      "Similar to #2903, please see comment there: https://github.com/pytorch/fairseq/pull/2903#issuecomment-731650795",
      "Oh gosh, so sorry to spam you. I'll try to decouple the fork in github and be more careful!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621be9fac254a19ac29c501",
    "number": 2876,
    "body": "With this PR we start using flashlight bindings instead of wav2letter.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n",
    "head_branch": "use_flashlight",
    "is_a_fork": true,
    "comments": [
      "thanks for the pr!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea0ac254a19ac29c502",
    "number": 2871,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nUpdate WikiText-103 url\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea0ac254a19ac29c503",
    "number": 2869,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\nSorry, I didn't find a suitable place to update the doc. Would you please let me know where I should put the description for this new feature?\r\n- [x ] Did you write any new necessary tests?  \r\nI modified the example in the instruction here https://github.com/pytorch/fairseq/tree/master/examples/bart#evaluating-the-bartlargecnn-model \r\n```python\r\nfrom tqdm import tqdm\r\nimport torch \r\nfrom fairseq.models.bart import BARTModel \r\nimport json\r\n\r\nbart = torch.hub.load('pytorch/fairseq', 'bart.large.cnn')\r\nbart.cuda()\r\nbart.eval()\r\nbart.half()\r\ncount = 1\r\nbsz = 32\r\nwith open('test.source') as source, open('test.hypo', 'w') as fout:\r\n    sline = source.readline().strip()\r\n    slines = [sline]\r\n    for sline in tqdm(source):\r\n        if count % bsz == 0:\r\n            with torch.no_grad():\r\n                hypotheses_batch = bart.sample(slines, beam=5, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3, nbest=3)\r\n\r\n            for hypothesis in hypotheses_batch:\r\n                fout.write(json.dumps(hypothesis)  + '\\n')\r\n                fout.flush()\r\n            slines = []\r\n\r\n        slines.append(sline.strip())\r\n        count += 1\r\n    if slines != []:\r\n        hypotheses_batch = bart.sample(slines, beam=5, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3, nbest=3)\r\n        for hypothesis in hypotheses_batch:\r\n            fout.write(json.dumps(hypothesis) + '\\n')\r\n            fout.flush()\r\n```\r\n\r\n## What does this PR do?\r\nFixes #2848 . Add an argument for getting the top n best predictions from the model.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\nYes! Thanks!",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @yuchenlin! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232869). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea1ac254a19ac29c504",
    "number": 2846,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAdding --mask-multiple-length and --mask-stdev options to masked_lm task, allowing to mask sequences of multiple lengths when training a masked language model.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Hi @tuanh208! \n\nThank you for your pull request. We require contributors to sign our Contributor License Agreement, and yours needs attention.\n\nYou currently have a record in our system, but we do not have a signature on file. \n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232846). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "@alexeib merged this pull request in pytorch/fairseq@7cdef0a9bce575738ffb7b3c5fcad07181f149ac."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea2ac254a19ac29c505",
    "number": 2835,
    "body": "## What does this PR do?\r\nHi,\r\nThis PR is an augmentation option to mix noise audio in process `RawAudioDataset.post_process()`,\r\nsometimes we want to apply our ASR model just in some specific sound environments,\r\nto make the model more adaptive to the target environment,\r\nmixing the target environment sound into datasets while training should be better than `SpecAugment` to decrease `WER`.\r\n \r\n\r\nUsage:\r\nyou can add the args below when execute `train.py` or `example/speech_recognition/infer.py`:\r\n`--train-noise-dir`: str, A directory, put the soundfile readable files into it, the progress will automatically mix them into train datasets.\r\n`--valid-noise-dir`: str, A directory, put the soundfile readable files into it, the progress will automatically mix them into valid datasets.\r\n`--test-noise-dir`: str, A directory, put the soundfile readable files into it, the progress will automatically mix them into test datasets.\r\n`--noise-min-snr-db`: float, default=3, The minimum signal-to-noise ratio between audio and noise.\r\n`--noise-max-snr-db`: float, default=40, The maximum signal-to-noise ratio between audio and noise.\r\n\r\nP.S. To test if the noise really mixed into datasets properly or not, we should hack into `RawAudioDataset.post_process` to dump the mixed feats out by `sf.write`, and listen it by our EARS.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\nYES!\r\n",
    "head_branch": "noise-mixer",
    "is_a_fork": true,
    "comments": [
      "Hi @mychiux413! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232835). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "we have some internal impl based on https://github.com/facebookresearch/WavAugment, which i believe can also do additive noise. i have a dataset that can be used to wrap around audio dataset to add the noise, like this:\r\n\r\n```\r\n# Copyright (c) Facebook, Inc. and its affiliates.\r\n#\r\n# This source code is licensed under the MIT license found in the\r\n# LICENSE file in the root directory of this source tree.\r\n\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom .. import BaseWrapperDataset\r\n\r\n\r\nclass ChainRunner(object):\r\n    \"\"\"\r\n    Takes an instance of augment.EffectChain and applies it on pytorch tensors.\r\n    \"\"\"\r\n\r\n    def __init__(self, chain):\r\n        self.chain = chain\r\n\r\n    def __call__(self, x):\r\n        \"\"\"\r\n        x: torch.Tensor, (channels, length). Must be placed on CPU.\r\n        \"\"\"\r\n        x = x.view(1, -1)\r\n        src_info = {'channels': x.size(0),  # number of channels\r\n                    'length': x.size(1),  # length of the sequence\r\n                    'precision': 32,  # precision (16, 32 bits)\r\n                    'rate': 16000.0,  # sampling rate\r\n                    'bits_per_sample': 32}  # size of the sample\r\n\r\n        target_info = {'channels': 1,\r\n                       'length': x.size(1),\r\n                       'precision': 32,\r\n                       'rate': 16000.0,\r\n                       'bits_per_sample': 32}\r\n\r\n        y = self.chain.apply(x, src_info=src_info, target_info=target_info)\r\n\r\n        # sox might misbehave sometimes by giving nan/inf if sequences are too short (or silent)\r\n        # and the effect chain includes eg `pitch`\r\n        if torch.isnan(y).any() or torch.isinf(y).any():\r\n            return x\r\n        return y.squeeze(0)\r\n\r\n\r\nclass AudioAugmentDataset(BaseWrapperDataset):\r\n    def __init__(self, dataset):\r\n        super().__init__(dataset)\r\n\r\n        import augment\r\n        effect_chain = augment.EffectChain() \\\r\n            .pitch(\"-q\", self.random_pitch_shift) \\\r\n            .rate(\"-q\", 16_000) \\\r\n            .reverb(50, 50, self.random_room_size).channels() \\\r\n            .time_dropout(max_seconds=50. / 1_000)\r\n        # .additive_noise(noise_generator, snr=0, denormalize_noise=False) \\\r\n\r\n        self.runner = ChainRunner(effect_chain)\r\n\r\n    def random_pitch_shift(self):\r\n        return np.random.randint(-300, 300)\r\n\r\n    def random_room_size(self):\r\n        return np.random.randint(0, 100)\r\n\r\n    def __getitem__(self, index):\r\n        item = self.dataset[index]\r\n        item[\"original_source\"] = item[\"source\"]\r\n        item[\"source\"] = self.runner(item[\"source\"])\r\n\r\n        return item\r\n```\r\n\r\ndo you think it makes sense to rework your pr to use this kind of approach instead?",
      "That would be great to introduce WavAugment into fairseq framework, and it has even more augment functions.",
      "> this is looking good! could you please move the augment functionality into a wrapper dataset (like in my previous comment) and then choose to wrap or not wrap a dataset in audio_pretraining task based on a flag\r\n> \r\n> something like this:\r\n> \r\n> ```\r\n>         manifest = os.path.join(self.args.data, \"{}.tsv\".format(split))\r\n>         self.datasets[split] = FileAudioDataset(\r\n>             manifest,\r\n>             sample_rate=self.args.sample_rate,\r\n>             max_sample_size=self.args.max_sample_size,\r\n>             min_sample_size=self.args.max_sample_size,\r\n>             min_length=self.args.min_sample_size,\r\n>             pad=self.args.labels is not None or self.args.enable_padding,\r\n>             normalize=self.args.normalize,\r\n>         )\r\n> \r\n>         if getattr(self.args, 'augment', False) and split not in self.args.valid_subset:\r\n>             self.datasets[split] = AudioAugmentDataset(self.datasets[split])\r\n> ```\r\n\r\nI implemented the AudioAugmentDataset, but wrapping FileAudioDataset can make the `normalize` processing before `WavAugment`, I'm not sure whether it will affect the model while training with normalize.",
      "i believe wavaugment expects audio data to be normalized, and also rescaled to [-1,1] (which --normalize currently doesnt do). when i was playing around with this i was doing something like this:\r\n\r\n```\r\n        if self.normalize or self.rescale:\r\n            with torch.no_grad():\r\n                feats = F.layer_norm(feats, feats.shape)\r\n                if self.rescale:\r\n                    feats = feats / feats.abs().max()\r\n```",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea3ac254a19ac29c506",
    "number": 2833,
    "body": "Summary: Add support for filling masks using BART on a batch of sentences. This will be helpful when running on GPU\n\nReviewed By: myleott\n\nDifferential Revision: D24687773\n\n",
    "head_branch": "export-D24687773",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D24687773](https://phabricator.intern.facebook.com/D24687773)",
      "This pull request has been merged in pytorch/fairseq@de977736f91d23c53e6a60c45822973a615daa15."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea4ac254a19ac29c507",
    "number": 2825,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "savvi/output-test",
    "is_a_fork": true,
    "comments": [
      "Hi @ryooit! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232825). Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea5ac254a19ac29c508",
    "number": 2815,
    "body": "This PR reverts recent changes that attempted to make `--user-dir` work with non-unique module names. But that new approach introduced other issues (e.g., poor compatibility with multiprocessing and Windows), so let's revert to the previous simpler implementation.",
    "head_branch": "fix_user_dir",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@a4356b1da2b19ebd2e1be5c12ff882026ea4d7d2.",
      "wow, the update was very helpful! It solved the problem I just encountered. In the previous version, if I use my own task file and train on multiple GPUs it will raise ModuleNotFoundError: No module named 'fairseq_user_dir_11950'. "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea5ac254a19ac29c509",
    "number": 2813,
    "body": "â€¦d of cfg.task\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix2",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@9c66ff54c4acd8fa3280a9a5ab6d5fe58d1a2cf3."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea6ac254a19ac29c50a",
    "number": 2812,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # 2807.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYUP ðŸ™ƒ\r\n",
    "head_branch": "fix_fetch_model_state_from_cfg_wav2vec2_asr",
    "is_a_fork": true,
    "comments": [
      "Thanks again for the jumping in and sending the PR! Looks like @alexeib merged this in 65b02d529a45f687da8bbc6ec37611b8a9c96297"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea7ac254a19ac29c50b",
    "number": 2801,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1205\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/fix-dist-issue",
    "is_a_fork": true,
    "comments": [
      "Note: the current workaround on Windows is to always provide `--distributed-world-size 1`. ",
      "@myleott merged this pull request in pytorch/fairseq@3c726544d240f610cd35ea264d893d6a6ada074a."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea8ac254a19ac29c50c",
    "number": 2796,
    "body": "Fixed link.\r\n\r\n# Before submitting\r\n\r\n- [-] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [+] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [+] Did you make sure to update the docs?   \r\n- [-] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes link.\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@81677d751de120f69eef0c3eb36e849c977f7814."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea9ac254a19ac29c50d",
    "number": 2786,
    "body": "- Rename type -> key in fairseq/tasks/sentence_prediction.py (fixes #2746)\r\n- Update preprocessing docs (fixes #2565)\r\n- Turn off logging in test_fp16_optimizer.TestGradientScaling\r\n- Documentation updates\r\n- Remove some unused code\r\n- Fix noisychannel example (fixes #2213)",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@1bc83c703ad70d7f62c1e54b197e29b95d07b1f0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bea9ac254a19ac29c50e",
    "number": 2784,
    "body": "Configs can either be in `/fairseq/configs` (once the package is installed) or `/configs` (if using an editable installation). This centralizes the hydra init and supports these two possible config locations.",
    "head_branch": "hydra_init",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@01be083e46d2e4614dc274b0edf29d0ddd516186."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beaaac254a19ac29c50f",
    "number": 2778,
    "body": "",
    "head_branch": "fix_interactive",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beabac254a19ac29c510",
    "number": 2774,
    "body": "This will produce version strings like `1.0.0a0+3065963`, similar to PyTorch version strings.",
    "head_branch": "dynamic_versions",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@e0737c3c2985d2a71f0a30bb29f6d8741b4f87f3."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beacac254a19ac29c511",
    "number": 2773,
    "body": "Program to convert PDF to audio\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Thanks for the PR, @vaishnavi362! Can you explain the purpose of this? I can't seem to find any corresponding issues and it doesn't seem to really do anything that fairseq would be responsible for handling - just want to make sure we're understanding the motivation. ðŸ˜„ "
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beacac254a19ac29c512",
    "number": 2768,
    "body": "",
    "head_branch": "error_msg",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beadac254a19ac29c513",
    "number": 2767,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\nNo need I believe\r\n- [x] Did you write any new necessary tests?  \r\nNo\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/2724\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes! It is not a big PR at all but it allowed me to familiarize with the caching/downloading logic used in fairseq (which is very similar to that used in pytorch/transformers)\r\n",
    "head_branch": "request_wrap_exception",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beaeac254a19ac29c514",
    "number": 2762,
    "body": "",
    "head_branch": "fix_hub",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@9b0611e6786a048b6c4a70e36051027671d951a7."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beafac254a19ac29c515",
    "number": 2754,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nUpgrade args: `max_sentences` to `batch_size`\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-18",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@8248a12a6433f45b1757fac206f453f24b88403a."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beafac254a19ac29c516",
    "number": 2753,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? issue #1790\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1790.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "Thanks for contributing this!",
      "Thank you for adding wandb support! ðŸŽ‰ \r\nIt would be great if we can find a way to log all the hyperparameters. This is super important for the reproducibility and, in my opinion, is an essential part of wandb.\r\n\r\nInitially, this was implemented via adding optional parameter `args` to `progress_bar` function and `WandBProgressBarWrapper` constructor, but if you don't think this is a good idea, we can figure out some other way. Maybe via updating `wandb.config` before the train loop starts.\r\n\r\nAlso, the latest versions of tensorboard have hparams logging, so we can probably figure out a more universal implementation.",
      "@Guitaricet it saves command. In my opinion that's enough\r\n![image](https://user-images.githubusercontent.com/9883873/97468254-1f6c3880-1956-11eb-9965-4a7abd872d04.png)\r\n",
      "Command saving is really good, but the upsides of logging the actual hparams are:\r\n\r\n* You can search for particular hparams in wandb and plot with legend that shows hparams. This is one of the main reasons why I, personally, use wandb.\r\n* You can compare the runs in wandb interface.\r\n* All hparams are logged, even if you didn't specify them explicitly. This may help a lot if the default params change, plus you don't need to look into the code to figure out the hparams if you need them.\r\n\r\n\r\nJust to illustrate, here are some plots that wouldn't been possible without hparams logging\r\n\r\nHparam tuning:\r\n\r\n<img width=\"1687\" alt=\"Screen Shot 2020-10-28 at 1 33 53 PM\" src=\"https://user-images.githubusercontent.com/2821124/97474197-3b55e700-1922-11eb-836f-83184953916a.png\">\r\n\r\nLegend that depends on hparam values:\r\n\r\n<img width=\"532\" alt=\"Screen Shot 2020-10-28 at 1 36 00 PM\" src=\"https://user-images.githubusercontent.com/2821124/97474484-866ffa00-1922-11eb-9020-79fa5c65a026.png\">\r\n",
      "Hi @CrafterKolyan! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232753). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "Merged in 1a709b2a401ac8bd6d805c8a6a5f4d7f03b923ff"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb0ac254a19ac29c517",
    "number": 2745,
    "body": "## What does this PR do?\r\nFixes #2744 \r\n\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@7a3f20d0fad62d696aed17d801b840d5c25cc4f5."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb1ac254a19ac29c518",
    "number": 2735,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-readme",
    "is_a_fork": false,
    "comments": [
      "@xianxl merged this pull request in pytorch/fairseq@05a5232d04a6e5eccf0e1392b17b4908e5035d44."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb2ac254a19ac29c519",
    "number": 2733,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nIt's sufficient to set logging.basicConfig in the most outside calling code like train.py or generate.py. Actually the setting of logging.basicConfig () (like [here](https://github.com/pytorch/fairseq/blob/master/fairseq_cli/generate.py#L54)) will been overwritten if logging.basicConfig is set in the inner part of the whole code.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix2",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@d6f2c907be8f7351195184981e4f3a9e003a4258."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb3ac254a19ac29c51a",
    "number": 2732,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "examples-translation",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb4ac254a19ac29c51b",
    "number": 2726,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAdds Windows CI support so we can ensure fairseq is and remains Windows-compatible. ðŸ˜„ \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/add-windows",
    "is_a_fork": true,
    "comments": [
      "~~Contrary to what CI says, the real errors arise in a difference in how garbage collection happens in Windows. Specifically, memmaps are not being released/closed appropriately due to strong references being held. The big offender is `MMapIndexedDataset`. This causes every test in `test_binaries.py` to fail (and anything that uses calls from that file).~~",
      "~The last remaining hurdle is a flaky assertion in sequence generator. ðŸ¤”~",
      "~~The flaky test here looks like it happens on all platforms (see [here](https://github.com/pytorch/fairseq/issues/2749) for more context), so this doesn't need to necessarily wait for that since it's broken already. ðŸ˜‰   Once I address some of the hacky stuff to make tests pass here with raw datasets, hopefully this can land.~~",
      "~~Ok, the lingering fixes are both related to how raw datasets work. Specifically it doesn't seem like the language codes get appended to the files that are read by the `RawIndexedTextDataset`. See:~~\r\n\r\n- ~[here](https://github.com/pytorch/fairseq/pull/2726/commits/5212150f8de108bab9b1d9a82602070f271a506c#diff-79363775c289bed05f79e4cb99e1af48ebd135f9d0a632b8204249a2ef962c8dR94)~\r\n- ~[here](https://github.com/pytorch/fairseq/pull/2726/commits/956010c4798f771fd167dd6f54f0b5d8c16acfa6#diff-bdcc431c39fa533547a7183b9e3e3f94fb12c94e0d770cc5473d11002b585b39R198)~\r\n\r\n~~I think maybe it should be the responsibility of [`fairseq.data.indexed_dataset.make_dataset`](https://github.com/pytorch/fairseq/blob/master/fairseq/data/indexed_dataset.py#L57) to accept a language code (from args) and append that to the filename - this seems least invasive. Do you have any thoughts, @myleott?~~",
      "Ok, I think I've got something in place for the hacky solution before. ",
      "This looks awesome! Will import and review in a bit.",
      "I can't peer into the black box to see where the errors are, but if there are things I can do to help it land I'm happy to.",
      "Oh, it actually looks like the delta between this and [this PR](https://github.com/pytorch/fairseq/pull/2733) is a single linter advice and a single test warning so maybe this isn't such a big effort. ðŸ˜„ ",
      "It also doesn't seem like the datasets are being prefetched, which is causing (fatal) cache misses. ðŸ¤” ",
      "OK, @myleott - I've fixed the symlink issue and switched everything to using cached datasets. It looks like there were a few minor issues with prefetching that I've addressed, but I think this is good for a (hopefully) final review whenever. ðŸ˜º ",
      "Any chance of importing this to make sure I haven't broken anything, @myleott? If a review has to wait a bit, that's OK.",
      "I've cleaned this PR up so it's hopefully easier to review. Let me know if you need anything else from me, @myleott ",
      "Any thoughts here, @myleott? The only \"core\" fairseq changes (i.e., not changes to tests) are: \r\n\r\n- [closing the file handler](https://github.com/pytorch/fairseq/pull/2726/commits/d0ce5f70e3066ad8d84b24734bee6aaf827d1c13)\r\n- [fixing dtypes to `int64`](https://github.com/pytorch/fairseq/pull/2726/commits/9e1f527dde4d8235b99f828606c9dc3177572f76)\r\n- [prefetching fixes](https://github.com/pytorch/fairseq/pull/2726/commits/8b1a0e7ac3d6ca96db519132e861c50870c6d031)",
      "Any chance you could review/merge this, @myleott or @alexeib? It's been imported a handful of times without review so I have to keep fixing conflicts when upstream diverges. ",
      "Sorry for the delay and thanks for rebasing.\r\n\r\nA quick status update on this. We'd like to avoid duplicated versions of each test (one with mmap and another with cached), so we'll probably merge a simplified version of this diff if we can get mmap working on Windows. Having looked into it a bit, the problem seems to be that Windows refuses to delete the temporary directories in the tests while the mmap is open, and technically the tempfile.TemporaryDirectory context manager closes before the mmap is garbage collected.\r\n\r\nI think tests should work if we do something like this instead:\r\n```python\r\nclass TestLanguageModeling(unittest.TestCase):\r\n    def setUp(self):\r\n        logging.disable(logging.CRITICAL)\r\n        self._tmp_dir = tempfile.TemporaryDirectory()\r\n        self.data_dir = self._tmp_dir.name\r\n\r\n    def tearDown(self):\r\n        logging.disable(logging.NOTSET)\r\n        self._tmp_dir.cleanup()\r\n\r\n    # everything else as before, but use `self.data_dir` instead of the context manager\r\n```\r\n\r\nI started along this path and things seem to be working, just need to find some time to get it over the finish line. I'll try to merge it later in the week.\r\n\r\nLonger term, to set expectations, I do worry that Windows support will continue to break, since I don't think any of our production or research teams use Windows ðŸ˜¢ But in any case having CI coverage will be helpful.",
      "> I think tests should work if we do something like this instead ...\r\n\r\nClever. :-) I'm happy to rework that if you'd like.\r\n\r\n> Longer term, to set expectations, I do worry that Windows support will continue to break\r\n\r\nUndoubtedly yes. My hope is to begin adding tests to increase coverage to the point that we have some confidence in cross-platform parity. Adding CI is a first-step and I hope that it operates as due diligence for Windows compatibility much like every other aspect of CI; i.e., if some tests fail, the PR cannot land. \r\n\r\n",
      "> Clever. :-) I'm happy to rework that if you'd like.\r\n\r\nAh thanks, that would be very helpful. So basically revert to the original mmap-based tests everywhere, but using this new tearDown pattern ðŸ˜„ ",
      "I don't know why pycharm dorked up the formatting, but if you can share your black config I can reformat as a part of this. â˜¹ï¸ ",
      "The failures are because of temp directories that are created in multilingual transformer tests. I've mis-ported these because it's not a static data directory like other tests, but a data directory per lang pair. I need to give this some more thought...",
      "> I don't know why pycharm dorked up the formatting, but if you can share your black config I can reformat as a part of this.\r\n\r\nWe plan to provide configs and possibly even commit hooks later, but for now I think standard `black` is fine.\r\n\r\n> The failures are because of temp directories that are created in multilingual transformer tests. I've mis-ported these because it's not a static data directory like other tests, but a data directory per lang pair. I need to give this some more thought...\r\n\r\nAh, you can move this to a new test class",
      "It still doesn't work on Windows â˜¹ï¸ I'll give this another look later, but mmap is causing a lot of issues.",
      "The current strategy is quite hacky, but basically it defers temp directory cleanup until gc by which point it seems like the task has been gc'd, too.",
      "Finally. ðŸ˜…  OK, @myleott - this change now has a much smaller surface area.",
      "OK, this has been rebased and seems to be working again.",
      "This failure is somewhat mysterious to me - @alexeib or @myleott, do you have any idea why apex would fail to be found in my tests but not yours? ",
      "Oh, I think it's because my test runner in this PR (`pytest`) discover the tests in the submodule and yours (`python setup.py test`) does not. By discovering it, it tries to import something with a strict dependency on apex. This seems like a bug in master currently.",
      "Is this waiting on anything? ",
      "Now that the binary tests have been reworked to support mmap, are there any other blockers here @myleott? ",
      "Sorry to keep harassing you all, but it takes a fair bit of mental effort to keep this branch up to date and to fix conflicts and other issues resulting from upstream changes. Are there plans to eventually merge this or should I abandon it, @myleott?",
      "Since this has lost interest from maintainers, I'm going to assume there's no hope of this landing.",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb5ac254a19ac29c51c",
    "number": 2723,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThe [recent commit](https://github.com/pytorch/fairseq/commit/5379461e613263911050a860b79accdf4d75fd37) introduce LM rescoring in SequenceGenerator. I think the LM should wrap an Incremental_decoder where a forward pass with incremental_state is being used. But in SequenceGenerator, it is doing a forward pass without an incremental_state. This PR is attempting to resolve this.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [
      "@myleott I think an LM with `incremental_state` is the only use case here. What do you think?",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb5ac254a19ac29c51d",
    "number": 2722,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix link in CamemBERT readme\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@4948d890a4701170c5a84b62cfd310e08af39273."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb6ac254a19ac29c51e",
    "number": 2717,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1948\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n\r\nThis may be something to discuss - it seems like there's a lot of confusion about what features are supported when in fairseq. Hopefully versioning will allow for more discrete cuts.",
    "head_branch": "fix/bump-version",
    "is_a_fork": true,
    "comments": [
      "please update to `1.0.0a0` and then I'll import/merge it",
      "Done!",
      "@myleott merged this pull request in pytorch/fairseq@d6cdc2f47b74e3126df748c0da02be43d7356a07."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb7ac254a19ac29c51f",
    "number": 2716,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2693\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hotfix/2693",
    "is_a_fork": true,
    "comments": [
      "I can't tell if this is a related error or not. It seems like #2717 failed similarly, but that was such a small change that I somehow doubt the failure was related. ðŸ˜„ Is CI currently passing on the FB side with what's in the main branch?",
      "@myleott merged this pull request in pytorch/fairseq@e0d5d8e669528be579d7aa4749fbcfe5cacdce90."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb8ac254a19ac29c520",
    "number": 2715,
    "body": "Changing so attention is returned for joint alignment example.\r\n\r\nrelated to this issue:\r\nhttps://github.com/pytorch/fairseq/issues/2695\r\nAnd this one:\r\nhttps://github.com/pytorch/fairseq/issues/2634\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@f910ea9d4cf9c9964ec307dde3144622c4b61e62."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb8ac254a19ac29c521",
    "number": 2708,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "mstyp/scribblelens",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beb9ac254a19ac29c522",
    "number": 2703,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nOpensource code for Deep Transformer with Latent Depth (https://arxiv.org/pdf/2009.13102.pdf).\r\n\r\nNew features and design choices made:\r\n\r\n- New feature: allow non-residual block to be weighted by sample z (generated per batch) instead of `x = residual + x`.\r\n- Design choice: move  `x = residual + x` in transformer_layer.py into a function where the subclass (with latent depth) could overwrite it to `x = residual + z*x`.\r\n\r\n\r\n- New feature: allow TransformerEncoder or TransformerDecoder to have additional logits parameters which will generate the samples z.\r\n- Design choice: added subclass LatentTransformerEncoder and LatentTransformerDecoder, which has additional attributes for the logits parameters, and instantiate the corresponding LatentTransformerEncoderLayer and LatentTransformerDecoderLayer.\r\n\r\n- New feature: allow multilingual_translation task to train with latent depth (results in the paper).  \r\n- Design choice: \r\n  - added additional arguments in the multilingual_translation task. \r\n  - added option for multilingual_transformer to use LatentTransformerEncoder and LatentTransformerDecoder besides standard TransformerEncoder.\r\n  - added option in multilingual_translation task's `train_step` to generate the samples z and compute the KL (and sparsity) loss per batch.\r\n\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "latent-depth-oss",
    "is_a_fork": false,
    "comments": [
      "@xianxl merged this pull request in pytorch/fairseq@573c2f4b60a50dc7c4ff17084b753c05452381f9."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebaac254a19ac29c523",
    "number": 2692,
    "body": "## What does this PR do\r\n* I fixed a bug of sentence permutation in denoising tasks.\r\nThis bug was that sentence permutation does not work at all.\r\n* And I modify the default settings of the bart architecture to avoid an error that occurs when finetuning models which are pretrained by myself.\r\n",
    "head_branch": "fix_sentence_permutation",
    "is_a_fork": true,
    "comments": [
      "Hi @utanaka2000! \n\nThank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file.\n\nIn order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.\n\nIf you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20pytorch%2Ffairseq%20%232692). Thanks!",
      "Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebbac254a19ac29c524",
    "number": 2687,
    "body": "",
    "head_branch": "fix_hub",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@7c292af66f61b1125854218519bf81d494e5b11e."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebcac254a19ac29c525",
    "number": 2679,
    "body": "Fixes links in README",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Will merge https://github.com/pytorch/fairseq/pull/2657, which has one more fix as well. Thanks for the PR!"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebcac254a19ac29c526",
    "number": 2675,
    "body": "Fixes #2673.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2673 (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_issue_2673_full_context_alignment",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@c049749c7a7c08cca9e4663c85bd3961f4b260f8."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebdac254a19ac29c527",
    "number": 2670,
    "body": "",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@caea771afafbcb3471f9007ca1cd46a4d3d8c869."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebeac254a19ac29c528",
    "number": 2668,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n\r\nIn issue #2651, #2618, Some researcher wants a single python script that loads and runs inference with wav2vec 2.0 pre-trained model on a single wav file or on a programmatically loaded waveform signal.\r\n\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n\r\nYes, I did.\r\n\r\n- [ ] Did you make sure to update the docs?   \r\n\r\nI clone today.\r\n\r\n## What does this PR do?\r\n  \r\nAdded a code that gives the inference of the Wav2vec-2.0 model to one audio file. like following:\r\n* Command\r\n```\r\n$ python3 examples/wav2vec/recognize.py --wav_path $WAV_PATH --w2v_path $W2V_PATH --target_dict_path $TARGET_DICT_PATH\r\n```\r\n* Output\r\n```\r\nI LOVE THEE FREELY AS MEN STRIVE FOR RIGHT I LOVE THEE PURELY AS THEY TURN FROM PRAISE\r\n```\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nYes !! \r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "I made only one independent `recognize.py` without touching other codes.",
      "Hmm... I don`t know what is wrong. I just add recognize.py for one audio inference.",
      "This is awesome by the way @sooftware ! Any chance you could merge master and update your PR? I'm looking forward to playing with this!\r\n\r\nFor instance `base_architecture` is no longer (or isn't) in the `fairseq.models.wav2vec.wav2vec2_asr` module :) \r\n\r\nThanks again!\r\n",
      "I have some problems with merging this code. There was a lot of change when Fairseq version was upgraded.  \r\nIf you use Fairseq 0.9.0 version, you can refer to [this issue](https://github.com/pytorch/fairseq/issues/2651) for information.",
      "I'm on master. I might try to have a look to see which parts changed and if I can help port this PR over in anyway later if I find time :) Thanks for your quick answer.",
      "I've created pull request https://github.com/pytorch/fairseq/pull/3244 for similar purpse."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bebfac254a19ac29c529",
    "number": 2663,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # 2662.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@e4a5427ef4ffad63fa265acbade098bb963a814b."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec0ac254a19ac29c52a",
    "number": 2657,
    "body": "Typo fixes.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "Also reported in #2431.",
      "@myleott merged this pull request in pytorch/fairseq@fc1c38aa1c70e1d1ef45a6af335e3c6571ba436d."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec0ac254a19ac29c52b",
    "number": 2646,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThis adds a new decoding strategy `search.PrefixConstrainedBeamSearch` that limits the vocabulary of the next token generation given a prefix (that is the previously generated tokens during beam search). An end user has just to give the optional argument `prefix_allowed_tokens_fn` to `.generate` or `.sample` to activate `PrefixConstrainedBeamSearch`. `prefix_allowed_tokens_fn(batch_id, tokens)` is a callback function that given the `batch_id` and `tokens` returns the list of allowed token for the next generation step.\r\n\r\n## Did you have fun?\r\nYES! ðŸ™ƒ\r\n",
    "head_branch": "add_PrefixConstrainedBeamSearch",
    "is_a_fork": true,
    "comments": [
      "The test failed on something that is not part of the pull request\r\n",
      "> @myleott has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/intern/diff/D24006805/).\r\n\r\nI am not a Facebook employee so I cannot see the warnings and why this fails.",
      "> > @myleott has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/intern/diff/D24006805/).\r\n> \r\n> I am not a Facebook employee so I cannot see the warnings and why this fails.\r\n\r\nI'm taking care of this :)",
      "@myleott merged this pull request in pytorch/fairseq@086fe1c5d1317caad090b2ff60f965d2dfa130f7."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec1ac254a19ac29c52c",
    "number": 2642,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nProvides an option to pad the sequence lengths to a multiple. We found this provided significant speed-up when using tensor-cores (multiple of 8). Currently the code only allows for the number of sequences in a batch to be a multiple of 8 which we found to be not as important as padding the batch length to a multiple of 8.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "pad_seq_lengths",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@22007c4419da1108af2f5ac54560c73f047e7b36."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec2ac254a19ac29c52d",
    "number": 2641,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n* The current README **incorrectly states** that Sockeye 2 no longer supports constrained decoding.\r\n* Other minor updates\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec3ac254a19ac29c52e",
    "number": 2623,
    "body": "Summary: Title, testing.\n\nDifferential Revision: D23727758\n\n",
    "head_branch": "export-D23727758",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D23727758](https://phabricator.intern.facebook.com/D23727758)",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec4ac254a19ac29c52f",
    "number": 2611,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nIn some cases, we want to use the decoder-only model from a pretrained transformer (`encoder_attn` is not None).\r\n\r\nThis commit is minor but important, which not only makes `TransformerDecoderLayer` more robust but also make it compitible with decoder-only model from pretrained transformer.\r\n\r\nIf you want to use decoder from pretrained transformer, you can just set `encoder_out` as None.\r\n\r\n\r\n\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-17",
    "is_a_fork": true,
    "comments": [
      "@myleott merged this pull request in pytorch/fairseq@cad08709450714c8a815836c98fcfcee6aff09b0."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec4ac254a19ac29c530",
    "number": 2607,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFix tests from d47067937abacfe87f2963adca8daeada3c631fe\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "lematt/fix-device-error",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec5ac254a19ac29c531",
    "number": 2601,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nRemove dummy variable\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-16",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec6ac254a19ac29c532",
    "number": 2596,
    "body": "For some reason, this fixes flake8 errors, no idea why...\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [
      "@lematt1991 merged this pull request in pytorch/fairseq@df45f42efdaab751e698726bc1922f7643aa9276."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec7ac254a19ac29c533",
    "number": 2594,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nIt is more preferable to use `cls` rather than the actual class name to construct an instance.\r\n\r\nActual class name may cause issues if you subclass it and use the class method.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-15",
    "is_a_fork": true,
    "comments": [
      "@lematt1991 merged this pull request in pytorch/fairseq@9e4088bc3d2630d5a4285138662fed4426190e73."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec8ac254a19ac29c534",
    "number": 2587,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/2574 (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bugfix/fix-apply_sparse_mask-signature",
    "is_a_fork": true,
    "comments": [
      "Also fixed a couple of error messages in `speech_recognition` (formatting issues)",
      "@lematt1991 I did a rebase, and now all checks pass. However, now there is a lot of changes displayed in this request which are already committed to the repo by the developers...\r\n\r\n:smile:, it appears to be a known GutHub issue https://stackoverflow.com/questions/16306012/github-pull-request-showing-commits-that-are-already-in-target-branch. I changed the target branch, then brought back the master branch as the target one. And now in the `Files changed` tab everything OK)",
      "@lematt1991 merged this pull request in pytorch/fairseq@5e831033069b52b09905e0bf8ba104d016e04efd."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec8ac254a19ac29c535",
    "number": 2585,
    "body": "Summary: title\n\nDifferential Revision: D23581045\n\n",
    "head_branch": "export-D23581045",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D23581045](https://phabricator.intern.facebook.com/D23581045)",
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bec9ac254a19ac29c536",
    "number": 2567,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1263\n\nThe PyTorch Mobile liter interpreter does not support object creation (CREATE_OBJECT).\nI tried using `sorted` and `lambda` to replace the sorting code but TorchScript does not support lambda functions. This change enables to save a scripted model to the lite interpreter. I can make as much testing as possible and push this as a core change, that would benefit on-device MT and on-device sequence modeling in general. Or I'm happy to make this an experimental change with an `fb_` file.\n\nReviewed By: myleott, jhcross\n\nDifferential Revision: D23440771\n\n",
    "head_branch": "export-D23440771",
    "is_a_fork": true,
    "comments": [
      "This pull request was **exported** from Phabricator. Differential Revision: [D23440771](https://phabricator.intern.facebook.com/D23440771)",
      "This pull request was **exported** from Phabricator. Differential Revision: [D23440771](https://phabricator.intern.facebook.com/D23440771)"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621becaac254a19ac29c537",
    "number": 2564,
    "body": "Fixes #2563.\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621becbac254a19ac29c538",
    "number": 2562,
    "body": "avoid the error when the hypotheses are not in the 3rd column\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beccac254a19ac29c539",
    "number": 2555,
    "body": "Fixes ( https://github.com/pytorch/fairseq/issues/1485 ), ( https://github.com/pytorch/fairseq/pull/1530 ).\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/1485 and https://github.com/pytorch/fairseq/pull/1530.",
    "head_branch": "return_decoder_self_attn",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beccac254a19ac29c53a",
    "number": 2551,
    "body": "- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1484 and https://github.com/pytorch/fairseq/pull/1532.\r\n",
    "head_branch": "return_encoder_attn",
    "is_a_fork": true,
    "comments": [
      "This pull request has been automatically marked as stale. **If this pull request is still relevant, please leave any comment** (for example, \"bump\"), and we'll keep it open. We are sorry that we haven't been able to prioritize reviewing it yet. Your contribution is very much appreciated.\n",
      "Closing this pull request after a prolonged period of inactivity. If this issue is still present in the latest release, please ask for this pull request to be reopened. Thank you!\n"
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621becdac254a19ac29c53b",
    "number": 2529,
    "body": "This pull request implements a variant of the Transformer model that uses an attention distribution for pointing to input words. The attention distribution over the input words is interpolated with the normal output distribution over the vocabulary words, as in [See et al. (2017)](https://arxiv.org/abs/1704.04368). This allows the model to generate words that appear in the input, even if they don't appear in the vocabulary, helping especially with small vocabularies.\r\n\r\nThe mechanism for copying out-of-vocabulary words from the input has been implemented differently to See et al. In their [implementation](https://github.com/abisee/pointer-generator) they convey the word identities through the model in order to be able to produce out-of-vocabulary words. We wanted to minimize changes to the Fairseq code base and took a different approach, which I'll describe below. The entire implementation is contained in one file (plus there's one new test).\r\n\r\nCopying out-of-vocabulary words is possible by pre-processing the input and post-processing the output. The user may add special words to the end of the vocabulary that can be used in place of `<unk>` tokens to identify different input positions (e.g. `<unk-0>`, `<unk-1>`, `<unk-2>`, ...). The number of these special words is given to the model with the `--source-position-markers` argumentâ€”the model simply maps all of these to the same word embedding as `<unk>`. With a simple post-processing the user may retrieve word at position N in the original text and use it in place of `<unk-N>`.\r\n\r\nI didn't find a good place to document this usage of this model, so let me know if you think I should improve documentation somewhere.\r\n\r\nThis feature has not yet been discussed via a GitHub issue, but I'll open a new issue for discussion.\r\n",
    "head_branch": "transformer_pointer_generator",
    "is_a_fork": true,
    "comments": [
      "Re: documenting, would it make sense to add a README to a new `examples/pointer_generator_networks/` directory? You could also clarify some of the usage and implementation differences there.",
      "> Re: documenting, would it make sense to add a README to a new `examples/pointer_generator_networks/` directory? You could also clarify some of the usage and implementation differences there.\r\n\r\nThat's a good idea. I added documentation and some example scripts.\r\n\r\nAlso, I noticed that the pull request doesn't have the CLA Signed tag. I assume because my e-mail in the commits was in lowercase letters, while in the CLA it's capitalized. I fixed the commits (using \"git commit --amend --author\"), but it looks like you may have to rerun the bot.",
      "@myleott could you check why this still doesn't have the CLA Signed tag?",
      "@senarvi Can't wait for this model to be part of FairSeq. Thanks! Would you provide a concrete training script in the [README.md](https://github.com/nuance-research/fairseq/blob/transformer_pointer_generator/examples/pointer_generator/README.md) with any toy dataset? I think your <unk-0,1,..> idea is awesome. I just hope the documentation will be a little better.",
      "> @senarvi Can't wait for this model to be part of FairSeq. Thanks! Would you provide a concrete training script in the [README.md](https://github.com/nuance-research/fairseq/blob/transformer_pointer_generator/examples/pointer_generator/README.md) with any toy dataset? I think your <unk-0,1,..> idea is awesome. I just hope the documentation will be a little better.\r\n\r\nThanks! I added this page with detailed instructions how to run this on the XSum task: https://github.com/nuance-research/fairseq/blob/transformer_pointer_generator/examples/pointer_generator/README.xsum.md",
      "@myleott merged this pull request in pytorch/fairseq@3b7d85c91f0afaa8b78a3bcb9b216a2ff38c1b01."
    ],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beceac254a19ac29c53c",
    "number": 2525,
    "body": "",
    "head_branch": "can_reuse_epoch_itr",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621becfac254a19ac29c53d",
    "number": 2524,
    "body": "",
    "head_branch": "misc",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed0ac254a19ac29c53e",
    "number": 2521,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2483\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hotfix/fix-2483",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed0ac254a19ac29c53f",
    "number": 2520,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nSuggest `fairseq.types` as a way to easily access some commonly used data types such as those involving\r\nincremental states and transformer encoder.\r\n\r\n## Motivation\r\n\r\nIt's very tedious to have to copy and paste `Dict[str, Dict[str, Optional[Tensor]]]` over and over and some commonly used data types like `EncoderOut` are located in not so convenient locations.\r\nSometimes, we just wanted to import the type for type hints but as things stand, it's a little inconvenient.\r\n\r\nCommonly used types can be stored in `fairseq.types` as an easy way for developers to access various `fairseq` related datatypes.\r\n\r\nIf it's not immediately obvious, this PR is WIP. Let me know if I should continue.",
    "head_branch": "feature/types",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed1ac254a19ac29c540",
    "number": 2511,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed2ac254a19ac29c541",
    "number": 2505,
    "body": "",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed3ac254a19ac29c542",
    "number": 2500,
    "body": "Added mention of D-, T-, A- and E- output line types. Missing I-.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2499.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ : ðŸ™‚\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed3ac254a19ac29c543",
    "number": 2492,
    "body": "",
    "head_branch": "misc_fixes2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed4ac254a19ac29c544",
    "number": 2491,
    "body": "We were previously a bit too lenient with boundary conditions to support `CountingIterator.take`. Let's instead handle this more explicitly.",
    "head_branch": "itr_bounds2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed5ac254a19ac29c545",
    "number": 2490,
    "body": "There have been issues with some dynamic datasets where the iteration count stored in the checkpoint overflows the actual iterator size, but we've been unable to reproduce it in any reliable way. This overflow can apparently cause the epoch to advance when loading checkpoints, which is undesirable.\r\n\r\nThis PR changes two things. First at the end of an epoch we advance the iterator to the next epoch directly in `state_dict`, so that we can distinguish this overflow corner case and the more typical end-of-epoch situation. We then raise an exception in the case of iterator overflow, which will hopefully help us (via the community) find a more reliable repro for the underlying issue.",
    "head_branch": "itr_bounds",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed6ac254a19ac29c546",
    "number": 2477,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nreplace PadDataset with RightPadDataset\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-14",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed7ac254a19ac29c547",
    "number": 2473,
    "body": "Minor fixes to the documentation for running wav2vec 2.0.",
    "head_branch": "w2v",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed8ac254a19ac29c548",
    "number": 2464,
    "body": " # What does this PR do ? \r\n\r\n--criterion changed from binary_cross_entropy to wav2vec in the wav2vec README.md\r\nIt fixes the issue reported there :  https://github.com/pytorch/fairseq/issues/2434\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed8ac254a19ac29c549",
    "number": 2458,
    "body": "Adding `\\` will break the command and produce an error to the optimizer.\r\n\r\n# Before submitting\r\n- [ No ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ Yes ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ Yes ] Did you make sure to update the docs?   \r\n- [ Yes ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThe command produces an error because of `\\`. A small update is just to remove it\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bed9ac254a19ac29c54a",
    "number": 2455,
    "body": "## What does this PR do?\r\nImplements R3F and R4F coming from Facebook Research: https://arxiv.org/abs/2008.03156\r\n\r\nThis code was used to generate all the results from the paper excluding probing results.",
    "head_branch": "finetuning_rxf",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedaac254a19ac29c54b",
    "number": 2448,
    "body": "",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedbac254a19ac29c54c",
    "number": 2447,
    "body": "",
    "head_branch": "fix_legacy_ddp",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedbac254a19ac29c54d",
    "number": 2445,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nIt collate/pack more input datas into batches besides 'source' and 'target'.\r\n\r\nGeneralized function is more favorable to fit various collater. e.g. pack `src_lang_id` into batches\r\nhttps://github.com/pytorch/fairseq/blob/488254c88030d4e1fbfb85dbb8a90c97256bf491/fairseq/data/multilingual/sampled_multi_dataset.py#L285-L286\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-13",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedcac254a19ac29c54e",
    "number": 2444,
    "body": "â€¦utedDataParallel. Otherwise there will be a RuntimeError at [this line](https://github.com/pytorch/fairseq/blob/master/fairseq/legacy_distributed_data_parallel.py#L91) while doing validation with LegacyDDP.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beddac254a19ac29c54f",
    "number": 2430,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n`dictionary` is not a required parameter for `load_indexed_dataset`.\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-12",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedeac254a19ac29c550",
    "number": 2429,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\n### 1. Enable `already_numberized` dataset for preprocess\r\n\r\nThis feature enable binarization for any `already_numberized` dataset.\r\n\r\n### 2. Enable named binary file for  only-source dataset\r\n\r\nBefore this commit:  `fairseq-preprocess --only-source ...` will export training data as `train.bin` and `train.idx`\r\n\r\nAfter this commit:  `fairseq-preprocess --only-source --source-lang en ...` will export training data as `train.en.bin` and `train.en.idx`, which is more flexible and clear.\r\n\r\n## Related src code\r\n\r\nRelease `already_numberized` parameter to `fairseq-preprocess`.\r\n\r\nhttps://github.com/pytorch/fairseq/blob/8449c5f4e85d7658e533ffae3dac716d04cb2f0e/fairseq/binarizer.py#L24-L36\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-11",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedfac254a19ac29c551",
    "number": 2426,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "wav-format",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bedfac254a19ac29c552",
    "number": 2402,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nThis PR implements constrained decoding ([Hokamp & Liu, 2017](https://www.aclweb.org/anthology/P17-1141/); [Post & Vilar, 2018](https://www.aclweb.org/anthology/N18-1119/)) with vectorization for batching ([Hu et al., 2019](https://www.aclweb.org/anthology/N19-1090/)). In addition, it add *ordered constraints*, where the constraints are generated on the target side in order, with zero or more unconstrained tokens in between. This variant allows for optimizations that increase speed and BLEU scores (when testing with random scraps from the references).\r\n\r\n### Usage and quick start\r\n\r\nIt works with `fairseq-interactive` via a new command-line option: `fairseq-interactive --constraints [ordered,unordered]`, defaulting to `ordered` if nothing is provided. When active, it will split lines from STDIN on `\\t`, with separate constraints each separated by a tab. For example (after downloading the [Fairseq WMT19 German--English model](https://github.com/pytorch/fairseq/blob/master/examples/wmt19/README.md)):\r\n\r\n```bash\r\necho -e \"Die maschinelle Ãœbersetzung ist schwer zu kontrollieren.\\thard\\tinfluence\" \\\r\n  | [normalize.py](https://gist.github.com/mjpost/4c54446b7030d7c64b57461d27090650) \\\r\n  | [tok.py](https://gist.github.com/mjpost/ed7456f6a987c533102fc121678ed302) \\\r\n  | PYTHONPATH=$HOME/code/fairseq-constraints fairseq-interactive $modeldir \\\r\n  --bpe fastbpe \\\r\n  --bpe-codes $modeldir/bpecodes \\\r\n  --constraints \\\r\n  --constraints-both\r\n  -s de -t en \\\r\n  --path $modeldir/model1.pt \\\r\n  --max-tokens 1000 \\\r\n  --beam 5 \\\r\n```\r\n\r\nAdding the `--constraints-both` option causes it to batch-decode the input sentence both with and without the constraints. When run with the Fairseq WMT19 German--English model, the following results are produced (here run on a CPU, don't be alarmed by the times!)\r\n\r\n```text\r\nS-0     Die masch@@ in@@ elle Ãœber@@ setzung ist schwer zu kontrollieren .\r\nW-0     1.844   seconds\r\nC-0     hard\r\nC-0     influence\r\nH-0     -1.5333266258239746     Mach@@ ine trans@@ lation is hard to influence .\r\nD-0     -1.5333266258239746     Machine translation is hard to influence .\r\nP-0     -0.5434 -0.1423 -0.1930 -0.1415 -0.2346 -1.8031 -0.1701 -11.7727 -0.1815 -0.1511\r\nS-0     Die masch@@ in@@ elle Ãœber@@ setzung ist schwer zu kontrollieren .\r\nW-0     1.844   seconds\r\nH-0     -0.3731671869754791     Mach@@ ine trans@@ lation is difficult to control .\r\nD-0     -0.3731671869754791     Machine translation is difficult to control .\r\nP-0     -0.5434 -0.1423 -0.1930 -0.1415 -0.2346 -1.1430 -0.1665 -0.8482 -0.1678 -0.1514\r\n2020-07-31 12:17:55 | INFO | fairseq_cli.interactive | Total time: 12.803 seconds; translation time: 3.688\r\n```\r\n\r\nNote the new tags present in the output:\r\n\r\n* `C-#` records active constraints (after applying preprocessing) for a sentence\r\n* `W-#` reports the sentence-level translation time (a useful unrelated feature I hope you'll accept)\r\n\r\nSome unit tests are written (`fairseq/test_constraints.py`) but not yet integrated. Advice here on where to place this is welcome. I also have not run this through lint; if someone can tell me the command to run, I'd appreciate it.\r\n\r\n### Implementation notes\r\n\r\nThis is largely self-contained, implemented in a new `LexicallyConstrainedBeamSearch` class in `search.py`. It does require a few minimal hooks from `_generate()` in `sequence_generator.py`, to ensure that constraints are updated at each timestep. (Edit: most changes in that file are documentation clarifications, corrections, and updates). Unconstrained sentences that are intermingled with constrained ones will not incur any time penalty, so long as they do not occur in the same batch.\r\n\r\nAddresses #1536.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "constraints",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee0ac254a19ac29c553",
    "number": 2394,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee1ac254a19ac29c554",
    "number": 2391,
    "body": "- uses downloaded paths in mbart commands, as defaults.\r\n- corrects path to `sentencepiece.bpe.model`",
    "head_branch": "mbart-doc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee2ac254a19ac29c555",
    "number": 2388,
    "body": "https://stackoverflow.com/questions/8738388/x-not-in-y-or-not-x-in-y\r\nhttps://www.python.org/dev/peps/pep-0008/#programming-recommendations\r\nE713: https://pycodestyle.pycqa.org/en/latest/intro.html#error-codes\r\n\r\n\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee3ac254a19ac29c556",
    "number": 2381,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes link\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-10",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee3ac254a19ac29c557",
    "number": 2369,
    "body": "small bug fix in dataset creation\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee4ac254a19ac29c558",
    "number": 2367,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nadd sort_order for `prev_output_tokens`\r\n\r\n",
    "head_branch": "patch-9",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee5ac254a19ac29c559",
    "number": 2365,
    "body": "Fixes https://github.com/pytorch/fairseq/issues/2351\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee6ac254a19ac29c55a",
    "number": 2360,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee6ac254a19ac29c55b",
    "number": 2356,
    "body": "I think .format() should be added to the return line as the latest sacrebleu.corpus_bleu() now returns an object not a string\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee7ac254a19ac29c55c",
    "number": 2354,
    "body": "â€¦st_checkpoint_metric`\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #(2205).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nLots of fun!\r\n",
    "head_branch": "fix-reduce_lr_on_plateau-mode",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee8ac254a19ac29c55d",
    "number": 2342,
    "body": "",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee9ac254a19ac29c55e",
    "number": 2336,
    "body": "Fixed to add BOS at the time of inference in translation_lev task.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2335 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "issue2335",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bee9ac254a19ac29c55f",
    "number": 2334,
    "body": "s/BART/bart/",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beeaac254a19ac29c560",
    "number": 2333,
    "body": "Summary:\nThis change adds a new option (`--stop-time-hr`) which if specified limits the total training time to that number of hours. In order to stop training within the inner training loop (after the first update exceeding the time limit) the starting time is stored on the trainer.\n\nIn addition, in order to persist the training time when when restoring from checkpoints (important because training runs are sometimes killed due to resource constraints), training time already completed is stored as extra state in the checkpoints (though this change is backward compatible with existing checkpoints).\n\nDifferential Revision: D22573166\n\n",
    "head_branch": "export-D22573166",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beebac254a19ac29c561",
    "number": 2331,
    "body": "Note: this is still a WIP\r\n\r\nWorks:\r\n```bash\r\npython train.py --tpu \\\r\n  --task dummy_lm --tokens-per-sample 1024 --max-sentences 2 \\\r\n  --arch transformer_lm_megatron --share-decoder-input-output-embed \\\r\n  --decoder-embed-dim 1536 --decoder-ffn-embed-dim 6400 \\\r\n  --decoder-layers 24 --decoder-attention-heads 32 \\\r\n  --optimizer adam --clip-norm 0.0 --lr 0.001 \\\r\n  --log-format simple --log-interval 1 \\\r\n  --model-parallel-size 8 --distributed-world-size 8\r\n```\r\n\r\nDoesn't work:\r\n```bash\r\npython train.py --tpu \\\r\n  --task dummy_lm --tokens-per-sample 1024 --max-sentences 2 \\\r\n  --arch transformer_lm_megatron --share-decoder-input-output-embed \\\r\n  --decoder-embed-dim 1536 --decoder-ffn-embed-dim 6400 \\\r\n  --decoder-layers 48 --decoder-attention-heads 32 \\\r\n  --optimizer adam --clip-norm 0.0 --lr 0.001 \\\r\n  --log-format simple --log-interval 1 \\\r\n  --model-parallel-size 8 --distributed-world-size 8\r\n```",
    "head_branch": "misc_model_parallel",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beecac254a19ac29c562",
    "number": 2320,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (doc change, so not necessary)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  n/a\r\n\r\n## What does this PR do?\r\nAddresses #2319 by making it clear that the --sentencepiece-vocab requires a path to a sentencepiece model.",
    "head_branch": "sentencepiece-vocab-flag",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beedac254a19ac29c563",
    "number": 2313,
    "body": "## ðŸ“š typo in wav2vec example README.md\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests? \r\n\r\n## What does this PR do?\r\nFixes(#2313)\r\n\r\nI find a Documentation typo in `fairseq/examples/wav2vec/README.md` \r\n\r\nthe **Prepare training data manifest** command script has an error.\r\n\r\nthe error command script in [line 35](https://github.com/pytorch/fairseq/blame/master/examples/wav2vec/README.md#L33) is\r\n```\r\n$ python scripts/wav2vec_manifest.py /path/to/waves --dest /manifest/path --ext wav\r\n```\r\n\r\nwhich is different from the command in [line 89](https://github.com/pytorch/fairseq/blame/master/examples/wav2vec/README.md#L89), and `fairseq/scripts/wav2vec_manifest.py` does not exist, so I think the right command should be\r\n\r\n```\r\n$ python examples/wav2vec/wav2vec_manifest.py /path/to/waves --dest /manifest/path --ext wav\r\n```\r\n\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beeeac254a19ac29c564",
    "number": 2308,
    "body": "Summary:\nImplemented Monte Carlo dropout. Added README to reproduce the results from our paper\nthat applies this idea for unsupervised quality estimation of NMT (joint work of Facebook AI and the University of Sheffield):\n\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, FrÃ©dÃ©ric Blain, Francisco GuzmÃ¡n, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, Lucia Specia. Unsupervised Quality Estimation for Neural Machine Translation. Accepted to TACL\n\nRetaining dropout at test time is not possible in the current code base. The statement\n```\nif not self.retain_dropout:\n  model.eval()\n```\nin `SequenceGenerator` does not have any effect, since model `training` attribute is already set to False by the method `make_generate_fast_`, which is applied before initializing `SequenceGenerator` in `generate.py`. `make_generate_fast_` throws an exception when trying to set `training` to True after its application. Also, if I am not mistaken `self.training=True` can have other effects, so setting it to True only for the purpose of retaining dropout at test time might be confusing. I propose an alternative implementation where `retain_dropout` is an attribute of FairseqModel class.\n\n# Before submitting\n\n- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\n- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\n- [Y] Did you make sure to update the docs?\n- [Y] Did you write any new necessary tests?\n\n## What does this PR do?\nNew feature.\n\n## PR review\nAnyone in the community is free to review the PR once the tests have passed.\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\n\n## Did you have fun?\nMake sure you had fun coding ï¿½\nPull Request resolved: https://github.com/pytorch/fairseq/pull/2151\n\nDifferential Revision: D22048889\n\nPulled By: myleott\n\n",
    "head_branch": "export-D22048889",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beeeac254a19ac29c565",
    "number": 2288,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?  (n/a)\r\n- [x] Did you write any new necessary tests?  (n/a)\r\n\r\n## What does this PR do?\r\nFixes #2287 where a file `open` without encoding would cause a `UnicodeEncodeError` when using `fairseq-generate` with Unicode characters on Windows. A similar change was made in #460 but this line seems to have been added after that PR.\r\n\r\n## Notes for similar future fixes\r\nThe repo has some other calls to `open` with non-binary files (i.e. mode is not `rb` or `wb`) that may cause similar issues. Perhaps there could be a codemod to inject encoding into all of these call sites, unless there are situations in this repo where text files should have a different encoding? Alternatively or in addition, perhaps a linter rule/plugin could be added to check for this case like this one: https://pypi.org/project/flake8-file-encoding/.\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beefac254a19ac29c566",
    "number": 2284,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFix zombie parameter\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-8",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef0ac254a19ac29c567",
    "number": 2283,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nupdate dependency from `pytorch_transformers` to `transformers`\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-7",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef1ac254a19ac29c568",
    "number": 2280,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nBefore this commit, fairseq **log** is as follows, which may confuse us with `layernorm_embedding`\r\n\r\n```\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(21128, 768, padding_idx=0)\r\n    (embed_positions): LearnedPositionalEmbedding(512, 768, padding_idx=0)\r\n    (layers): ModuleList(\r\n      (0): ...\r\n      (1): ...\r\n      (2): \r\n      (3): \r\n      (4): \r\n      (5): \r\n    )\r\n    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)  # ----- wrong order ----\r\n  )\r\n```\r\n\r\nAfter this commit\r\n\r\n```\r\n  (encoder): TransformerEncoder(\r\n    (embed_tokens): Embedding(21128, 768, padding_idx=0)\r\n    (embed_positions): LearnedPositionalEmbedding(512, 768, padding_idx=0)\r\n    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)  # ----- right order ----\r\n    (layers): ModuleList(\r\n      (0): ...\r\n      (1): ...\r\n      (2): \r\n      (3): \r\n      (4): \r\n      (5): \r\n    )\r\n  )\r\n```\r\n\r\n\r\n## Related src code\r\n\r\n `layernorm_embedding` is applied before self-attention layer\r\n\r\nhttps://github.com/pytorch/fairseq/blob/894ae64858b62927d849c0fbc05e8f55d680a4f1/fairseq/models/transformer.py#L364-L372\r\n\r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-6",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef1ac254a19ac29c569",
    "number": 2276,
    "body": "# Before submitting\r\n\r\nFixes https://github.com/pytorch/fairseq/issues/2275\r\n- [ ] Was this discussed/approved via a Github issue? \r\n(no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2275 \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef2ac254a19ac29c56a",
    "number": 2272,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n- [x] Did you write any new necessary tests?\r\n\r\n## What does this PR do?\r\nImplements transformer models with LSTM decoder. #2237\r\nOver the standard transformer, these models would perform sequence generation with constant memory and linear time complexity against the length of sequences generated.\r\n\r\n## Did you have fun?\r\nYes. I only wish I have more time to work on community initiatives and open source. Been too busy with work recently.\r\n\r\n## Overview of new features\r\n2 implementations of Transformer with LSTM decoder\r\n- `LightLSTMDecodeTransformerModel`\r\n- `LSTMDecodeTransformerModel`\r\n\r\n## `LightLSTMDecodeTransformerModel`\r\nThe decoder is a rewritten version of `LSTMDecoder`, local testing suggests that context from the previous time step improves results substantially.\r\nOf all transformer models with single-layer decoder, this model performs the best.\r\nAn interesting observation is that the performance of this model does not appear to variate with decoder depth.\r\n\r\n## `LSTMDecodeTransformerModel`\r\nThe decoder is architecturally similar to the `TransformerDecoder`. The only difference is that self-attention blocks are replaced by LSTM layers.\r\nIn local testing, this model basically achieves the same performance as the standard transformer while being a little bit faster.\r\n\r\n## Bugfixes\r\n- Fixed docs for command-line tools\r\n\r\n## Self-debates\r\n- I've thought about simply refactoring the `LSTMDecoder` but realized that the current implementation highly depends on the `LSTMEncoder`. Disconnecting this dependency changes some fundamental logic. To avoid the rabbit hole of backward compatibility, whole new models are written from scratch.\r\n- Why not just have a single LSTM decode model? Both of them are good in different scenarios though.",
    "head_branch": "feature/lstm-decode",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef3ac254a19ac29c56b",
    "number": 2266,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\ndisable shuffle for test dataset\r\n\r\n",
    "head_branch": "patch-5",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef4ac254a19ac29c56c",
    "number": 2261,
    "body": "Add an optional ```timeout``` argument to ```EpochBatchIterator```. \r\n\r\nI need it to fix this issue: https://github.com/pytorch/pytorch/issues/2474\r\n\r\nI could do something more general, allowing one to pass ```**dataloader_kwargs``` to ```torch.utils.data.DataLoader```, if you think it's worth.\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef5ac254a19ac29c56d",
    "number": 2253,
    "body": "fix bug for print_alignment\r\n\r\n# Before submitting\r\n\r\n- [ V] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ V] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   not relevant\r\n- [ ] Did you write any new necessary tests?  not relevant\r\n\r\n## What does this PR do?\r\nFixes #1880 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef5ac254a19ac29c56e",
    "number": 2245,
    "body": "",
    "head_branch": "tpu_translation",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef6ac254a19ac29c56f",
    "number": 2238,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ x ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThis PR reduces unnecessary communication overhead between GPUs since we only need to sync up once for all lang-pairs. We see significant training speedup especially with large number of lang-pairs.\r\n",
    "head_branch": "multilingual_distributed_training_speedup",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef7ac254a19ac29c570",
    "number": 2229,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef8ac254a19ac29c571",
    "number": 2228,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2227 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "preprocess_dict_number",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef9ac254a19ac29c572",
    "number": 2226,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "herring",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bef9ac254a19ac29c573",
    "number": 2225,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nImplements learned relative positional embeddings for transformer models as introduced in the paper [Self-Attention with Relative Position Representations](https://www.aclweb.org/anthology/N18-2074.pdf)\r\nThere was some discussion on this in the issue #556\r\nThere are currently some problems:\r\n- Model export tests are failing.\r\n- The training speed is noticeably slower than the regular transformer using absolute positional embeddings.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "relative_positional_embeddings",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befaac254a19ac29c574",
    "number": 2222,
    "body": "Summary:\nWhen share_input_output_embed is set to True, the existing code always overrides output_projection.weight with embed_tokens.weight\n\nThis is unncessary, and caused a very obscure bug in our custom BART model.\n\nAdded a check to skip the update to state_dict if f\"{name}.output_projection.weight\" is already in the checkpoint.\n\nReviewed By: myleott\n\nDifferential Revision: D21915833\n\n",
    "head_branch": "export-D21915833",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befbac254a19ac29c575",
    "number": 2214,
    "body": "\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nAdd support for manually defined prev_output_tokens in LanguagePairDataset. \r\n\r\nThis is important for recent models such as MASS(not start with PAD but the last token), ProphetNet, .etc.\r\n\r\n",
    "head_branch": "patch-4",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befcac254a19ac29c576",
    "number": 2193,
    "body": "",
    "head_branch": "misc3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befcac254a19ac29c577",
    "number": 2190,
    "body": "",
    "head_branch": "misc2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befdac254a19ac29c578",
    "number": 2184,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621befeac254a19ac29c579",
    "number": 2176,
    "body": "This PR fixes various small bugs, which are self-explanatory from each commit's message. Most of them are related to `multilingual denoising` and `semisupervised_translation` tasks. I also added a workaround for this issue https://github.com/pytorch/fairseq/issues/1274.",
    "head_branch": "various_fixes",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621beffac254a19ac29c57a",
    "number": 2174,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nPartially Fixes #2173 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_print_alignment",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf00ac254a19ac29c57b",
    "number": 2172,
    "body": "Solved  #1778\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nThis MR provides a T5 model into fairseq. I implemented only the finetuning task (without masking one). I tested it on the trivia-qa task and got comparable results.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nSure, I learned a lot about this model, especially finding bugs.\r\n",
    "head_branch": "applica-t5",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf00ac254a19ac29c57c",
    "number": 2170,
    "body": "masked_lm is actually encoder-only model\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nClass Refactor \r\n\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf01ac254a19ac29c57d",
    "number": 2163,
    "body": "",
    "head_branch": "misc",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf02ac254a19ac29c57e",
    "number": 2162,
    "body": "",
    "head_branch": "parallel_validation",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf03ac254a19ac29c57f",
    "number": 2151,
    "body": "Summary:\r\n\r\nImplemented Monte Carlo dropout. Added README to reproduce the results from our paper\r\nthat applies this idea for unsupervised quality estimation of NMT (joint work of Facebook AI and the University of Sheffield):\r\n\r\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya, FrÃ©dÃ©ric Blain, Francisco GuzmÃ¡n, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, Lucia Specia. Unsupervised Quality Estimation for Neural Machine Translation. Accepted to TACL\r\n\r\nRetaining dropout at test time is not possible in the current code base. The statement\r\n```\r\nif not self.retain_dropout:\r\n  model.eval()\r\n```\r\nin `SequenceGenerator` does not have any effect, since model `training` attribute is already set to False by the method `make_generate_fast_`, which is applied before initializing `SequenceGenerator` in `generate.py`. `make_generate_fast_` throws an exception when trying to set `training` to True after its application. Also, if I am not mistaken `self.training=True` can have other effects, so setting it to True only for the purpose of retaining dropout at test time might be confusing. I propose an alternative implementation where `retain_dropout` is an attribute of FairseqModel class.\r\n\r\n# Before submitting\r\n\r\n- [N] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [Y] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [Y] Did you make sure to update the docs?   \r\n- [Y] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nNew feature.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "inference_dropout",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf04ac254a19ac29c580",
    "number": 2150,
    "body": "Use of \"set tokenizer\" instead of \"get tokenizer\".\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf04ac254a19ac29c581",
    "number": 2144,
    "body": "Only project the masked tokens, not unmasked tokens.\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes roberta document\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf05ac254a19ac29c582",
    "number": 2139,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFixes generation for hf_gpt2. \r\n\r\nBefore fix:\r\n```\r\nenv CUDA_VISIBLE_DEVICES=\"2,3\" fairseq-interactive data-bin/wikitext-103 \\\r\n  --task language_modeling \\\r\n  --path $PREFIX/checkpoints_gpt2/transformer_wikitext-103/checkpoint_best.pt\r\n```\r\n![image](https://user-images.githubusercontent.com/22627794/82102310-05021380-96c4-11ea-8073-1ae6919559ba.png)\r\n\r\nAfter fix:\r\n![image](https://user-images.githubusercontent.com/22627794/82102316-092e3100-96c4-11ea-825a-c41254ce9efe.png)\r\n\r\nThis test follows the [language model example](https://github.com/pytorch/fairseq/tree/master/examples/language_model), but with hf_gpt2.\r\n\r\nTrained for one epoch:\r\n```\r\nenv CUDA_VISIBLE_DEVICES=\"2,3\" fairseq-train --task language_modeling \\\r\n  data-bin/wikitext-103 \\\r\n  --save-dir $PREFIX/checkpoints_gpt2/transformer_wikitext-103 \\\r\n  --dropout 0.1 \\\r\n  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\r\n  --lr 0.0005 --reset-optimizer --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\r\n  --tokens-per-sample 1024 --sample-break-mode none \\\r\n  --max-tokens 1024 --update-freq 16 \\\r\n  --fp16 \\\r\n  --arch hf_gpt2 --max-target-positions 1024 \\\r\n  --skip-invalid-size-inputs-valid-test\r\n```\r\n\r\nDetails of fix:\r\nadd the unexpected keyword argument, encoder_out, to forward()  \r\nincremental_state is {}, not None => change to handle this case\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hf_gpt2_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf06ac254a19ac29c583",
    "number": 2137,
    "body": "",
    "head_branch": "tpu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf07ac254a19ac29c584",
    "number": 2129,
    "body": "Fixes https://github.com/pytorch/fairseq/issues/1785, where in multilingual_denoising the generic eos token is used instead of the `langid` token. \r\n\r\nSpecifically, the problem is that the [collater](https://github.com/pytorch/fairseq/blob/master/fairseq/data/denoising_dataset.py#L355) in the denoising_dataset uses the generic `self.vocab.eos()` instead of `self.eos`, which in the case of `multilingual_denoising` is [set](https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/multilingual_denoising.py#L159) to the language id of a given language.\r\n\r\nSee https://github.com/pytorch/fairseq/issues/1785#issuecomment-626171602 for an example of the issue.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf07ac254a19ac29c585",
    "number": 2127,
    "body": "",
    "head_branch": "bugfixes4",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf08ac254a19ac29c586",
    "number": 2113,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf09ac254a19ac29c587",
    "number": 2112,
    "body": "â€¦erLayer #1893\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0aac254a19ac29c588",
    "number": 2100,
    "body": "",
    "head_branch": "bugfixes3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0bac254a19ac29c589",
    "number": 2099,
    "body": "This is a minor safety thing, since otherwise it may be tempting to call `Dictionary(filename)` instead of `Dictionary.load(filename)`. After this change, all arguments to `Dictionary.__init__` must be given explicitly.",
    "head_branch": "dict_kwonly",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0cac254a19ac29c58a",
    "number": 2098,
    "body": "This is needed for TPUs",
    "head_branch": "cpu_checkpoint",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0cac254a19ac29c58b",
    "number": 2096,
    "body": "Summary:\nNo change to existing behavior.\n\nAllows the use of an extra learned linear projection (bottleneck layer) before the output projection. This structure was already supported in `TransformerDecoder` via args.decoder_output_dim, used in architectures such as `transformer_lm`, but this change surfaces a command-line option for the basic transformer architecture.\n\nDifferential Revision: D21443249\n\n",
    "head_branch": "export-D21443249",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0dac254a19ac29c58c",
    "number": 2090,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2022.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fair-2022",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0eac254a19ac29c58d",
    "number": 2076,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2075 \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "hotfix/2075",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf0fac254a19ac29c58e",
    "number": 2074,
    "body": "Summary:\nTorch's DataLoader keeps a buffer of only 2 ready batches only, which cannot be changed. This causes a data loading bottleneck at times where data preparation time fluctuates.\n\nAdding BufferedIterator, which is a generic wrapper for an iterator, implementing a buffer using queue.\n\nAdding FairseqTask support, and in BatchSamplerIterator as default.\n\nReviewed By: myleott\n\nDifferential Revision: D21261026\n\n",
    "head_branch": "export-D21261026",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf10ac254a19ac29c58f",
    "number": 2073,
    "body": "Summary:\nWe identified issues in TorchScript builtin hasattr occasionally has inconsistency so reorder_incremental_state is not always triggered in exported model. We had these overrides once before named_module / id were supported in TorchScript and Chen removed in D20896200. Plan to have these until hasattr issue gets fixed.\n\nWith this we observed ~1 bleu score regression is gone, see test plan.\n\nDifferential Revision: D21273014\n\n",
    "head_branch": "export-D21273014",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf10ac254a19ac29c590",
    "number": 2060,
    "body": "Summary:\nNow I separate out the forward pass from SequenceGenerator. 2 changes to make exported TS model work under multithreading settings.\n1. Avoid stateful attribute including incremental states, src_lengths in EnsembleModel / SequenceGenerator. This is the main reason I hesitated to merge with the original version. myleott, is there specific reason why we have incremental states as module attributes, instead of initializing it in forward pass?\n2. Replace in place operator with out of place equivalent. This caused a bit of code bloat, mostly for index_put operators\n\ne.g.\n    eos_mask[:, :beam_size][blacklist] = torch.tensor(0).to(eos_mask)\n\nbecomes\n\n    slices = torch.arange(eos_mask.numel()).view(eos_mask.shape)[:, :beam_size][\n                blacklist\n            ]\n    eos_mask = eos_mask.index_put([slices], torch.tensor(0).to(eos_mask))\n\nDifferential Revision: D20995796\n\n",
    "head_branch": "export-D20995796",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf11ac254a19ac29c591",
    "number": 2059,
    "body": "Summary:\ntest_ensemble_sequence_generator is green on fbcode master but Pytorch 1.5 release cut happened before the TorchScript fix, so updating the gate to 1.6\nRemove quantization test from fairseq as FBGEMMS is binded at OSS side. Will add the test back in fbtranslate but land this first to fix OSS side failures.\n\nDifferential Revision: D21231873\n\n",
    "head_branch": "export-D21231873",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf12ac254a19ac29c592",
    "number": 2052,
    "body": "Summary: Convert the transformer model output projection to use an `nn.Linear` module in order to support quantization during TorchScript inference.\n\nDifferential Revision: D21222066\n\n",
    "head_branch": "export-D21222066",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf13ac254a19ac29c593",
    "number": 2050,
    "body": "## What does this PR do?\r\nFixes #2042 \r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf14ac254a19ac29c594",
    "number": 2040,
    "body": "Usage:\r\n\r\n- Text Translation\r\n\r\n``` shell\r\npython $fairseq/example/simultaneous_translation/eval/evaluate_local.py \\\r\n    --scorer-type text \\\r\n    --agent-type simul_trans_text \\\r\n    --tokenizer 13a \\\r\n    --src-file $src \\\r\n    --tgt-file $tgt \\\r\n    --data-bin $data_bin \\\r\n    --model-path $model \\\r\n    --src-splitter-type $src_splitter_type \\\r\n    --src-splitter-path $src_splitter_path \\\r\n    --tgt-splitter-type $tgt_splitter_type \\\r\n    --tgt-splitter-path $tgt_splitter_path \\\r\n    --num-threads 10 \\\r\n    --output\r\n```\r\n- Speech Translation\r\n\r\n``` shell\r\npython $fairseq/example/simultaneous_translation/eval/evaluate_local.py \\\r\n    --scorer-type speech \\\r\n    --agent-type simul_trans_speech \\\r\n    --tokenizer 13a \\\r\n    --tgt-file $json_file \\\r\n    --data-bin $data_bin \\\r\n    --model-path $model \\\r\n    --src-splitter-type $src_splitter_type \\\r\n    --src-splitter-path $src_splitter_path \\\r\n    --tgt-splitter-type $tgt_splitter_type \\\r\n    --tgt-splitter-path $tgt_splitter_path \\\r\n    --num-threads 10 \\\r\n    --output\r\n```",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf14ac254a19ac29c595",
    "number": 2038,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "citation-edit",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf15ac254a19ac29c596",
    "number": 2035,
    "body": "Checks for set_epoch on nested datasets in NestedDictionaryDataset, same/similar issue as https://github.com/pytorch/fairseq/issues/989",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf16ac254a19ac29c597",
    "number": 2032,
    "body": "This reverts commit 6379573c9e56620b6b4ddeb114b030a0568ce7fe.\r\n\r\nIt doesn't tie weights and breaks old checkpoints.",
    "head_branch": "revert",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf17ac254a19ac29c598",
    "number": 2030,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1862.\r\nNobody responded to my issue. Nevertheless change is very small, therefore i think is doesn't need much discussion.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "applica-lstm-api-extension",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf18ac254a19ac29c599",
    "number": 2028,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #2027 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fair-2027",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf18ac254a19ac29c59a",
    "number": 2021,
    "body": "",
    "head_branch": "fix_lstm_lm_tests",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf19ac254a19ac29c59b",
    "number": 2016,
    "body": "Summary: It is to update Fairseq LSTM to jitable version\n\nDifferential Revision: D20937370\n\n",
    "head_branch": "export-D20937370",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf1aac254a19ac29c59c",
    "number": 2014,
    "body": "",
    "head_branch": "fix_1983",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf1bac254a19ac29c59d",
    "number": 2011,
    "body": "Create checkpoint directory in ```save_checkpoint()```instead of ```load_checkpoint()```.\r\n\r\nhttps://github.com/pytorch/fairseq/issues/1986\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf1cac254a19ac29c59e",
    "number": 2001,
    "body": "",
    "head_branch": "quant_noise_myle",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #2001 from pytorch/quant_noise_myle\n\nWIP: [quant-noise] rebase + additional cleanup"
    ],
    "has_test_file": true,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": true
  },
  {
    "_id": "6621bf1cac254a19ac29c59f",
    "number": 1999,
    "body": "Pass CLIENT_COMMAD to dockerfile instead of many env vars\r\n",
    "head_branch": "simulastsharedtask_docker_doc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1999 from xutaima/simulastsharedtask_docker_doc\n\nUpdate Dockerfile and doc"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf1dac254a19ac29c5a0",
    "number": 1998,
    "body": "Add a new way to start speech  server, which might be easier for participants who didn't run the baseline experiments or is working on other framework.\r\n\r\nThe new one needs two files\r\n- TGT_FILE: the file with reference sentences\r\n- WAV_LIST_FILE: the file with a list of paths to WAVs, line aligned to TGT_FILE \r\n\r\nIn this case we can start the server\r\n```shell\r\npython $fairseq/example/simultaneous_translation/eval/server.py \\\r\n    --tokenizer 13a \\\r\n    --src-file SRC_FILE \\\r\n    --tgt-file WAV_LIST_FILE \\\r\n    --tgt-file-type text \\\r\n    --scorer-type speech \\\r\n    --output $result_dir \\\r\n    --port 12321\r\n```",
    "head_branch": "simulastsharedtask_wav_list",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1998 from xutaima/simulastsharedtask_wav_list\n\nAdd a new way to start speech server"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf1eac254a19ac29c5a1",
    "number": 1997,
    "body": "Update RESTful API for participants who would like to write their own client\r\n\r\nI modified the RESTful API to make it more generic. Nothing needs to be change for the old code.",
    "head_branch": "simulastsharedtask_rest_api",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1997 from xutaima/simulastsharedtask_rest_api\n\nUpdate RESTful API"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": true
  },
  {
    "_id": "6621bf1fac254a19ac29c5a2",
    "number": 1996,
    "body": "This pull request fixed the bug for server when the output dir does not exist. When the --output is not none, the server will try to create the directory, skip writing predictions if failed. \r\n\r\nAlso change the writing pattern to $output.text to $output/text.\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1996 from xutaima/simulastsharedtask\n\nfix the bug when the output dir does not exist"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf20ac254a19ac29c5a3",
    "number": 1995,
    "body": "Update dockerfile for the modified evaluation scripts",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1995 from kahne/simulastsharedtask\n\nUpdate dockerfile"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf21ac254a19ac29c5a4",
    "number": 1994,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1994 from xutaima/simulastsharedtask\n\nChange user_dir to the real path"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf21ac254a19ac29c5a5",
    "number": 1993,
    "body": "Summary:\nF.linear -> nn.Linear so FBGEMM backend could quantize the linear projection. We observed 3x+ speedup.\n\nBackward compatibility code is added to upgrade_state_dict_named. Locally it worked.\n\nTesting loading OSS checkpoints.\n\nDifferential Revision: D20967830\n\n",
    "head_branch": "export-D20967830",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf22ac254a19ac29c5a6",
    "number": 1992,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask-baseline-doc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1992 from xutaima/simulastsharedtask-baseline-doc\n\n Update server commands in baseline.md"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf23ac254a19ac29c5a7",
    "number": 1991,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1991 from xutaima/simulastsharedtask\n\nUpdate the instructions of customized agent"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf24ac254a19ac29c5a8",
    "number": 1985,
    "body": "",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1985 from jmp84/simulastsharedtask\n\nAdd links to speech checkpoints."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf25ac254a19ac29c5a9",
    "number": 1984,
    "body": "Summary: The fix in MHA is suggested by driazati, to avoid JIT compilation for if branch in MHA forward when in scripting. Without this quantization wouldn't work. Details in https://fb.workplace.com/groups/2240361332735959/permalink/626166461295703/\n\nReviewed By: jhcross\n\nDifferential Revision: D20881076\n\n",
    "head_branch": "export-D20881076",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf26ac254a19ac29c5aa",
    "number": 1981,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1981 from xutaima/simulastsharedtask\n\nfix the bug when the utterance is too short"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf26ac254a19ac29c5ab",
    "number": 1980,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf27ac254a19ac29c5ac",
    "number": 1979,
    "body": "If the forward pass fails in all the gpus then sample size is 0. Which leads to error in `self.optimizer.multiply_grads`. We can raise an error to catch this. There might be a more elegant solution, this implementation is a hack\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf28ac254a19ac29c5ad",
    "number": 1975,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1120\n\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1940\n\nDeprecate the SequenceGenerator in Fairseq with the Scripted vision.\n\nPass all integration unit tests\n\n- Copy ScriptSequenceGenerator to SequenceGenerator:\n  - Modified the forward_decoder to fix bug when using adaptive_softmax in `get_prob_normalize` (marked with the inline comment)\n   - Add support for other EnsembleModels as input arg (marked with the inline comment)\n - Add `FBEnsembleModelWithFork` to support folk/join in ensemblemodel\n   - Add `test_fb_ensemble_model` to test folk/join feature\n   - Still have bugs in folk/join feature when running in the Fairseq interface (like generation and interactive). Need further investigation P128130029. cc cndn, jhcross\n- Modified SequenceGenerator initialization the interface\n- Clear up the codes: delete unused functions `get_normalized_probs` and `_decode`\n\nReland reverted diff D20685075\n\nReviewed By: cndn\n\nDifferential Revision: D20895977\n\n",
    "head_branch": "export-D20895977",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf29ac254a19ac29c5ae",
    "number": 1972,
    "body": "Summary: Unifying on the Black auto-formatter.\n\nDifferential Revision: D20887312\n\n",
    "head_branch": "export-D20887312",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf29ac254a19ac29c5af",
    "number": 1970,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1970 from xutaima/simulastsharedtask\n\nUpdate text model test curve"
    ],
    "has_test_file": true,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2aac254a19ac29c5b0",
    "number": 1969,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes minor typo in docstring.\r\n\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2bac254a19ac29c5b1",
    "number": 1968,
    "body": "",
    "head_branch": "paraphraser",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2cac254a19ac29c5b2",
    "number": 1963,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1128\n\nIt's a common issue that short inputs (< 5 tokens) get repeated due to default length constraint (max_len_a=1.1, max_len_b=5) https://fb.workplace.com/groups/2286753504877951/permalink/2674177509468880/.\n\nIn the future we want to use no_ngram_repeat to handle the issue. The functionality is in sequence generator but it needs to be scripted for production use.\n\nDifferential Revision: D20801865\n\n",
    "head_branch": "export-D20801865",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2dac254a19ac29c5b3",
    "number": 1958,
    "body": "Make transformer model beam search aware to reduce memory usage and get better speed. \r\nGiven beam size N, it reduce memory usage for encoder statue to 1/N, and enable using Nx batch size to get sqrt(N) speed up. \r\n\r\n# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1957.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "transformer_beamsearch_aware",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2eac254a19ac29c5b4",
    "number": 1953,
    "body": "Summary:\nScript the `reorder_incremental_states` in the base FairseqModel\nRemove the overwrite scriptable `reorder_incremental_states` in the TransformerModel\n\nDifferential Revision: D20797390\n\n",
    "head_branch": "export-D20797390",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2fac254a19ac29c5b5",
    "number": 1951,
    "body": "# Description\r\n\r\nIn [examples/translation](https://github.com/pytorch/fairseq/tree/master/examples/translation), the code will not run if you change the model from `transformer.wmt16` to `transformer.wmt19`, since the BPE they are using are different. I corrected that with a note at the end of the section.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf2fac254a19ac29c5b6",
    "number": 1942,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1942 from xutaima/simulastsharedtask\n\nfix a typo in speech curve legend"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf30ac254a19ac29c5b7",
    "number": 1941,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nAdd speech curve and some modifications\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1941 from xutaima/simulastsharedtask\n\nAdd speech curve and some modifications"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf31ac254a19ac29c5b8",
    "number": 1940,
    "body": "Summary:\nDeprecate the SequenceGenerator in Fairseq with the Scripted vision.\n\nPass all integration unit tests\n\n- Copy ScriptSequenceGenerator to SequenceGenerator:\n  - Modified the forward_decoder to fix bug when using adaptive_softmax in `get_prob_normalize` (marked with the inline comment)\n   - Add support for other EnsembleModels as input arg (marked with the inline comment)\n - Add `FBEnsembleModelWithFork` to support folk/join in ensemblemodel\n   - Add `test_fb_ensemble_model` to test folk/join feature\n   - Still have bugs in folk/join feature when running in the Fairseq interface (like generation and interactive). Need further investigation. cc cndn, jhcross\n- Modified SequenceGenerator initialization the interface\n- Clear up the codes: delete unused functions `get_normalized_probs` and `_decode`\n\nDifferential Revision: D20685075\n\n",
    "head_branch": "export-D20685075",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf32ac254a19ac29c5b9",
    "number": 1934,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes validation happening twice at the end of epoch after refactor. Spotted by @freewym \r\n here: https://github.com/pytorch/fairseq/commit/b5dad3b7e02d66dc98d4707bc8aeacf95618ccd7#r38103577\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix-refactor-training-save-and-evaluate",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf33ac254a19ac29c5ba",
    "number": 1927,
    "body": "Summary:\n- Switches the model to the scripted sequence generator recently implemented in fairseq. Involved making the input/ouput format of this model to conform to that in Fairseq TransformerEncoder/Decoder\n- Modify the `EncoderOut` format for fairseq transformer and added optional fields needed for copy ptr decoder\n- Switches to using WordEmbedding directly instead of the non scriptable EmbeddingList for src/trg embedding layer\n- Small assorted syntactic changes to make it jit scriptable\n- Adds a torchscriptify method for this model. Will do latency analysis shortly\n- Currently the Roberta decoupled model is not scriptable because the base TransformerSentenceEncoder it is based on is not scriptable. We can look at adding that later\n\nDifferential Revision: D20687247\n\n",
    "head_branch": "export-D20687247",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf34ac254a19ac29c5bb",
    "number": 1922,
    "body": "Summary: We are planning to deprecate the original SequenceGenerator and use the ScriptSequenceGenerator in the Fairseq. Due to the change of scripted Sequence Generator constructor, I change `build_generator` interface in Fairseq, pyspeech and pytorch translate.\n\nDifferential Revision: D20683836\n\n",
    "head_branch": "export-D20683836",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf34ac254a19ac29c5bc",
    "number": 1920,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf35ac254a19ac29c5bd",
    "number": 1914,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf36ac254a19ac29c5be",
    "number": 1911,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1109\n\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1289\n\nFirst remove the assertion since when I load the model for fine-tuning I don't necessarily have all the language directories on my data.\nI tried several ideas here for general bart archs, however none of them worked well for mbart on multilingual data, but haven't experimented with bart on en data:\n1. initializing ontology embeddings as average pooling of all subword embeddings.\n2. initializing pointer attention mha with some layer from bart decoder\n\nDifferential Revision: D20530432\n\n",
    "head_branch": "export-D20530432",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf37ac254a19ac29c5bf",
    "number": 1902,
    "body": "added links to pretrained models and how to evaluate them, added a couple clarifying points to readme based on questions received",
    "head_branch": "layerdrop_model_add",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf38ac254a19ac29c5c0",
    "number": 1897,
    "body": "Rendering can be checked here: https://github.com/jmp84/fairseq/blob/simulastsharedtask/examples/simultaneous_translation/docs/baseline.md",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1897 from jmp84/simulastsharedtask\n\nAdd quality-latency curve for the text-to-text baseline"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf38ac254a19ac29c5c1",
    "number": 1896,
    "body": "FUNCTIONALITY:\r\nThis diff provides two core pieces of functionality\r\n- Adds training with quantization noise from \"Training with Quantization Noise for Extreme Model Compression\" - controlled by the \"quant_noise\" and \"quant_noise_block_size\" parameters. Added in embeddings, attention, FFN for BERT and Transformer LM training\r\n- Adds quantization with product quantization based on code from \"And the bit goes down: Revisiting the quantization of neural networks\" (Stock et al, 2019). This is applied to a fairseq trained model to quantize after training. \r\n\r\nTODO:\r\n-> @Pierre, look at quantization code\r\n-> int4 and int8 quantization will be added soon. \r\n\r\nEVALUATED TEST CASES:\r\n\r\n0. Training of LM and BERT models starts from scratch with no errors -> yes \r\n\r\n1. Retrain LM from scratch with code, no quantization, reproduces Wikitext-103 LM results -> yes, see /checkpoint/angelafan/qn_open_source_noise\r\n\r\n2. Reload previously trained LM from scratch, not trained with quant noise, reproduces Wikitext-103 LM results -> yes \r\n\r\n3. Train LM from scratch with code, no trained with quant noise, reproduces Wikitext-103 LM results -> yes, see /checkpoint/angelafan/qn_open_source_baseline\r\n\r\n4. Train BERT model from scratch with code, no quantization, training curve looks the same as before -> yes\r\n\r\n5. Check wps during training and wps during inference, no large change from before -> yes\r\n\r\n6. Check structured dropout isn't being applied at eval time -> yes \r\n\r\n7. Works in combination with LayerDrop -> yes\r\n",
    "head_branch": "quant_noise",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf39ac254a19ac29c5c2",
    "number": 1894,
    "body": "Summary: Having a uniform return type for `FairseqEncoder` makes these test models function more similarly to real models.\n\nDifferential Revision: D20596971\n\n",
    "head_branch": "export-D20596971",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3aac254a19ac29c5c3",
    "number": 1893,
    "body": "",
    "head_branch": "rm_bias_kv_zero_attn",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3bac254a19ac29c5c4",
    "number": 1892,
    "body": "* remove pdb\r\n* typos\r\n* add more requirements\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1892 from jmp84/simulastsharedtask\n\nSimulastsharedtask"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3cac254a19ac29c5c5",
    "number": 1891,
    "body": "",
    "head_branch": "cosine_max_lr",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3cac254a19ac29c5c6",
    "number": 1890,
    "body": "We want to wait until `sample_size` is a float before applying dummy batch logic.",
    "head_branch": "dummy_batch",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3dac254a19ac29c5c7",
    "number": 1889,
    "body": "This comes up during OOM recovery.",
    "head_branch": "clip_grad",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3eac254a19ac29c5c8",
    "number": 1888,
    "body": "",
    "head_branch": "fix1881",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf3fac254a19ac29c5c9",
    "number": 1875,
    "body": "This should fix #1826.",
    "head_branch": "cpu_all_gather_list",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf40ac254a19ac29c5ca",
    "number": 1874,
    "body": "Fixes #1866",
    "head_branch": "fix_wsc",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf40ac254a19ac29c5cb",
    "number": 1873,
    "body": "Fixes #1860\r\n\r\nThis also includes a fix required for LayerNorm due to a bug in apex: https://github.com/NVIDIA/apex/issues/770",
    "head_branch": "non_gpu_0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf41ac254a19ac29c5cc",
    "number": 1869,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nI've seen trainings being frozen in `requests.head` for ever. Discussed with @ngoyal2707 \r\n\r\nAbout timeouts in the doc of the `requests` module: \"Nearly all production code should use this parameter in nearly all requests. Failure to do so can cause your program to hang indefinitely\"\r\nhttps://requests.readthedocs.io/en/master/user/quickstart/#timeouts\r\n\r\n",
    "head_branch": "timeouts",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf42ac254a19ac29c5cd",
    "number": 1868,
    "body": "# Before submitting\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs? \r\n- [ ] Did you write any new necessary tests?  Most of the code was just moved around.\r\n\r\n## What does this PR do?\r\nImplements #1828 . \r\n\r\n## PR review    \r\nThere are some points in the code where I was not sure how to properly handle them (e.g. where to place the general options for all eval-scorers). I left some FIXMEs in there, would be nice if someone could go over it. \r\n\r\nFurthermore I currently don't have enough time and gpus to test this thoroughly. Maybe someone could run the IWSLT'14 example twice, with and without this pull request and verify that both report the same validation BLEU. \r\n\r\n## Did you have fun?\r\nYes, sure! ðŸ™ƒ\r\n",
    "head_branch": "eval-scorers-#1828",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf43ac254a19ac29c5ce",
    "number": 1864,
    "body": "â€¦trogram\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1863.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fair-1863",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf44ac254a19ac29c5cf",
    "number": 1861,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nShows correct epoch for data loading in logs.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "minor-fix-epoch-data-loading",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf44ac254a19ac29c5d0",
    "number": 1858,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1858 from kahne/simulastsharedtask\n\nAdd Docker for IWSLT 2020 SimulAST shared task evaluation"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf45ac254a19ac29c5d1",
    "number": 1857,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1857 from xutaima/simulastsharedtask\n\nfix the bug that using the absolute data path in checkpoint"
    ],
    "has_test_file": true,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf46ac254a19ac29c5d2",
    "number": 1852,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1851 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "beamsearch_perf_improve",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf47ac254a19ac29c5d3",
    "number": 1848,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n- Refactor the code for shared task\r\n- Add options for online and offline feature extraction\r\n- Add options to disable feature normalization along the time axis\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1848 from xutaima/simulastsharedtask\n\nUpdate speech translation task and dataset"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": true
  },
  {
    "_id": "6621bf48ac254a19ac29c5d4",
    "number": 1847,
    "body": "Very minor update, just changing the url. The old one is 404.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf48ac254a19ac29c5d5",
    "number": 1846,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1846 from jmp84/simulastsharedtask\n\nSimulastsharedtask"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf49ac254a19ac29c5d6",
    "number": 1845,
    "body": "When `--patience=1`, we should stop as soon as the validation loss decreases.",
    "head_branch": "myleott-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4aac254a19ac29c5d7",
    "number": 1844,
    "body": "Fixes #1843",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4bac254a19ac29c5d8",
    "number": 1838,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1837  (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "time_stretch",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4cac254a19ac29c5d9",
    "number": 1836,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1835 .\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "specaugment",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4cac254a19ac29c5da",
    "number": 1834,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1833\r\n\r\nExample output after this update:\r\n```\r\n2020-03-12 15:39:26 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-03-12 15:39:26 | INFO | train | epoch 001 | loss 69041.9 | nll_loss 6820 | wps 12712 | ups 6.53 | wpb 1640 | bsz 162 | num_updates 2 | lr 1e-05 | gnorm 2603.12 | loss_scale 16 | train_wall 1 | ppl inf | wall 1\r\n/SFS/user/wp/prihodad/git/fairseq/fairseq/utils.py:351: RuntimeWarning: overflow encountered in power\r\n  return safe_round(np.power(base, loss), round)\r\n2020-03-12 15:39:26 | WARNING | root | NaN or Inf found in input tensor.\r\n2020-03-12 15:39:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 71135.5 | nll_loss 7034 | wps 0 | wpb 1699 | bsz 168 | ppl inf | num_updates 2\r\n\r\n```\r\n",
    "head_branch": "feature/ppl_overflow",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4dac254a19ac29c5db",
    "number": 1831,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1830 \r\nAdds tests for RoBERTa (masked_lm, classification, single regression, multiple regression)",
    "head_branch": "feature/multiple_regression",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4eac254a19ac29c5dc",
    "number": 1824,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bart_addxsum_model_torch_hub",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf4fac254a19ac29c5dd",
    "number": 1823,
    "body": "Summary:\nCreate a separate workflow for fairseq native evaluation with a trained checkpoint. This is developed from fairseq/evaluate with simplification, refactoring and manifold migration. If fairseq folks are interested we could unify with their implementation.\n\npreprocess_and_eval() takes in raw test data, lowercase, preprocess and get lc bleu score using fairseq generate.\n\nSince test data is still on gluster and yoda related operators haven't been migrated to Manifold, we are still having sporadic gluster usage across the file.\n\nIntegration to base training, right after ensemble_and_sweep will be in next diff.\n\nFairseq side changes only include ones to support manifold path. cc myleott\n\nDifferential Revision: D20194073\n\n",
    "head_branch": "export-D20194073",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf50ac254a19ac29c5de",
    "number": 1816,
    "body": "",
    "head_branch": "lstm_padding",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf50ac254a19ac29c5df",
    "number": 1814,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1814 from jmp84/simulastsharedtask\n\nAdd pre-trained models to download."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf51ac254a19ac29c5e0",
    "number": 1813,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\nFix the bug of missing criterion when loading model\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1813 from xutaima/simulastsharedtask\n\nFix the bug of missing criterion when loading model"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf52ac254a19ac29c5e1",
    "number": 1810,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1810 from jmp84/simulastsharedtask\n\nFix spurious `+`"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf53ac254a19ac29c5e2",
    "number": 1809,
    "body": "Summary:\nWhen --validate-interval is set to greater than 1, the validation loss is set to None for those epochs where no validation was done.\n\nThis caused a crash in Early Stopping if the \"--patience\" option was set to a positive value. The reason is that `valid_loss` is compared with `prev_best`, resulting a crash when None is compared to a float:\n```\nTypeError: '>' not supported between instances of 'NoneType' and 'float'\n```\n\nA simple check is added to the should_stop_early() method which skips checking if no validation is done in the current epoch.\n\nReviewed By: myleott\n\nDifferential Revision: D20318486\n\n",
    "head_branch": "export-D20318486",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf53ac254a19ac29c5e3",
    "number": 1805,
    "body": "Add the link to RoBERTa-based language models (PhoBERT) pre-trained for Vietnamese!\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf54ac254a19ac29c5e4",
    "number": 1803,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1802\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix/1802",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf55ac254a19ac29c5e5",
    "number": 1798,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes epoch check for curriculum learning. `epoch_itr.epoch` has not been incremented before calling `next_epoch_itr()`. So the check for curriculum learning should look like the proposed fix.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf56ac254a19ac29c5e6",
    "number": 1792,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1791.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fair-1791",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf57ac254a19ac29c5e7",
    "number": 1789,
    "body": "Hi,\r\n\r\nthis PR updates the link to mBART documentation in main readme.",
    "head_branch": "readme-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf57ac254a19ac29c5e8",
    "number": 1788,
    "body": "Add the link to RoBERTa-based language models (PhoBERT) pre-trained for Vietnamese\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf58ac254a19ac29c5e9",
    "number": 1784,
    "body": "# Before submitting\r\n\r\n- [X] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [X] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [X] Did you make sure to update the docs?   \r\n- [X] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1777.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf59ac254a19ac29c5ea",
    "number": 1783,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1782.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fair-1782",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5aac254a19ac29c5eb",
    "number": 1781,
    "body": "closes #1790\r\n\r\nadd WandBProgressBarWrapper\r\nadd --wandb-project argument\r\nupdate progress_bar to support wandb_project\r\nlog args to wandb\r\nupload all --user-dir *.py files to wandb\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? issue #1790\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?\r\n\r\n**NOTE:** I'm not sure what tests I can add. I wanted to look into tensorboard support tests, but there is none.\r\n\r\n## What does this PR do?\r\n\r\nAdds Weights and Biases support for logging ([example project](https://app.wandb.ai/guitaricet/unet_transformer?workspace=user-guitaricet))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n**NOTE:** windows tests are failing in all PRs, this is master branch problem not related to this PR\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "wandb_support",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5bac254a19ac29c5ec",
    "number": 1774,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/1067\n\nOverwrite the reorder_incremental_state function in TransformerModel to temporally unblock the scriptable sequence generator. Since the `self` is not supported in the jit script, it is fundimentally hard to refact the base  `reorder_incremental_state`.\n\nDifferential Revision: D20150320\n\n",
    "head_branch": "export-D20150320",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5bac254a19ac29c5ed",
    "number": 1773,
    "body": "",
    "head_branch": "fix_div_beam",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5cac254a19ac29c5ee",
    "number": 1767,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes a link in a README.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5dac254a19ac29c5ef",
    "number": 1759,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf5eac254a19ac29c5f0",
    "number": 1757,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n- Refactor the evaluation code\r\n   - Update server client protocol\r\n   - Update speech and text scorers\r\n   - Update speech and text agents\r\n- Implemented faster evaluation code\r\n   - Speech ~13min and text ~2min\r\n   - Enable agent for multi-threads\r\n   - Enable multi-client evaluation\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n@jmp84 \r\n\r\n## Did you have fun?\r\nYep\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1757 from xutaima/simulastsharedtask\n\nSimulastsharedtask"
    ],
    "has_test_file": true,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": true
  },
  {
    "_id": "6621bf5fac254a19ac29c5f1",
    "number": 1752,
    "body": "",
    "head_branch": "tpu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf60ac254a19ac29c5f2",
    "number": 1745,
    "body": "",
    "head_branch": "tensorboard_files",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf60ac254a19ac29c5f3",
    "number": 1744,
    "body": "",
    "head_branch": "fix_dummy",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf61ac254a19ac29c5f4",
    "number": 1743,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 6: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-tasks",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf62ac254a19ac29c5f5",
    "number": 1735,
    "body": "",
    "head_branch": "missing_dummy",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf63ac254a19ac29c5f6",
    "number": 1733,
    "body": "## Branch name is wrong - this fixes tokenizers, not BPE.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 5: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-bpe",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf63ac254a19ac29c5f7",
    "number": 1732,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 4: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-tokenizers",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf64ac254a19ac29c5f8",
    "number": 1731,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 3: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-optim",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf65ac254a19ac29c5f9",
    "number": 1730,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 2: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-lr-scheduler",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf66ac254a19ac29c5fa",
    "number": 1729,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 in part (part 1: [context](https://github.com/pytorch/fairseq/pull/1714#issuecomment-587507040))\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces-criterion",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf67ac254a19ac29c5fb",
    "number": 1722,
    "body": "Summary: Add manifold support in fairseq. This should be transparent to users.\n\nDifferential Revision: D19979639\n\n",
    "head_branch": "export-D19979639",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf67ac254a19ac29c5fc",
    "number": 1721,
    "body": "Summary: Manifold path (of format manifold://...) contains \":\" which was used as delimiter in fairseq. Add support for splitting manifold paths with \"|\" as delimiter, as we currently do in pytorch_translate.\n\nDifferential Revision: D19963376\n\n",
    "head_branch": "export-D19963376",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf68ac254a19ac29c5fd",
    "number": 1714,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672 \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf69ac254a19ac29c5fe",
    "number": 1710,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/1711",
    "head_branch": "feature/sentence_prediction_freeze",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6aac254a19ac29c5ff",
    "number": 1709,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/1708\r\n",
    "head_branch": "feature/classify_per_token",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6bac254a19ac29c600",
    "number": 1707,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nAdd code for published paper from FB\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n\r\n*Still WIP*\r\n@jmp84 ",
    "head_branch": "monotonic_multihead_attention",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6cac254a19ac29c601",
    "number": 1706,
    "body": "Change MAX_TOKENS=2048 --> 1024, as per  yinhanliu in https://github.com/pytorch/fairseq/issues/1685\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6cac254a19ac29c602",
    "number": 1704,
    "body": "Summary: Fairseq JIT scriptable Simple Sequence Generator. This scriptable Sequence Generator is fully compatiable with fairseq transformerModel meeting LATTE team demand. In the following Diff, it will support ensemble model for Sequence Generator. The test have verified this implementation is able to load previous sequence generator test, and given out same results.\n\nDifferential Revision: D19745260\n\n",
    "head_branch": "export-D19745260",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6dac254a19ac29c603",
    "number": 1697,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n  - #1060 \r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n  - I think it is not needed to update the docs\r\n- [ ] Did you write any new necessary tests? \r\n  - Cannot write test code for installing package, but I got test on my forked github repository and local system\r\n    - ex) pip install git+https://github.com/AppleHolic/fairseq@fetch_build_toml\r\n\r\n## What does this PR do?\r\nFixes #1060 \r\n\r\nHello, I got an error to install fairseq when I install another repository that has an depandancy on fairseq. There is no problem when the cython is already installed, but I wanna solve this problem with [PEP](https://www.python.org/dev/peps/pep-0518/). \r\n\r\nThis PR gives build-ability cython extension on setup time without pre-installation of cython. And I tested on two cases (install from github, install from local directory). \r\n\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ!\r\n",
    "head_branch": "fetch_build_toml",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6eac254a19ac29c604",
    "number": 1693,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "simulastsharedtask",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #1693 from xutaima/simulastsharedtask\n\nSimulastsharedtask"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": true,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6fac254a19ac29c605",
    "number": 1691,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # 1679\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_multilingual_mgpu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf6fac254a19ac29c606",
    "number": 1689,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1688\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-registry",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf70ac254a19ac29c607",
    "number": 1677,
    "body": "# Before submitting\r\n\r\nI'll be doing this incrementally since it's such a massive change...\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1672\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/refactor-namespaces",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf71ac254a19ac29c608",
    "number": 1673,
    "body": "Summary:\nProvide an option to binarize numberized text files, where the lines consist of space-separated decimal string representations of the underlying dictionary indices.\n\nFaceboook:\n\nProduction training currently works by producing files of this type using the VocabProcessor mechanism, and this approach seemed the least disruptive (though another possibility would be to subclass `Binarizer` in an `fb_` file).\n\nDifferential Revision: D19704131\n\n",
    "head_branch": "export-D19704131",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf72ac254a19ac29c609",
    "number": 1671,
    "body": "Summary: Formatting changes for uniformity and to make subsequent changes more clear.\n\nDifferential Revision: D19701981\n\n",
    "head_branch": "export-D19701981",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf73ac254a19ac29c60a",
    "number": 1662,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "semi_supervised",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf73ac254a19ac29c60b",
    "number": 1658,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes `fairseq-interactive` failure when `--buffer-size` is greater than 1.\r\n\r\n## PR review\r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix_logging_format",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf74ac254a19ac29c60c",
    "number": 1653,
    "body": "Summary: Earlier we had some issues at pickling. Type information gets lost. Fixed in https://github.com/pytorch/pytorch/pull/32569.\n\nDifferential Revision: D19435988\n\n",
    "head_branch": "export-D19435988",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf75ac254a19ac29c60d",
    "number": 1650,
    "body": "The first step in the CNN/DM fine-tuning instructions for BART is misleading (see #1391). This PR fixes the README and adds links to #1391 as well as to a repository with CNN/DM processing code adjusted for BART.\r\n",
    "head_branch": "update-bart-cnn-readme",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf76ac254a19ac29c60e",
    "number": 1647,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1643 to make tests pass on Windows... hopefully. ðŸ˜„ \r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix/refactor-cli-for-tests",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf77ac254a19ac29c60f",
    "number": 1642,
    "body": "Very minor fix to avoid overwriting validation data.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/1641.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "prepare-iwslt17",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf78ac254a19ac29c610",
    "number": 1639,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf78ac254a19ac29c611",
    "number": 1634,
    "body": "* fix: mid-epoch validation metrics were previously polluting training metrics\r\n* fix: mid-epoch metrics were not properly saved/restored in checkpoints\r\n* added tests, both for metrics and for mid-epoch reproducibility",
    "head_branch": "fix_logging",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf79ac254a19ac29c612",
    "number": 1633,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7aac254a19ac29c613",
    "number": 1631,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1622\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "fix/windows-dbz",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7bac254a19ac29c614",
    "number": 1629,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nI think if we keep pass **padding index of vocabulary** as `padding_idx` to adaptive embedding layers, \r\nthere will be no chance to train some words.\r\n\r\ne.g. If `cut_off` is (20000,60000) and vocab is larger than 60000, \r\nwe can't learn[**20,000+padding_idx**]th word and [**60,000+padding_idx**]th word.\r\nBecause those words' ids will be **padding_idx** by subtraction logic and eventually get zero tensors.\r\n\r\nSo, I changed `self.padding_idx` to `None` after assign vocab's `padding_idx` \r\n**for the first time at head embedding representation**.",
    "head_branch": "adapinp-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7cac254a19ac29c615",
    "number": 1627,
    "body": "Summary:\nPython logging offers a number of benefits, such as logging timestamps, better\ncross-library compatibility, ability to add multiple output handlers, etc.\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/646\n\nReviewed By: spencerp\n\nDifferential Revision: D15815620\n\nPulled By: myleott\n\n",
    "head_branch": "export-D15815620",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7cac254a19ac29c616",
    "number": 1621,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nFixes CLI issues with Windows builds.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "feature/fix-cli-windows",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7dac254a19ac29c617",
    "number": 1620,
    "body": "Summary:\nMake Fairseq transformer scriptable. Discussion points on code quality:\n\n(1) Original decoder output is a tuple (x, {\"attn\": attn, \"inner_states\": inner_states}). TorchScript does not support dictionary with values of different types (attn: Tensor, inner_states: List[Tensor]). Current workaround is to use [attn] for attention field and access via output[\"attn\"][0] in downstream. This is currently used in fairspeq custom transformer code. Another (maybe) cleaner alternative is to use namedtuple for decoder output but involves tons of downstream changes too.\n\n(2) Currently TorchScript doesn't support **kwargs. Some unused arguments might get passed in due to polymorphism. Now the only workaround I can think of is to add possible unused arguments, (e.g. line 666 in transformer.py)\n\nDifferential Revision: D19234599\n\n",
    "head_branch": "export-D19234599",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7eac254a19ac29c618",
    "number": 1615,
    "body": "- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you write any new necessary tests?\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?\r\n\r\n## What does this PR do?\r\nDon't merge this PR yet, this is a breaking change.\r\n\r\nInstead of using `<unk>` token as placeholders in Levenstein transformer,\r\nthis PR changes that to the `<mask>` token.\r\n\r\nThe breaking portion is that lev transformer models would not be backward compatible with\r\n`fairseq-0.9` due to the missing `<mask>` token embedding, a dirty solution would be to use existing `<unk>` token embeddings as a drop-in.\r\n\r\nThe current changes are the minimum required code edits.\r\nHowever this PR is not complete, at least 2 other changes are suggested:\r\n- Refactor `levenshtein_utils.py`\r\n- Add `unk_penalty` to `iterative_refinement_generator`\r\n",
    "head_branch": "feature/lev_mask_tokens",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf7fac254a19ac29c619",
    "number": 1612,
    "body": "Summary: Make SinusoidalPositionalEmbedding scriptable. Mostly adding types. The only change that affects lots of downstream code is to have max_positions as member variable instead of method.\n\nDifferential Revision: D18924939\n\n",
    "head_branch": "export-D18924939",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf80ac254a19ac29c61a",
    "number": 1611,
    "body": "In 1e324a5bbe4b1f68f9dadf3592dab58a54a800a8 we introduced `fairseq.metrics`, which allows metrics to be logged and aggregated based on their context. For example:\r\n```python\r\nwith metrics.aggregate(\"train\"):\r\n    for step, batch in enumerate(epoch):\r\n        with metrics.aggregate(\"train_inner\") as agg:\r\n            loss = get_loss(batch)\r\n            metrics.log_scalar(\"loss\", loss)\r\n            if step % log_interval == 0:\r\n                # print the average loss over this log_interval\r\n                print(agg.get_smoothed_value(\"loss\"))\r\n                agg.reset()\r\n# print the average loss over the whole epoch\r\nprint(metrics.get_smoothed_value(\"train\", \"loss\"))\r\n```\r\n\r\nThis interface allows one to log metrics from anywhere, without having to pass the values up the call stack, resulting in a much simpler `train.py`.\r\n\r\nTo make the transition smoother, we introduce `FairseqTask.reduce_metrics` and `FairseqCriterion.reduce_metrics` to replace the old `aggregate_logging_outputs` method, although we provide backward compatibility for old Tasks/Criterions.",
    "head_branch": "export-D19292402",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf80ac254a19ac29c61b",
    "number": 1606,
    "body": "",
    "head_branch": "doc_improvements",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf81ac254a19ac29c61c",
    "number": 1604,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nThis refactors the CLI of `fairseq` for use as a module script to be invoked with `python -m fairseq ...` in addition to the fairseq CLI executables which are installed when built from source.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n",
    "head_branch": "feature/refactor-cli",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf82ac254a19ac29c61d",
    "number": 1602,
    "body": "**Fixes issue when finetuning with `SentencePredictionTask`**\r\n\r\nLabels (instance of `FairseqDataset` in `SentencePredictionTask().datasets[split][\"target\"]`) would be encoded from `source_dictionary` instead of `_label_dictionary`.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf83ac254a19ac29c61e",
    "number": 1596,
    "body": "change ``cut -f3-`` to ``cut -f2-``",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf83ac254a19ac29c61f",
    "number": 1595,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nRuns CI for `fairseq` on all major platforms provided by GitHub actions.\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.",
    "head_branch": "feature/build-other-platforms",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf84ac254a19ac29c620",
    "number": 1594,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\n\r\nThis PR replaces symlinks to files sitting in the project root with the files they're linked to in order to make the process of building from source cross platform. Currently `fairseq_cli` cannot be built from source on `master` because the symlinks use [unix paths](https://github.com/pytorch/fairseq/blob/master/fairseq_cli/eval_lm.py#L1).\r\n\r\nThis PR also inadvertantly fixes a bug of a missing symlink to [`fairseq-validate`](https://github.com/pytorch/fairseq/blob/master/setup.py#L167).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.",
    "head_branch": "feature/fix-cli-windows",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf85ac254a19ac29c621",
    "number": 1584,
    "body": "â€¦s DLL exports.\r\n\r\n# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #292\r\n\r\nâ˜ï¸ technically this issue should remain open\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nðŸ‘ \r\n",
    "head_branch": "fix/windows-libbleu",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf86ac254a19ac29c622",
    "number": 1583,
    "body": "# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [x] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes #1586\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nðŸ‘ \r\n",
    "head_branch": "fix/add-results-path",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf87ac254a19ac29c623",
    "number": 1577,
    "body": "â€¦oderModel\r\n\r\n",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf87ac254a19ac29c624",
    "number": 1563,
    "body": "Recent releases of apex removed the `fused_adam_cuda` function used in https://github.com/pytorch/fairseq/blob/3f4fc5016334255d6908b20202267ca0b0287335/fairseq/optim/adam.py#L220. Users need to use the `--deprecated_fused_adam` option to isntall `fused_adam_cuda`\r\n\r\n# Before submitting\r\n\r\n- [ ] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [ ] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [ ] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes # (issue).\r\n\r\n## PR review    \r\nAnyone in the community is free to review the PR once the tests have passed.     \r\nIf we didn't discuss your PR in Github issues there's a high chance it will not be merged.\r\n\r\n## Did you have fun?\r\nMake sure you had fun coding ðŸ™ƒ\r\n",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf88ac254a19ac29c625",
    "number": 1546,
    "body": "",
    "head_branch": "fix_kwarg_moe",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf89ac254a19ac29c626",
    "number": 1545,
    "body": "",
    "head_branch": "hub_mask",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8aac254a19ac29c627",
    "number": 1544,
    "body": "",
    "head_branch": "grad_check",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8aac254a19ac29c628",
    "number": 1543,
    "body": "",
    "head_branch": "trunc_seq",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8bac254a19ac29c629",
    "number": 1542,
    "body": "",
    "head_branch": "ls_moe",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8cac254a19ac29c62a",
    "number": 1539,
    "body": "# Before submitting\r\n\r\n- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [] Did you write any new necessary tests?  \r\n\r\n## What does this PR do?\r\nFixes https://github.com/pytorch/fairseq/issues/1508.\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8dac254a19ac29c62b",
    "number": 1535,
    "body": "",
    "head_branch": "fix_build",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8eac254a19ac29c62c",
    "number": 1532,
    "body": "- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\nFixes #1484.",
    "head_branch": "tr_encoder_attn",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8eac254a19ac29c62d",
    "number": 1530,
    "body": "- [x] Was this discussed/approved via a Github issue? (no need for typos, doc improvements)\r\n- [x] Did you read the [contributor guideline](https://github.com/pytorch/fairseq/blob/master/CONTRIBUTING.md)?\r\n- [x] Did you make sure to update the docs?   \r\n- [ ] Did you write any new necessary tests?  \r\n\r\nFixes #1485.",
    "head_branch": "self_attn_decoder",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf8fac254a19ac29c62e",
    "number": 1528,
    "body": "",
    "head_branch": "fix_mha",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf90ac254a19ac29c62f",
    "number": 1524,
    "body": "Summary:\nMake fairseq MultiheadAttention scriptable. Looking for feedbacks.\n\n1. Add types\n2. Move incremental state management logic from util functions to initializers. TorchScript in general doesn't support global dict. As a result modules with multihead attention in it would assign itself fairseq_instance_id in the initializer.\n3. There might be opportunities to make assertions and annotations cleaner.\n\nDifferential Revision: D18772594\n\n",
    "head_branch": "export-D18772594",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf91ac254a19ac29c630",
    "number": 1521,
    "body": "",
    "head_branch": "myleott-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf92ac254a19ac29c631",
    "number": 1515,
    "body": "",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf92ac254a19ac29c632",
    "number": 1500,
    "body": "@myleott, @ngoyal2707 anything else you'd like included?",
    "head_branch": "issues-template",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf93ac254a19ac29c633",
    "number": 1499,
    "body": "Summary:\nIn sound event detection, the evaluation metrics are mean average precision (MAP) and mean area under the curve (MAUC).\nThese metrics evaluate how well the system *sorts* instances.\nThey are not decomposable across instances, so one needs to collect the predictions and truths of all instances of a validation set.\n\nThis diff adds a switch `log_pred` to the `forward` function of the binary cross entropy.\nWhen this switch is turned on, it saves the predictions and truths to the `logging_output` dictionary.\n\nThis dictionary needs to be synced across workers by the `all_gather_list` function in `distributed_utils.py`.\nThe existing implementation restricts the size of the dictionary to 64KB, because it only allocates 2 bytes for the size during serialization. The predictions and truths often exceed this limit.\nThis diff uses 4 bytes for the size during serialization, and increases the limit.\n\nReviewed By: myleott\n\nDifferential Revision: D18922980\n\n",
    "head_branch": "export-D18922980",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf94ac254a19ac29c634",
    "number": 1496,
    "body": "At least under windows it wasn't working for me.\r\n\r\nPossible fix for the issue described here https://github.com/pytorch/fairseq/issues/1370. Not sure if the change brakes other things though",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf95ac254a19ac29c635",
    "number": 1473,
    "body": "",
    "head_branch": "fix-spmdecode",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf96ac254a19ac29c636",
    "number": 1470,
    "body": "add args in interactive.py of multilingual_translation task.\r\n- sentencepiece_vocab : path to sentencepiece.bpe.model",
    "head_branch": "BPE_bug",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf96ac254a19ac29c637",
    "number": 1469,
    "body": "add args in interactive.py of multilingual_translation task.\r\n- sentencepiece_vocab : path to sentencepiece.bpe.model",
    "head_branch": "BPE_bug",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf97ac254a19ac29c638",
    "number": 1457,
    "body": "The scale window condition is `elif (self._iter - self._last_overflow_iter) % self.scale_window == 0`, which requires self.scale_window is an integer. If self.scale_window is float, the result of `(self._iter - self._last_overflow_iter) % self.scale_window` will never be 0.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf98ac254a19ac29c639",
    "number": 1454,
    "body": "https://github.com/pytorch/fairseq/issues/1308\r\ntgt in AsrDataset is list of torch tensors and it cause SIGSEGV error because tgt has too many objects to create shared memory in multiprocessing of dataloaders.",
    "head_branch": "asr",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf99ac254a19ac29c63a",
    "number": 1453,
    "body": "https://github.com/pytorch/fairseq/issues/1308\r\n\r\n\"tgt\" in AsrDataset is list of torch tensor and it cause SIGSEGV while using multi-workers DataLoader on multi GPU.",
    "head_branch": "asr",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9aac254a19ac29c63b",
    "number": 1452,
    "body": "Possibly breaking changes:\r\n- Set global numpy seed (4a7cd58)\r\n- Split `in_proj_weight` into separate k, v, q projections in MultiheadAttention (fdf4c3e)\r\n- TransformerEncoder returns namedtuples instead of dict (27568a7)\r\n\r\nNew features:\r\n- Add `--fast-stat-sync` option (e1ba32a)\r\n- Add `--empty-cache-freq` option (315c463)\r\n- Support criterions with parameters (ba5f829)\r\n\r\nNew papers:\r\n- Simple and Effective Noisy Channel Modeling for Neural Machine Translation (49177c9)\r\n- Levenshtein Transformer (86857a5, ...)\r\n- Cross+Self-Attention for Transformer Models (4ac2c5f)\r\n- Jointly Learning to Align and Translate with Transformer Models (1c66792)\r\n- Reducing Transformer Depth on Demand with Structured Dropout (dabbef4)\r\n- Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa) (e23e5ea)\r\n- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (a92bcda)\r\n- CamemBERT: a French BERT (b31849a)\r\n\r\nSpeed improvements:\r\n- Add CUDA kernels for LightConv and DynamicConv (f840564)\r\n- Cythonization of various dataloading components (4fc3953, ...)\r\n- Don't project mask tokens for MLM training (718677e)",
    "head_branch": "v0.9.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9aac254a19ac29c63c",
    "number": 1444,
    "body": "`fairseq-interactive` currently ignores the `--skip_invalid_size_inputs_valid_test` parameter. I think this is the intended use of the parameter.",
    "head_branch": "interactive-skip-invalid",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9bac254a19ac29c63d",
    "number": 1441,
    "body": "Cloning with SSH URL raises permission error.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9cac254a19ac29c63e",
    "number": 1438,
    "body": "By default `return_all_hiddens` is False, the shape of `features` will be BxTxC.\r\nIf use `return_all_hiddens`, the shape of `features` will be TxBxC.\r\nSee \r\nhttps://github.com/pytorch/fairseq/blob/9398a2829596393b73f5c5f1b99edf4c2d8f9316/fairseq/modules/transformer_sentence_encoder.py#L227",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9dac254a19ac29c63f",
    "number": 1436,
    "body": "Small typo in the inference script, in the README for finetuning BART on CNN/DM dataset.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9eac254a19ac29c640",
    "number": 1408,
    "body": "I faced the error while using warmup for fixed lr schedule\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/private/home/antares/.conda/envs/fairseq-20190809/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\r\n    fn(i, *args)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py\", line 291, in distributed_main\r\n    main(args, init_distributed=True)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py\", line 81, in main\r\n    train(args, trainer, task, epoch_itr)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/train.py\", line 122, in train\r\n    log_output = trainer.train_step(samples)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/trainer.py\", line 409, in train_step\r\n    self.optimizer.step()\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/fp16_optimizer.py\", line 153, in step\r\n    self.fp32_optimizer.step(closure)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/fairseq_optimizer.py\", line 98, in step\r\n    self.optimizer.step(closure)\r\n  File \"/private/home/antares/work/unsupervised/blank_test/fairseq-py/fairseq/optim/nag.py\", line 68, in step\r\n    lr_correct = lr / lr_old\r\nZeroDivisionError: float division by zero\r\n```\r\nwhich is due to `num_updates=0` for the first iteration and thus `lr` we set to the optimizer is zero.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9eac254a19ac29c641",
    "number": 1406,
    "body": "Here's a quick fix for https://github.com/pytorch/fairseq/issues/1403.\r\n\r\nTo keep it short, this fix allows the user to checkpoint a translation model properly after applying layer pruning on a restored transformer file.",
    "head_branch": "bugfix/structured_dropout_layer_misalignment",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bf9fac254a19ac29c642",
    "number": 1397,
    "body": "Add bias False condition",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa0ac254a19ac29c643",
    "number": 1396,
    "body": "Add bias False condition",
    "head_branch": "kekmodel",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa1ac254a19ac29c644",
    "number": 1386,
    "body": "Fixes #1376",
    "head_branch": "fix_hub",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa1ac254a19ac29c645",
    "number": 1385,
    "body": "",
    "head_branch": "huihuifan-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa2ac254a19ac29c646",
    "number": 1381,
    "body": "Allow option to only include batches of max_positions length\r\n\r\n* only implemented for training",
    "head_branch": "mp/batches",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa3ac254a19ac29c647",
    "number": 1379,
    "body": "1. #1315 fix\r\n2. Added new `MultilingualLSTMModel`, taking `MultilingualTransformerModel` as example",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa4ac254a19ac29c648",
    "number": 1375,
    "body": "â€¦r to correctly recover the training from a \"non-shuffle\" checkpoint",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa5ac254a19ac29c649",
    "number": 1373,
    "body": "",
    "head_branch": "ret_self_attn",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa6ac254a19ac29c64a",
    "number": 1362,
    "body": "Summary:\nSplit the Fariseq MemoryEfficientFP16Optimizer class into 2 classes so that it can be easily imported in pytext through a wrapper class.\n\nIter 2 - fixed some issues to ensure code runs correctly on fblearner.\n\nReviewed By: chenyangyu1988\n\nDifferential Revision: D18410916\n\n",
    "head_branch": "export-D18410916",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa6ac254a19ac29c64b",
    "number": 1340,
    "body": "",
    "head_branch": "fix_docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa7ac254a19ac29c64c",
    "number": 1333,
    "body": "Summary: This in_proj_weight and in_proj_bias properties are not the right way of providing backward compatibility, and it's causing other incompatibilities with the new Dynamic Quantization API. So, let's remove this, and properly fix the failing tests.\n\nDifferential Revision: D18264129\n\n",
    "head_branch": "export-D18264129",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa8ac254a19ac29c64d",
    "number": 1328,
    "body": "Following the change in PyTorch1.3. The current implementation can been improved to fix a small discrepancy between the theory and the implementation. More details can be found at: [here](https://github.com/pytorch/pytorch/commit/fed5ca192c98619947cca2fb9491ac9624b787d2#diff-1ad7ee25f9d26b47f9a3a1e38555caf3)",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa9ac254a19ac29c64e",
    "number": 1325,
    "body": "Summary: torch.cat should apply on tensor instead of Parameters. Need to keep the variable just to make sure the old checkpoint still loads.\n\nDifferential Revision: D18217995\n\n",
    "head_branch": "export-D18217995",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfa9ac254a19ac29c64f",
    "number": 1310,
    "body": "\"pytorch.fairseq\" -> \"pytorch/fairseq\" to avoid following error:\r\n```\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfaaac254a19ac29c650",
    "number": 1304,
    "body": "Summary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1065\n\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/889\n\nWe are converting matmuls to quantizable nn.Linear modules in this diff. First let's test profile after the diff to see how low level operations are changing.\n\nReviewed By: jmp84, edunov, lly-zero-one, jhcross\n\nDifferential Revision: D17964796\n\n",
    "head_branch": "export-D17964796",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfabac254a19ac29c651",
    "number": 1299,
    "body": "Summary: LevT calls into tracing compliant transformer we didn't plan to OSS earlier. This is a workaround to unbreak the master. Will revisit and simplify the code later.\n\nDifferential Revision: D18110339\n\n",
    "head_branch": "export-D18110339",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfacac254a19ac29c652",
    "number": 1292,
    "body": "Models seem to train fine with this modification. I checked that the mask for beginning of words is correct but didn't check if the actual masking worked correctly.",
    "head_branch": "whole-word-masking-sentencepiece",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfadac254a19ac29c653",
    "number": 1288,
    "body": "",
    "head_branch": "boolq",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfadac254a19ac29c654",
    "number": 1281,
    "body": "Fix for #1240 \r\nTested with MaskedLMTask.\r\n\r\n",
    "head_branch": "fix-load-dataset",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfaeac254a19ac29c655",
    "number": 1279,
    "body": "Very small change.\r\nThe previous message was misleading, the length of TokenBlocksDataset is a number of \"blocks\" or \"streams\" but not the number of batches strictly speaking if I am not mistaken. I use the notion of batch from roberta https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md.\r\nIt took me some time to understand what was going on, I hope it saves some time for others.",
    "head_branch": "fix-mask-lm-load-msg",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfafac254a19ac29c656",
    "number": 1275,
    "body": "If the training stopped in the middle of the last epoch, and then it was resumed from checkpoint, it will not continue the training because `epoch_itr.epoch < max_epoch` is not satisfied. This PR fixed the issue.",
    "head_branch": "bug-fix2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb0ac254a19ac29c657",
    "number": 1272,
    "body": "â€¦all set_epoch() for each sub dataset",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb1ac254a19ac29c658",
    "number": 1270,
    "body": "removed redundant quotes in the filename assigned for dev dataset for GLUE tasks",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb1ac254a19ac29c659",
    "number": 1268,
    "body": "As their names suggest, the parameters `embedding_dim`, `ffn_embedding_dim`, and `num_attention_heads` should have type `int`, not `float`.\r\n\r\nAlso validated by https://github.com/pytorch/fairseq/blob/b5f41f828b0ec9b67fa60aceb0778073d1b368b2/fairseq/modules/sparse_transformer_sentence_encoder.py#L22#L24.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb2ac254a19ac29c65a",
    "number": 1250,
    "body": "Summary:\nAdding config parameter \"use_torchscript\" that enables use of TS for BERT\ntraining\n\nDifferential Revision: D17872083\n\n",
    "head_branch": "export-D17872083",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb3ac254a19ac29c65b",
    "number": 1233,
    "body": "Adds CTC loss and corresponding transformer ctc based models.\r\n\r\nTested with\r\n`CUDA_VISIBLE_DEVICES=0 python train.py $DATA_PATH --save-dir $SAVE_DIR --max-epoch 30 --task speech_recognition --arch vggtransformer_enc_1 --optimizer adadelta --lr 1.0 --adadelta-eps 1e-8 --adadelta-rho 0.95 --clip-norm 10.0  --max-tokens 10000 --log-format json --log-interval 1 --criterion ctc_loss --user-dir examples/speech_recognition/ --validate-interval=10`",
    "head_branch": "asr_ctc_loss",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb4ac254a19ac29c65c",
    "number": 1230,
    "body": "Fixes #1229.",
    "head_branch": "additional-argument-dash",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb4ac254a19ac29c65d",
    "number": 1219,
    "body": "Solves #1218.",
    "head_branch": "fix-subword-nmt-locations",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb5ac254a19ac29c65e",
    "number": 1212,
    "body": "Originally, the 'ppl' is calculated but returned as a string, which will not be printed to the tensorboard.",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb6ac254a19ac29c65f",
    "number": 1211,
    "body": "Summary:\nAdded a new native op that does wordpiece tokenization while additionally returning token start and end indices in the raw text as required by BertSquadQA. Includes Unit Tests for the native op and also to check its parity with the PyText Wordpiece Tokenizer.\n\nAlso combined is a torchscript implementation of the Bert SQUAD QA Model.\n\nThere are scripts for evaluation and testing of the torchscript code as well.\n\nReviewed By: borguz\n\nDifferential Revision: D17455985\n\n",
    "head_branch": "export-D17455985",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb7ac254a19ac29c660",
    "number": 1201,
    "body": "Summary:\nThis PR implements a new attention module which combines cross-attention (encoder-decoder attention) and the decoder self-attention. This work was accepted as an abstract at WeCNLP 2019 (https://www.wecnlp.ai/wecnlp-2019).\n\nCross+Self-Attention reduces the amount of parameter and increases the inference speed without any degradation in translation quality.\nMore details can be found in the attached [abstract](https://github.com/pytorch/fairseq/files/3561282/paper.pdf)\nPull Request resolved: https://github.com/pytorch/fairseq/pull/1097\n\nDifferential Revision: D17170333\n\nPulled By: myleott\n\n",
    "head_branch": "export-D17170333",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb8ac254a19ac29c661",
    "number": 1200,
    "body": "",
    "head_branch": "fix_typo",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb8ac254a19ac29c662",
    "number": 1197,
    "body": "",
    "head_branch": "readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfb9ac254a19ac29c663",
    "number": 1195,
    "body": "For batched predictions in Roberta, the README was giving an example that was pretty unclear. After a thorough discussion with @ngoyal2707 in issue #1167 he gave a clear example of how batched predictions were supposed to be done. Since I spent a lot of time on this inconsistency, I thought that it might benefit the community if his solution was in the official README ðŸ˜„ ! \r\n\r\n\r\nFor for details, see issue #1167",
    "head_branch": "chetan",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbaac254a19ac29c664",
    "number": 1188,
    "body": "Hi,\r\n\r\nI think there is a minor mistake in the doc. `--distributed-no-spawn` argument is needed for distributed training on multiple machines without `slurm`. Otherwise, the program will start 8 jobs on each GPU, when `nproc_per_node=8`.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbbac254a19ac29c665",
    "number": 1185,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbcac254a19ac29c666",
    "number": 1180,
    "body": "Summary: extract FP16OptimizerMixin for share the same logic in PyText\n\nDifferential Revision: D17594102\n\n",
    "head_branch": "export-D17594102",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbcac254a19ac29c667",
    "number": 1179,
    "body": "Summary: support Fairseq FP16Optimizer\n\nDifferential Revision: D17535294\n\n",
    "head_branch": "export-D17535294",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbdac254a19ac29c668",
    "number": 1174,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbeac254a19ac29c669",
    "number": 1169,
    "body": "Adding CNN model of summarization ",
    "head_branch": "dev",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfbfac254a19ac29c66a",
    "number": 1165,
    "body": "This is to make this instructions a little more generalizable, since in some systems, bash will parse the spaces within quotes\r\n\r\nAddressing https://github.com/pytorch/fairseq/issues/1146",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc0ac254a19ac29c66b",
    "number": 1155,
    "body": "",
    "head_branch": "myleott-patch-2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc1ac254a19ac29c66c",
    "number": 1147,
    "body": "",
    "head_branch": "cython_lang_level",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc1ac254a19ac29c66d",
    "number": 1140,
    "body": "",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc2ac254a19ac29c66e",
    "number": 1130,
    "body": "",
    "head_branch": "roberta_updated_cqa_readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc3ac254a19ac29c66f",
    "number": 1125,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc4ac254a19ac29c670",
    "number": 1122,
    "body": "missing .unsqueeze(-1) in line 124, \r\nwithout this change we'll encounter runtime error for >2d convolutional kernels, with this fix, we're applying adafactor's 2d logic to the two final dimensions.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc5ac254a19ac29c671",
    "number": 1099,
    "body": "When I try to reproduce the experiment in  _Hierarchical Neural Story Generation_, I found the command about generation cannot be executed.\r\n\r\nIt said that **fairseq-generate: error: unrecognized arguments: --sampling-temperature 0.8**\r\nIn the document, I find:\r\n```\r\n--temperature   temperature for generation\r\nDefault: 1.0\r\n```\r\nAnd I don't find a parameter named `--sampling-temperature`, so I think the parameter `--sampling-temperature` should be changed to `--temperature`",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc5ac254a19ac29c672",
    "number": 1097,
    "body": "This PR implements a new attention module which combines cross-attention (encoder-decoder attention) and the decoder self-attention. This work was accepted as an abstract at WeCNLP 2019 (https://www.wecnlp.ai/wecnlp-2019).\r\n\r\nCross+Self-Attention reduces the amount of parameter and increases the inference speed without any degradation in translation quality.\r\nMore details can be found in the attached [abstract](https://github.com/pytorch/fairseq/files/3561282/paper.pdf)",
    "head_branch": "CrossSelfAttn",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc6ac254a19ac29c673",
    "number": 1095,
    "body": "This PR implements guided alignment training described in  \"Jointly Learning to Align and Translate with Transformer Models (https://arxiv.org/abs/1909.02074)\". \r\n\r\nIn summary, it allows for training selected heads of the Transformer Model with external alignments computed by Statistical Alignment Toolkits. During inference, attention probabilities from the trained heads can be used to extract reliable alignments. In our work, we did not see any regressions in the translation performance because of guided alignment training.",
    "head_branch": "alignment-pr",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc7ac254a19ac29c674",
    "number": 1089,
    "body": "",
    "head_branch": "fix_multigpu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc8ac254a19ac29c675",
    "number": 1083,
    "body": "I'm trying to replicate some of the results from Classical Structured Prediction Losses for Sequence to Sequence Learning. I've moved some of the code from the original branch into a branch of the latest fairseq repo. So far, I've only done the risk minimization criterion but would really like feedback if anyone has time! I've tried running it with experimental settings from the paper, but haven't been able to replicate results (command taken from the readme of the classic_seqlevel branch). BLEU score on the validation set of IWSLTâ€™14 German-English seems to hang at around 32 after a few epochs (same as when I start fine tuning with risk minimization). Is there something I'm missing in my code?\r\n\r\nOn another note, would there be any interest from the community in me updating the other criterion from the classic_seqlevel branch to work with the current releases of pytorch/fairseq?",
    "head_branch": "risk_dev",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc8ac254a19ac29c676",
    "number": 1078,
    "body": "",
    "head_branch": "fix_hub",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfc9ac254a19ac29c677",
    "number": 1076,
    "body": "",
    "head_branch": "fix_cython",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfcaac254a19ac29c678",
    "number": 1071,
    "body": "The cuda code for `lightconv` and dynamicconv` include some files that do not exist.",
    "head_branch": "fix_lightconv",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfcbac254a19ac29c679",
    "number": 1063,
    "body": "With this white space, the command might fail.\r\n```\r\nfairseq-preprocess: error: unrecognized arguments:\r\nzsh: command not found: --destdir\r\n```",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfccac254a19ac29c67a",
    "number": 1061,
    "body": "For system without python-numpy, the original setup.py will raise an error as follows:\r\n```\r\nfairseq/data/token_block_utils_fast.c:612:31: fatal error: numpy/arrayobject.h: No such file or directory\r\ncompilation terminated.                                                                                                   \r\nerror: command 'gcc' failed with exit status 1 \r\n```\r\nThat may be caused by lacking the symbolic link of `numpy` in `/usr/include`.\r\n\r\nBy adding following lines, it will firstly install numpy and then add numpy.get_include() into the include_dirs.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfccac254a19ac29c67b",
    "number": 1054,
    "body": "@myleott Hi Myle, I would like to continue contributing to this branch. I haven't done much open-source contributions before, so I would really appreciate any help with this process. Thank you!",
    "head_branch": "wip_transfer_learn",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfcdac254a19ac29c67c",
    "number": 1050,
    "body": "change string fromat in fairseq/data/subsample_dataset.py#20",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfceac254a19ac29c67d",
    "number": 1041,
    "body": "",
    "head_branch": "readme_pretune",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfcfac254a19ac29c67e",
    "number": 1040,
    "body": "Reviewed By: liezl200\n\nDifferential Revision: D16889252\n\n",
    "head_branch": "export-D16889252",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfcfac254a19ac29c67f",
    "number": 1037,
    "body": "MoE will encounter a dimension mismatch bug when using label-smoothed cross entropy as the criterion, which occurs at [https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/translation_moe.py#L125](url). This is a fix to the bug.",
    "head_branch": "fix-moe",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd0ac254a19ac29c680",
    "number": 1031,
    "body": "It will cause runtime error on some standard datasets (e.g. wikitext-103).\r\n\r\nDetails:\r\nAfter preprocessing to wikitext-103 folder with current master branch, I use fairseq-train and get the following Error:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/home/trinkle/.local/bin/fairseq-train\", line 11, in <module>\r\n    load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\r\n  File \"/data/git/Transformer/fairseq/fairseq_cli/train.py\", line 321, in cli_main\r\n    main(args)\r\n  File \"/data/git/Transformer/fairseq/fairseq_cli/train.py\", line 46, in main\r\n    task.load_dataset(valid_sub_split, combine=False, epoch=0)\r\n  File \"/data/git/Transformer/fairseq/fairseq/tasks/language_modeling.py\", line 167, in load_dataset\r\n    break_mode=self.args.sample_break_mode, include_targets=True,\r\n  File \"/data/git/Transformer/fairseq/fairseq/data/token_block_dataset.py\", line 54, in init\r\n    \"Found multiple blank lines in the dataset, please remove them\"\r\nAssertionError: Found multiple blank lines in the dataset, please remove them (eg. cat -s raw.txt) and preprocess the data again.\r\n```\r\n\r\nIt's because these datasets have multiple blank lines. The assertion is added in https://github.com/pytorch/fairseq/commit/851c022610b27da3beaa4e40a6834b5fb3b44f44, however, adding this assertion is not a good way.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd1ac254a19ac29c681",
    "number": 1028,
    "body": "Summary:\ntri-stage lr-scheduler consisted of 3 stages: 1. warmup; 2. hold; 3.\n(exponentially) decay; used in https://arxiv.org/pdf/1904.08779.pdf\n\nReviewed By: myleott\n\nDifferential Revision: D16806206\n\n",
    "head_branch": "export-D16806206",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd2ac254a19ac29c682",
    "number": 1017,
    "body": "Changelog:\r\n- Relicensed under MIT license\r\n- Add RoBERTa\r\n- Add wav2vec\r\n- Add WMT'19 models\r\n- Add initial ASR code\r\n- Changed torch.hub interface (`generate` renamed to `translate`)\r\n- Add `--tokenizer` and `--bpe`\r\n- f812e52: Renamed data.transforms -> data.encoders\r\n- 654affc: New Dataset API (optional)\r\n- `47fd985`: Deprecate old Masked LM components\r\n- `5f78106`: Set mmap as default dataset format and infer format automatically\r\n- Misc fixes for sampling\r\n- Misc fixes to support PyTorch 1.2",
    "head_branch": "v0.8.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd3ac254a19ac29c683",
    "number": 1015,
    "body": "Creates a fixed number of Processes each computing an equal chunk instead of one thread per entry. Speedup on librispeech 960h is about 8x.",
    "head_branch": "fastproc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd3ac254a19ac29c684",
    "number": 1014,
    "body": "",
    "head_branch": "commonsense_qa",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd4ac254a19ac29c685",
    "number": 1008,
    "body": "",
    "head_branch": "rm_lamb",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd5ac254a19ac29c686",
    "number": 1007,
    "body": "",
    "head_branch": "restore_file",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd6ac254a19ac29c687",
    "number": 1006,
    "body": "",
    "head_branch": "fix_mnli_hub",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd7ac254a19ac29c688",
    "number": 1005,
    "body": "",
    "head_branch": "python35",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd7ac254a19ac29c689",
    "number": 1004,
    "body": "",
    "head_branch": "roberta_wsc",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd8ac254a19ac29c68a",
    "number": 1002,
    "body": "Summary:\nThis task and loss are used for sentence ranking and multiple choice tasks such as RACE\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/809\n\nReviewed By: myleott\n\nDifferential Revision: D16715745\n\nPulled By: myleott\n\n",
    "head_branch": "export-D16715745",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfd9ac254a19ac29c68b",
    "number": 1000,
    "body": "To install on MacOS, `-stdlib=libc++` needs to be specified.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdaac254a19ac29c68c",
    "number": 997,
    "body": "Allow shell script to create sub directories with -p flag. Amends readme file too.",
    "head_branch": "mkdir_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdbac254a19ac29c68d",
    "number": 995,
    "body": "Datasets with many examples can generate very large indexes in TokenBlockDataset (and possibly elsewhere). When using `--num-workers>0` these indexes are pickled and transferred via a multiprocessing pipe, which is slow and can fail if the index grows beyond 4GB (~0.5B examples). Apache Arrow has an in-memory store called Plasma that will offload these arrays to shared memory, which both reduces duplication of the data and avoids needing to pickle.",
    "head_branch": "plasma",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdbac254a19ac29c68e",
    "number": 991,
    "body": "Fixes #989 ",
    "head_branch": "fix_tests",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdcac254a19ac29c68f",
    "number": 982,
    "body": "",
    "head_branch": "fix_masking",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfddac254a19ac29c690",
    "number": 972,
    "body": "to resolve the issue #971",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdeac254a19ac29c691",
    "number": 969,
    "body": "",
    "head_branch": "doc_encode",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdfac254a19ac29c692",
    "number": 965,
    "body": "",
    "head_branch": "roberta-bos_eos",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfdfac254a19ac29c693",
    "number": 960,
    "body": "Summary:\nTracing mode doesn't generalize correctly in positional embedding calculation, which caused -5 BLEU at transformer export when using pytorch native.\n\nDetails: The original issue was that in ensemble_export, _to_tensor(x) in scripting mode turns integer x into 1-d tensor torch.tensor([x]), not 0-d tensor (scalar x) which is expected in the embedding. So the return value in embedding forward() is actually of wrong shape. When self.weights is of size [x,y], the return value should be (bsz, y, 1) but it was (bsz, 1, y), which caused problem in downstream computation. Tracing only becomes an issue when I used pos = timestep.view(-1)[0] to fix the shape. Then casting the scalar to primary int, to be used as index is not generalizable by tracing mode. Thus I need to convert everything to tensor and replace the advanced indexing with index_select operator.\n\nIn summary, less understood features in both scripting&tracing sides caused the bleu drop. :)\n\nReviewed By: myleott\n\nDifferential Revision: D16623025\n\n",
    "head_branch": "export-D16623025",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe0ac254a19ac29c694",
    "number": 959,
    "body": "We will raise exceptions if these are needed and aren't available. Only keep minimum set of reqs",
    "head_branch": "fewer_hub_reqs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe1ac254a19ac29c695",
    "number": 948,
    "body": "Identity is not the same thing as equality in Python.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe2ac254a19ac29c696",
    "number": 944,
    "body": "",
    "head_branch": "fix_syntax",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe2ac254a19ac29c697",
    "number": 941,
    "body": "Summary:\r\nAdding a backslash in the convolutional language model training usage.",
    "head_branch": "language_model_readme",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe3ac254a19ac29c698",
    "number": 937,
    "body": "Just a small fix for issue #936 .",
    "head_branch": "use_cmdline_interface",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe4ac254a19ac29c699",
    "number": 931,
    "body": "Fixes #930.",
    "head_branch": "roberta_decode",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe5ac254a19ac29c69a",
    "number": 929,
    "body": "Fixes #926 ",
    "head_branch": "tokenization_bug",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe6ac254a19ac29c69b",
    "number": 924,
    "body": "",
    "head_branch": "roberta_max_pos",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe6ac254a19ac29c69c",
    "number": 923,
    "body": "",
    "head_branch": "add_missing_files",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe7ac254a19ac29c69d",
    "number": 921,
    "body": "",
    "head_branch": "roberta_torch_1.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe8ac254a19ac29c69e",
    "number": 920,
    "body": "",
    "head_branch": "fix_model_import",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe9ac254a19ac29c69f",
    "number": 917,
    "body": "",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfe9ac254a19ac29c6a0",
    "number": 916,
    "body": "",
    "head_branch": "roberta_v1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfeaac254a19ac29c6a1",
    "number": 914,
    "body": "",
    "head_branch": "adamax",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfebac254a19ac29c6a2",
    "number": 913,
    "body": "",
    "head_branch": "hub_generator_base",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfecac254a19ac29c6a3",
    "number": 912,
    "body": "",
    "head_branch": "pad_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfedac254a19ac29c6a4",
    "number": 911,
    "body": "",
    "head_branch": "dataset_improvements",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfeeac254a19ac29c6a5",
    "number": 910,
    "body": "",
    "head_branch": "fix_torch_compat",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfeeac254a19ac29c6a6",
    "number": 909,
    "body": "",
    "head_branch": "expose_more_layers",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfefac254a19ac29c6a7",
    "number": 904,
    "body": "Added patience flag to support early stop when the validation metric does not improve for certain no of patience epochs",
    "head_branch": "add_patience",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff0ac254a19ac29c6a8",
    "number": 901,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/763\n\nTransformer variant with two encoders, which allows conditioning on two representations of the input sentence.\n\nDifferential Revision: D16426396\n\n",
    "head_branch": "export-D16426396",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff1ac254a19ac29c6a9",
    "number": 899,
    "body": "",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff2ac254a19ac29c6aa",
    "number": 894,
    "body": "Summary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/804\n\nAdding an implementation of the sparse transformer to multi-head attention using the fixed attention pattern specified https://arxiv.org/pdf/1904.10509.pdf. The sparse_mask masks out words using -inf; after softmax, -inf becomes 0. Thus, a mask does not need to be re-calculated and re-applied when multiplying attn_weights and values.\n\nFour inputs are added to the config: sparse, is_bidirectional, stride, expressivity. If we are using the sparse transformer, is_bidirectional, stride, and expressivity must be specified (there are defaults). If is_bidirectional is False, the mask values using the fixed attention pattern described in the paper. If is_bidirectional is True, subset one includes all values in the current stride window and a summary from every stride window--all other values are masked. Stride (L in the paper) controls the window size and expressivity (c in the paper) controls the size of the summary.\n\nDifferential Revision: D16042988\n\n",
    "head_branch": "export-D16042988",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff3ac254a19ac29c6ab",
    "number": 893,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff4ac254a19ac29c6ac",
    "number": 891,
    "body": "No major API changes since the last release. Cutting a new release since we'll be merging significant (possibly breaking) changes to logging, data loading and the masked LM implementation soon.",
    "head_branch": "v0.7.2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff4ac254a19ac29c6ad",
    "number": 882,
    "body": "Two issues here:\r\n\r\n1. `last_included` should be the last included index `cumsum_mask[:, :, -1:]` instead of `cumsum_mask[:, :, :1]`  (which is either 0 or 1);\r\n\r\n2. If `--no-repeat-ngram-size` is set, the sum of `probs` may less than 1, we need to re-normalize to make it a valid probability distribution\r\n\r\nThe following code can reproduce this issues:\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\ndef _sample_topp(probs):\r\n\r\n    # =====  Code from  fairseq/search.py _sample_topp ======\r\n\r\n    # sort the last dimension (vocab dimension) in descending order\r\n    sorted_probs, sorted_indices = probs.sort(descending=True)\r\n\r\n    # compute a mask to indicate the words to be included in the top-P set.\r\n    cumsum_probs = sorted_probs.cumsum(dim=2)\r\n    mask = cumsum_probs.lt(sampling_topp)\r\n\r\n    # note that mask was computed by 'lt'. One more word needs to be included\r\n    # so that the cumulative probability mass can exceed p.\r\n    cumsum_mask = mask.cumsum(dim=2)\r\n    last_included = cumsum_mask[:, :, :1]\r\n    mask = mask.scatter_(2, last_included, 1)\r\n\r\n    # truncate unnecessary dims.\r\n    max_dim = last_included.max()\r\n    truncated_mask = mask[:, :, :max_dim + 1]\r\n    truncated_probs = sorted_probs[:, :, :max_dim + 1]\r\n    truncated_indices = sorted_indices[:, :, :max_dim + 1]\r\n\r\n    # trim the words that are not in top-P by setting their probabilities\r\n    # to 0, so that they would not be sampled later.\r\n    trim_mask = 1 - truncated_mask\r\n    trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)\r\n    return trimed_probs, truncated_indices\r\n\r\n    # ========================================================\r\n\r\n\r\nif __name__ == '__main__':\r\n    np.random.seed(1234)\r\n    torch.manual_seed(1234)\r\n\r\n    sampling_topp = 0.9\r\n    probs = torch.softmax(torch.randn(1, 1, 10), dim=-1)\r\n    # probs = tensor([0.0545, 0.0779, 0.0189, 0.0647, 0.0282, 0.0862, 0.0656, 0.1041, 0.0399, 0.4600])\r\n    print('probs =', probs[0][0])\r\n\r\n    trimed_probs, truncated_indices = _sample_topp(probs)\r\n\r\n    cum_probs = trimed_probs.cumsum(dim=-1)[0][0]\r\n    # cumsum = tensor([0.4600, 0.5641])\r\n    print('cumsum =', cum_probs)\r\n    # Will throw AssertionError\r\n    assert float(cum_probs[-1]) >= sampling_topp\r\n\r\n\r\n```",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff5ac254a19ac29c6ae",
    "number": 879,
    "body": "Summary: Details in https://fb.workplace.com/notes/ning-dong/closing-research-to-production-gap-a-story-of-latent-variable-model-migration/443418839813586/\n\nDifferential Revision: D15742439\n\n",
    "head_branch": "export-D15742439",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff6ac254a19ac29c6af",
    "number": 877,
    "body": "tensor resizing doesn't work well with tpus, this change is equivalent\r\nto the base and works better w/ tpus.",
    "head_branch": "model",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff7ac254a19ac29c6b0",
    "number": 876,
    "body": "applying non_pad_mask results in dynamic shapes = bad for tpus\r\nThis is an equivalent loss computation (tested), but tensor shapes are\r\nconstant (in the case of reduce=True)",
    "head_branch": "loss",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff8ac254a19ac29c6b1",
    "number": 875,
    "body": "Since mask really is a tensor of ints, this change should be mathematically\r\nequivalent to the base.\r\n\r\nOn the other hand, this has performance implications for xla, hence the\r\npull request.",
    "head_branch": "utils",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff9ac254a19ac29c6b2",
    "number": 872,
    "body": "self._optimizer has __getstate__\r\nWe need this so that fairseq_optimizer's work with pytorch/xla\r\n\r\n```\r\n% find . | xargs grep -s -i __getstate__\r\n./third_party/tensorflow/tensorflow/python/util/deprecation_wrapper.py:  def __getstate__(self):\r\n./torch_xla_py/xla_model.py:  for param_group in optimizer.__getstate__()['param_groups']:\r\n```",
    "head_branch": "optimizerstate",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bff9ac254a19ac29c6b3",
    "number": 871,
    "body": "We need this so that `progress_bar`s work with pytorch/xla i.e. TPUs. See [here](https://github.com/pytorch/xla/blob/master/torch_xla_py/data_parallel.py#L130).",
    "head_branch": "progressbarlen",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffaac254a19ac29c6b4",
    "number": 870,
    "body": "This functions like `fairseq-interactive`, for example:\r\n\r\n```bash\r\ncat input_file.txt | \\\r\n    python fairseq/encode.py \\\r\n    corpora/wmt15_en_fr/data-bin  \\\r\n    --path models/en-fr.pt \\\r\n    --buffer-size 100 \\\r\n    --batch-size 64 \\\r\n    --max-tokens 4000 \\\r\n    --output-file output_file.npy\r\n```\r\n\r\nThis will save the average encoding for each sentence in `input_file.txt` as a numpy array in `output_file.npy`.\r\n\r\nTo give an idea, with my transformer model (embedding size = 512), the encodings for a 1M sentences file weigh 2GB in `.npy` format.\r\n\r\n",
    "head_branch": "encode",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffbac254a19ac29c6b5",
    "number": 862,
    "body": "Fairseq wouldn't install on macOS.\r\nA workaround was found here: https://github.com/pytorch/fairseq/issues/289\r\nThis is now automatic in setup.py, maybe be there's a cleaner way to do it.\r\n\r\nI checked that it compiles fine on Linux and macOS.",
    "head_branch": "macos-compile-flags",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffcac254a19ac29c6b6",
    "number": 860,
    "body": "The PyTorch document on pack_padded_sequence has no information regarding a requirement that padding must be on the right. Therefore, this information is added as a comment on Line 212 of [https://github.com/vineetk1/fairseq/blob/master/fairseq/models/lstm.py](url)",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffdac254a19ac29c6b7",
    "number": 851,
    "body": "The PyTorch document on pack_padded_sequence has no information regarding a requirement that padding must be on the right. Therefore, this information is added as a comment on Line 212 of [https://github.com/vineetk1/fairseq/blob/master/fairseq/models/lstm.py](url), commit 8c2139a to inform the coder of such a requirement.\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffdac254a19ac29c6b8",
    "number": 849,
    "body": "Fix issue #848",
    "head_branch": "fix_epoch",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bffeac254a19ac29c6b9",
    "number": 847,
    "body": "",
    "head_branch": "fix_cp",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621bfffac254a19ac29c6ba",
    "number": 844,
    "body": "",
    "head_branch": "fix_transformer_docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c000ac254a19ac29c6bb",
    "number": 835,
    "body": "When we have multiple valid subsets, say `valid`, `valid1` and `valid2`, if `combine=True` holds, when loading `valid` subset, it will try to locate and load `valid`, `valid1`, `valid2`... and then combine them into one dataset. Set `combine` to `False` solves this issue.\r\n\r\nIn my experiment, I have 3 valid subsets with 3000, 5000 and 8701 examples, with argument `--valid-subset valid,valid1,valid2`, the log is as follows:\r\n\r\n```\r\n...... \r\n| ./mix_data/bin valid src-trg 3000 examples\r\n| ./mix_data/bin valid1 src-trg 5000 examples\r\n| ./mix_data/bin valid2 src-trg 7801 examples\r\n| ./mix_data/bin valid1 src-trg 5000 examples\r\n| ./mix_data/bin valid2 src-trg 7801 examples\r\n...... \r\n```\r\n\r\nAs shown above, `valid1` and `valid2` subsets are incorrectly loaded twice.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c001ac254a19ac29c6bc",
    "number": 831,
    "body": "Repeated use of 'i' in evaluate may cause some problems.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c001ac254a19ac29c6bd",
    "number": 830,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c002ac254a19ac29c6be",
    "number": 829,
    "body": "- Add ensemble features (different from the builtin implementation): support to ensemble many encoder-decoder models with weights.\r\n- Add support to ensemble two different task architecture tasks (Translation/LanguageModel Tasks).\r\n- Add option to enable output scores evaluated by language model.\r\n\r\n**If keep the origin cmd option, then generate.py just behave as what it does before.**\r\n\r\nDescription:\r\n> We absorb in idea from this code gist: https://github.com/marian-nmt/marian/blob/2c164a4363fcd65db3b0c3ee13071583007b9209/src/translator/beam_search.h#L238, if the weights were set set properly, the ensembled would achieve better performance.\r\n\r\nFor ensemble same architecture models:\r\n - add --ensemble-method option to set ensemble method (sum/logexpsum)\r\n - add --ensemble-weights option to set ensemble weights of models\r\n\r\nFor different tasks (Translation/Language Modeling):\r\n - add --with-ensemble-dir option to support models/data in the same directory.\r\n - add list parse support for --model-overrides with different arch/model support (The ensemble of a Translation task model and a Language Modeling task is tested).\r\n\r\nFor Language modeling sentence score:\r\n - add --output-sent-score option to enable scores and can be separated with \"grep ^-\"\r\n\r\nThe detailed description is in the commit message (last three).",
    "head_branch": "feature_ensemble",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c003ac254a19ac29c6bf",
    "number": 828,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c004ac254a19ac29c6c0",
    "number": 827,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c005ac254a19ac29c6c1",
    "number": 823,
    "body": "Summary:\nStochastic beam search aims to find top-k beams w/o replacement. They modify beam search at each time step using Gumbel-max trick but can guarantee beams are approximately optimal in global (complete sentences) in linear complexity. The generated translations are more diverse than BS, DBS, etc, and they show that this is helpful to estimate stats involving expectation over beams. We try to enable this in order to improve our RL training.\n\nThis paper won the runner-up reward at ICML'19 https://arxiv.org/abs/1903.06059\nThe algorithm is implemented in fairseq and the code is available online https://github.com/wouterkool/stochastic-beam-search\n\nDifferential Revision: D15871429\n\n",
    "head_branch": "export-D15871429",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c005ac254a19ac29c6c2",
    "number": 819,
    "body": "",
    "head_branch": "0.7.1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c006ac254a19ac29c6c3",
    "number": 818,
    "body": "",
    "head_branch": "pypi",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c007ac254a19ac29c6c4",
    "number": 817,
    "body": "Notable (possibly breaking) changes:\r\n- d45db80: Remove checkpoint utility functions from utils.py into checkpoint_utils.py\r\n- f2563c2: Move LM definitions into separate files\r\n- dffb167: Updates to model API:\r\n  - `FairseqModel` -> `FairseqEncoderDecoderModel`\r\n  - add `FairseqDecoder.extract_features` and `FairseqDecoder.output_layer`\r\n  - `encoder_out_dict` -> `encoder_out`\r\n  - rm unused `remove_head` functions\r\n- 34726d5: Move `distributed_init` into `DistributedFairseqModel`\r\n- cf17068: Simplify distributed launch by automatically launching multiprocessing on each node for all visible GPUs (allows launching just one job per node instead of one per GPU)\r\n- d45db80: Change default LR scheduler from `reduce_lr_on_plateau` to `fixed`\r\n- 96ac28d: Rename `--sampling-temperature` -> `--temperature`\r\n- fc1a19a: Deprecate dummy batches\r\n- a1c997b: Add memory mapped datasets\r\n- 0add50c: Allow cycling over multiple datasets, where each one becomes an \"epoch\"\r\n\r\nPlus many additional features and bugfixes",
    "head_branch": "version",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c008ac254a19ac29c6c5",
    "number": 816,
    "body": "I have made an upgrade to my previous implementation of MMapIndexedDataset, now:\r\n- It uses up to **4 times less memory and disk space**\r\n- Words per second is slightly improved thanks to less memory access ",
    "head_branch": "features/mmap_dataset",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c009ac254a19ac29c6c6",
    "number": 815,
    "body": "I have made an upgrade to my previous implementation of MMapIndexedDataset, now:\r\n- It uses up to **4 times less memory and disk space**\r\n- Words per second is slightly improved thanks to less memory access ",
    "head_branch": "features/mmap_dataset",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c009ac254a19ac29c6c7",
    "number": 813,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/663\n\nPull Request resolved: https://github.com/fairinternal/fairspeq/pull/4\n\nIntroduce new training for speech models which accept additional training data.\n\nDifferential Revision: D15846661\n\n",
    "head_branch": "export-D15846661",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00aac254a19ac29c6c8",
    "number": 812,
    "body": "â€¦ enabled.\r\n\r\nWhen doing multi-gpu training with --use-bmuf turned on and --global-sync-iter > 1, each replica may not sync with other replicas at each iteration. So logging_outputs only has stats of their own.  On the other hand, logging_outputs may be empty at the end of an epoch after \"a dummy iteration\" because the number of replicas does not divide the number of batches of the training data. If this happens, sample_size and ntokens would be 0 for some replica  and cause \"divided by 0\" error. This fix sets *loss to 0 if sample_size/ntokens is 0.",
    "head_branch": "bug_fix2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00bac254a19ac29c6c9",
    "number": 811,
    "body": "",
    "head_branch": "embed_dim",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00cac254a19ac29c6ca",
    "number": 804,
    "body": "â€¦rch.distributed.ReduceOp",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00dac254a19ac29c6cb",
    "number": 803,
    "body": "It's so much faster to extract (3 minutes instead of 20).",
    "head_branch": "paths",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00dac254a19ac29c6cc",
    "number": 801,
    "body": "",
    "head_branch": "hub_deps2",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00eac254a19ac29c6cd",
    "number": 800,
    "body": "it is better to train denoising autoencoder first.\r\nfor unsupervised mt, it may have better backtranslation.\r\nref: https://github.com/facebookresearch/XLM/blob/master/train.py#L288-L298",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c00fac254a19ac29c6ce",
    "number": 799,
    "body": "",
    "head_branch": "hub_deps",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c010ac254a19ac29c6cf",
    "number": 797,
    "body": "",
    "head_branch": "argparse_defaults",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c011ac254a19ac29c6d0",
    "number": 796,
    "body": "This is a temporary workaround to support sampling after #713. We'll need to revisit this to support sampling and beam more generally.",
    "head_branch": "bsz1_exception",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c011ac254a19ac29c6d1",
    "number": 794,
    "body": "See #467. Ping @myleott to review.\r\n\r\nThis is a work-related contribution. Ping @lark to review.",
    "head_branch": "python3.5-compat",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c012ac254a19ac29c6d2",
    "number": 793,
    "body": "",
    "head_branch": "iter_torch_hub",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c013ac254a19ac29c6d3",
    "number": 792,
    "body": "",
    "head_branch": "generic_registry",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c014ac254a19ac29c6d4",
    "number": 791,
    "body": "- make it possible to load file_utils.py without the dependencies\r\n- add some more demo features",
    "head_branch": "more_demo",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c014ac254a19ac29c6d5",
    "number": 784,
    "body": "Summary:\nThere are 2 problems with old sizes() function.\n\n1. It only supports sample ratios of type int\n\n2. some datasets (e.g. LanguagePairDataset) don't have sizes()\n\nDifferential Revision: D15720559\n\n",
    "head_branch": "export-D15720559",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c015ac254a19ac29c6d6",
    "number": 778,
    "body": "In the current progress bar, the counter for log_interval will always start from 0, which is not correct if  reloading from a checkpoint in the middle of an epoch. This fix obtains the offset from the iterator to set the counter correctly.",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c016ac254a19ac29c6d7",
    "number": 776,
    "body": "Resolves #762",
    "head_branch": "fix-xlm-docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c017ac254a19ac29c6d8",
    "number": 774,
    "body": "Summary:\nforked masked_lm_dictionary from fairseq\nchanged import in pytorch_translate to use the new masked_lm_dictionary\nregistered cooresponding tasks\n\nDifferential Revision: D15410352\n\n",
    "head_branch": "export-D15410352",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c018ac254a19ac29c6d9",
    "number": 772,
    "body": "Integrate torch.nn and fairseq MultiheadAttention modules. In the future, both libraries will be benefited from performance optimization together.\r\n\r\nUnder the following circumstances, the calculation of the MultiheadAttention will still remain in fairseq, including:\r\n1. onnx trace\r\n2. incremental state\r\n3. static kv\r\n\r\nWe plan to gradually mitigate those capabilities to PyTorch's core library.\r\n\r\nFaieseq users can user the attribute self.enable_torch_version to force the calculations in either torch or fairseq. We use the following script to ensure both versions yield the same results.\r\n\r\n------------------------------------------------------------------------------------\r\n```\r\nimport torch\r\nfrom fairseq.modules import MultiheadAttention\r\nimport time\r\n\r\nembed_dim = 64\r\nkv_embed_dim = 1208\r\nnum_heads = 16\r\nsrc_len = 20\r\ntgt_len = 30\r\nbsz = 10\r\n\r\nmodel = MultiheadAttention(embed_dim, num_heads, kdim=kv_embed_dim, vdim=kv_embed_dim,\r\n                           bias=True, add_bias_kv=True, add_zero_attn=True)\r\n\r\nquery = torch.rand((src_len, bsz, embed_dim))\r\nkey = torch.rand((src_len, bsz, kv_embed_dim))\r\nvalue = torch.rand((src_len, bsz, kv_embed_dim))\r\n\r\nattn_mask = torch.randint(0, 2, (src_len, src_len)).float()\r\nattn_mask.masked_fill_(attn_mask == 0, float('-inf'))\r\nattn_mask.masked_fill_(attn_mask > 0, float('0.0'))\r\n\r\nseq_mask = torch.randint(0, 2, (1, src_len))\r\nkey_padding_mask = seq_mask\r\nfor i in range(bsz-1):\r\n    key_padding_mask = torch.cat([key_padding_mask, seq_mask], axis=0)\r\nkey_padding_mask = key_padding_mask == 1\r\n\r\n# Apply torch.nn version\r\nmodel.enable_torch_version = True\r\ntorch_output, torch_weight = model(query, key, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\r\n\r\n# Apply fairseq version\r\nmodel.enable_torch_version = False\r\nfairseq_output, fairseq_weight = model(query, key, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\r\n\r\nprint(\"torch and fairseq generate same results: outputs are same ? \", \r\n      torch.allclose(torch_output, fairseq_output, atol=5e-6, rtol=1e-6),\r\n      \", weights are same ? \", \r\n      torch.allclose(torch_weight, fairseq_weight, atol=5e-6, rtol=1e-6)                                                    \r\n)\r\n```\r\n------------------------------------------------------------------------------------\r\nExpected results:\r\ntorch and fairseq generate same results: outputs are same ?  True , weights are same ?  True\r\n\r\n------------------------------------------------------------------------------------\r\nSimilar performance is expected for both two versions. Using the following setup and have the initial performance benchmark results:\r\n\r\n#########################\r\nembed_dim = 32\r\nkv_embed_dim = 32\r\nnum_heads = 4\r\nsrc_len = 3\r\ntgt_len = 2\r\nbsz = 4\r\nnum_samples = 50000\r\n\r\n#########################\r\ntorch-version MultiheadAttention cpu time: 0.46589  ms per iteration.\r\nfairseq-version MultiheadAttention cpu time: 0.47861  ms per iteration.\r\ntorch-version MultiheadAttention gpu time: 0.82330  ms per iteration.\r\nfairseq-version MultiheadAttention gpu time: 0.79410  ms per iteration.\r\n\r\n\r\n",
    "head_branch": "torch_version_multihead_attn",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c018ac254a19ac29c6da",
    "number": 770,
    "body": "Summary: Without this change comment here https://fburl.com/w1cejgw9 is inconsistent with the implementation.\n\nDifferential Revision: D15582826\n\n",
    "head_branch": "export-D15582826",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c019ac254a19ac29c6db",
    "number": 769,
    "body": "Resolves #768",
    "head_branch": "fix-dynamic-conv-docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01aac254a19ac29c6dc",
    "number": 766,
    "body": "Change the wording to avoid confusion. Mixed precision ensures both higher arithmetic throughput and numerical stability, not exactly synonymous to pure half-precision/FP16 training. Also add mentioning of tensor cores since older generation GPUs without tensor cores don't support true mixed precision training.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01bac254a19ac29c6dd",
    "number": 765,
    "body": "Summary: This diff has changes needed to make XLM torchscript exportable.\n\nReviewed By: bethebunny\n\nDifferential Revision: D15497208\n\n",
    "head_branch": "export-D15497208",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01cac254a19ac29c6de",
    "number": 763,
    "body": "Fix the mismatching between the parameter fed into `SummaryWriter` and the API of the latest [tensorboardX](https://github.com/lanpa/tensorboardX/blob/3e35c9b5f85e8ceb0294532d9eb772341a04c097/tensorboardX/writer.py#L192), i.e. \"log_dir\" -> \"logdir\".",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01cac254a19ac29c6df",
    "number": 758,
    "body": "Summary:\nPull Request resolved: https://github.com/fairinternal/fairseq-py/pull/603\n\nfixed a typo for _mask_block of mlm. This typo will make we never set masked token as random token, which should take 10% of the masked tokens.\n\nReviewed By: akinh\n\nDifferential Revision: D15492315\n\n",
    "head_branch": "export-D15492315",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01dac254a19ac29c6e0",
    "number": 754,
    "body": "Remove duplicate definition of PositionalEmbedding in `lightconv.py`",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01eac254a19ac29c6e1",
    "number": 753,
    "body": "",
    "head_branch": "dlcl",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c01fac254a19ac29c6e2",
    "number": 752,
    "body": "Summary: previously we sample masked tokens with replace=True (default). Because of this, we would mask same tokens multiple times, which will make us mask less tokens finally\n\nReviewed By: liaimi\n\nDifferential Revision: D15403556\n\n",
    "head_branch": "export-D15403556",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c020ac254a19ac29c6e3",
    "number": 747,
    "body": "Summary:\nIn https://github.com/pytorch/fairseq/pull/647, checkpoint averaging\nis not Implemented correctly when it comes to shared parameters. This diff\nhas the right Implementation and a test case to guard future change.\n\nDifferential Revision: D15402943\n\n",
    "head_branch": "export-D15402943",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c020ac254a19ac29c6e4",
    "number": 746,
    "body": "Not sure if I'm doing something wrong elsewhere, but I had a device error in `SinusoidalPositionalEmbedding` when running on GPU > 0 because the weights were on a different device than the input.",
    "head_branch": "sinusoidal_device",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c021ac254a19ac29c6e5",
    "number": 745,
    "body": "Not sure if i'm doing something wrong, but had a device error while trying to run sinusoid position embeddings on a non-0 GPU because the weight in `SinusoidalPositionalEmbedding` only copied type and not device.",
    "head_branch": "visatt",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c022ac254a19ac29c6e6",
    "number": 744,
    "body": "Summary:\nAfter we added additional prediciton layers for language model predictions. The fine-tuning is broken because of 2 reasons.\n1. checkpoint cannot be loaded since we didn't update state_dict names\n2. lm_output_learned_bias is not initialize if load_softmax is false\n\nReviewed By: myleott\n\nDifferential Revision: D15377380\n\n",
    "head_branch": "export-D15377380",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c023ac254a19ac29c6e7",
    "number": 743,
    "body": "Summary:\nOriginal commit changeset: 0afe37c9a031\n\nAccording to edunov: \"We need to be careful here with shared parameters, I believe right now it is broken if you have shared encoder/decoder input embeddings (encoder.embed_tokens.weight and decoder.embed_tokens.weight) as they get updated several times\"\n\nWe also have OSS issues that look related, e.g., https://github.com/pytorch/fairseq/issues/732.\n\nBacking this out until we can confirm the correct behavior for shared params.\n\nDifferential Revision: D15372673\n\n",
    "head_branch": "export-D15372673",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c024ac254a19ac29c6e8",
    "number": 736,
    "body": "",
    "head_branch": "scripts",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c024ac254a19ac29c6e9",
    "number": 735,
    "body": "`--output-format` -> `--dataset-impl` in Tutorial: Classifying Names with a Character-Level RNN",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c025ac254a19ac29c6ea",
    "number": 733,
    "body": "Hi all,\r\n\r\nThanks for releasing the code and models for the _Hierarchical Neural Story Generation_ paper!\r\n\r\nWe have been trying to reproduce the prompt ranking accuracy metric mentioned in the paper. The paper says (Figure 6) that the full fusion model gets 16.3% accuracy. Weirdly, when we run our implementation of the prompt ranking accuracy evaluation, we get something in the region of 39.8% accuracy instead.\r\n\r\nAs far as we can tell, we've done the evaluation as described in the paper - but perhaps we've done something wrong, or maybe our method for sampling the examples is different to what was done in the paper?\r\n\r\nThis PR contains code and instructions to run our prompt ranking accuracy evaluation and reproduce our 39.8% number. We're hoping that someone at FAIR might be able to help us figure out why we're getting a very different number to that reported in the paper.\r\n\r\nThanks a lot!\r\nAneesh",
    "head_branch": "prompt_ranking_pr",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c026ac254a19ac29c6eb",
    "number": 731,
    "body": "Summary: Currently the LearnedPositionalEmbedding module computes the position tensor based on the input data. However this really doesnt work for XLM where we have different behavior based on the Masked LM and Translation LM. In this diff I keep the same default behavior for LearnedPositionalEmbedding as before but add the ability for these models to work with pre-computed position tensors.\n\nDifferential Revision: D15305474\n\n",
    "head_branch": "export-D15305474",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c027ac254a19ac29c6ec",
    "number": 730,
    "body": "Summary:\nPull Request resolved: https://github.com/pytorch/translate/pull/528\n\nAdd/modify necessary functions for ConcatDataset to work in PytorchTranslateTask and replace MultiCorpusSampledDataset which doesn't support mixed batch.\n\nAny idea on how to implement collater here for mixed batch? Now I'm just using the collater of the first dataset.\n\nDifferential Revision: D15260872\n\n",
    "head_branch": "export-D15260872",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c028ac254a19ac29c6ed",
    "number": 727,
    "body": "",
    "head_branch": "fix_lr_scheduler",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #727 from pytorch/fix_lr_scheduler\n\nSet initial learning rate in LR schedulers by calling step_update(0) at init"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c028ac254a19ac29c6ee",
    "number": 726,
    "body": "",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c029ac254a19ac29c6ef",
    "number": 725,
    "body": "Co-authored-by: myleott <myleott@fb.com>\r\n\r\n1) The `task` is currently called `glue_finetune`, although it can be used to train from scratch and would work for any sentence[s] classification task. And currently it can be used only with `masked_lm` model. What would be good name @myleott, `masked_lm_classification`? \r\n",
    "head_branch": "adding_glue_finetuning_task",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02aac254a19ac29c6f0",
    "number": 723,
    "body": "",
    "head_branch": "dataset_impl_err",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02bac254a19ac29c6f1",
    "number": 722,
    "body": "",
    "head_branch": "dataset_impl_err",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02cac254a19ac29c6f2",
    "number": 721,
    "body": "1) Made the model compatible with using either `masked_lm_dataset` or `monolingual_dataset`.\r\n2) fixed default args setting task. (`bert` vs `masked_lm`) @myleott should we keep both?\r\n3) bug in setting default value of `sentence_class_num`\r\n4) bug for padding mask in `fp16`.\r\n ",
    "head_branch": "bug_fixes_and_small_changes_to_masked_lm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02cac254a19ac29c6f3",
    "number": 720,
    "body": "",
    "head_branch": "cleanup_lm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02dac254a19ac29c6f4",
    "number": 719,
    "body": "",
    "head_branch": "tokenblockfix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02eac254a19ac29c6f5",
    "number": 717,
    "body": "",
    "head_branch": "bugfix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c02fac254a19ac29c6f6",
    "number": 715,
    "body": "",
    "head_branch": "fix_args_passing_for_masked_lm_dataset",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c030ac254a19ac29c6f7",
    "number": 713,
    "body": "#712 ",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c030ac254a19ac29c6f8",
    "number": 711,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c031ac254a19ac29c6f9",
    "number": 710,
    "body": "Summary: Previously there was a bug in how we dealt with padding when computing the input representation from the segment and position embedding. D15144912 fixed this by adding an offset based on the padding id. However this makes assumptions about the padding id which may not hold true for vocabularies built outside of pyText and fairseq. Based on a discussion with barlaso, this diff 0's out all the embeddings associated with the padding.\n\nReviewed By: borguz\n\nDifferential Revision: D15209395\n\n",
    "head_branch": "export-D15209395",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c032ac254a19ac29c6fa",
    "number": 707,
    "body": "",
    "head_branch": "redundant_dist_init",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c033ac254a19ac29c6fb",
    "number": 706,
    "body": "Pass required \"sample_key\" argument to forward-backward call in semi-supervised task.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c034ac254a19ac29c6fc",
    "number": 705,
    "body": "Summary: This adds functionality in fairseq to load a pretrained encoder or decoder from another pretrained model into the current model.\n\nReviewed By: jmp84\n\nDifferential Revision: D15207084\n\n",
    "head_branch": "export-D15207084",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c034ac254a19ac29c6fd",
    "number": 704,
    "body": "",
    "head_branch": "faster_data",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c035ac254a19ac29c6fe",
    "number": 703,
    "body": "Summary: It's better to write one checkpoint and copy it, rather than repeatedly pickling the model via torch.save.\n\nDifferential Revision: D15213778\n\n",
    "head_branch": "export-D15213778",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c036ac254a19ac29c6ff",
    "number": 702,
    "body": "We can later get rid off `BertLayerNorm` also, as I think the implementation of that is exactly same as `LayerNorm`. (will confirm with @jingfeidu on that). \r\nBut this should be drop and replace. ",
    "head_branch": "use_fused_layer_norm_in_transformer_sentence_encoder",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c037ac254a19ac29c700",
    "number": 700,
    "body": "See https://github.com/facebookresearch/maskrcnn-benchmark/issues/172",
    "head_branch": "barrier",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c038ac254a19ac29c701",
    "number": 699,
    "body": "It was tedious defining these, let's try just taking the first batch lazily instead.",
    "head_branch": "dummy_batch",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c038ac254a19ac29c702",
    "number": 698,
    "body": "Added bert_large architecture",
    "head_branch": "adding_arch_bert_large",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c039ac254a19ac29c703",
    "number": 697,
    "body": "Co-authored-by: jingfeidu <jingfeidu@fb.com>\r\n\r\n1) Adding `masked_lm` task for BERT like training. Code mostly taken from @jingfeidu 's implementation.\r\n\r\n2) Added `has_eos` option to `block_pair_dataset` for working with dataset that has been preprocessed with having `eos`.\r\n\r\nDepends on: https://github.com/pytorch/fairseq/pull/696",
    "head_branch": "adding_masked_lm_task",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03aac254a19ac29c704",
    "number": 696,
    "body": "Co-authored-by: myleott <myleott@fb.com>\r\n\r\nChanging `data` to be `str` with colon separated list for loading sharded datasets. This change is useful for loading large datasets that cannot fit into, memory. The large dataset can be sharded and then each shard is loaded in one epoch in roudrobin manner.\r\n\r\nFor example, if there are `5` shards of data and `10` epochs then the shards will be iterated upon `[0, 1, 2, 3, 4, 0, 1, 2, 3, 4]`.\r\n\r\n\r\n@myleott We need to look into `translation.py` as it currently already expects a list and then concats the datasets.",
    "head_branch": "allow_sharded_datasets",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03bac254a19ac29c705",
    "number": 695,
    "body": "",
    "head_branch": "better_dist_init",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03cac254a19ac29c706",
    "number": 694,
    "body": "Summary:\nThe previous version applied the temperature after the softmax. Fix that, and\nalso generalize so it works with other search approaches.\n\nDifferential Revision: D15175160\n\n",
    "head_branch": "export-D15175160",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03cac254a19ac29c707",
    "number": 693,
    "body": "Differential Revision: D15174831\n\n",
    "head_branch": "export-D15174831",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03dac254a19ac29c708",
    "number": 692,
    "body": "Differential Revision: D15174954\n\n",
    "head_branch": "export-D15174954",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03eac254a19ac29c709",
    "number": 691,
    "body": "",
    "head_branch": "pickle",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c03fac254a19ac29c70a",
    "number": 690,
    "body": "",
    "head_branch": "mmap",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c040ac254a19ac29c70b",
    "number": 689,
    "body": "Summary:\nWe found not raising OOM during trainer.train_step causes various\nissue, including NCCL hangs / gloo sync errors because gradient is not synced\nproperly. Before we found the root cause, let's give users an option to raise\nOOMs.\n\nDifferential Revision: D15170357\n\n",
    "head_branch": "export-D15170357",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c040ac254a19ac29c70c",
    "number": 687,
    "body": "Summary: This should make rendezvous happen as lazily as possible.\n\nDifferential Revision: D15151145\n\n",
    "head_branch": "export-D15151145",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c041ac254a19ac29c70d",
    "number": 685,
    "body": "",
    "head_branch": "fix_oom",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c042ac254a19ac29c70e",
    "number": 684,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c043ac254a19ac29c70f",
    "number": 683,
    "body": "Co-authored-by: jingfeidu <jingfeidu@fb.com>\r\n\r\nThe implementation is by Jingfei Du from branch \"bigbert\". Copied over to this CR to get it merged in isolation since other changes seem to be already in master.\r\n\r\n**Small changes from original:**\r\nAdded following line in `__init__` as discovered by @myleott :\r\n\r\n```\r\nself.optimizer.set_lr(self.warmup_factor * self.lr)\r\n```",
    "head_branch": "adding_polynomial_lr_scheduler",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c043ac254a19ac29c710",
    "number": 682,
    "body": "",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c044ac254a19ac29c711",
    "number": 681,
    "body": "Differential Revision: D15147107\n\n",
    "head_branch": "export-D15147107",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c045ac254a19ac29c712",
    "number": 680,
    "body": "Summary:\nSome embedding names were renamed but this one was missed\n\nSo far I've only seen this affect our runs during continuing training. If you encountered any errors when continuing training from an XLM save_dir, rebasing past this diff (or patching this and canarying) should fix the problem\n\nReviewed By: pipibjc\n\nDifferential Revision: D15137463\n\n",
    "head_branch": "export-D15137463",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c046ac254a19ac29c713",
    "number": 679,
    "body": "Add missing backslash.",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c047ac254a19ac29c714",
    "number": 676,
    "body": "",
    "head_branch": "cite",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c047ac254a19ac29c715",
    "number": 675,
    "body": "",
    "head_branch": "rm_pt",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c048ac254a19ac29c716",
    "number": 673,
    "body": "Log fairseq's `args` and `sys.argv` in tensorboard to easily identify run hyperparameters from within tensorboard.\r\n\r\nThe idea was suggested in https://twitter.com/Thom_Wolf/status/1106300583835766786\r\n\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c049ac254a19ac29c717",
    "number": 672,
    "body": "Summary: title\n\nReviewed By: jmp84\n\nDifferential Revision: D15094977\n\n",
    "head_branch": "export-D15094977",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04aac254a19ac29c718",
    "number": 670,
    "body": "Summary: Pytorch-translate task needs to use extra arguments (such as vocabulary objects). By passing kwargs, we are able to have the ability to have extra arguments in setup_task\n\nDifferential Revision: D15086810\n\n",
    "head_branch": "export-D15086810",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04aac254a19ac29c719",
    "number": 669,
    "body": "",
    "head_branch": "comment",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04bac254a19ac29c71a",
    "number": 667,
    "body": "Summary: Use smaller models so that unittests won't timeout\n\nDifferential Revision: D15056894\n\n",
    "head_branch": "export-D15056894",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04cac254a19ac29c71b",
    "number": 666,
    "body": "Summary: Option to load the XLM weights into only the encoder or the decoder\n\nReviewed By: pipibjc\n\nDifferential Revision: D14881004\n\n",
    "head_branch": "export-D14881004",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04dac254a19ac29c71c",
    "number": 665,
    "body": "",
    "head_branch": "sample-stories",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04eac254a19ac29c71d",
    "number": 664,
    "body": "Summary:\nPreviously arguments for noising (dropout_prob for WordDropout and max_shuffle_distance for WordShuffle) are only passed in noising() so it could not be customized in NoisingDataset.\n\nNow add default argument in initializer so the value could be specified at construction.\n\nDifferential Revision: D15071632\n\n",
    "head_branch": "export-D15071632",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04eac254a19ac29c71e",
    "number": 662,
    "body": "Added link to blog post about incremental decoder in the FairseqIncrementalDecoder class description.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c04fac254a19ac29c71f",
    "number": 661,
    "body": "",
    "head_branch": "fix_reset_opt_valid_best",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c050ac254a19ac29c720",
    "number": 657,
    "body": "Summary:\nLibrary side change split from D14924942\n\nAdded 2 arguments for load_dataset in PytorchTranslateTask\n1. dataset_upsampling. A nested dictionary {direction:{dataset: upsampling_ratio}}. Upsampling_ratio larger than one mean that the bitext is ob- served more often than actually present in the combined bitext and synthetic training corpus.\n\n2. dataset_relative_ratio. A tuple (dataset, ratio). The ratio represents the frequency certain dataset gets sampled to the rest of corpora map.\n\nAt most one of them could be specified.\n\nDifferential Revision: D15041293\n\n",
    "head_branch": "export-D15041293",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c051ac254a19ac29c721",
    "number": 654,
    "body": "- Add --add-bos-token option to LM task\r\n- Cleanup utils.py and options.py",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c051ac254a19ac29c722",
    "number": 653,
    "body": "Summary:\nAfter this diff, you can train a transformer model with --activation-fn 'relu', 'gelu', or 'gelu_fast'\n\ngelu_fast is the default implementation in https://github.com/hendrycks/GELUs/blob/master/mnist_fcn.py#L72-L77\ngelu is the alternate implementation in https://github.com/hendrycks/GELUs/blob/master/mnist_fcn.py#L72-L77 and the default implementation in https://github.com/facebookresearch/XLM\n\nDifferential Revision: D14966006\n\n",
    "head_branch": "export-D14966006",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c052ac254a19ac29c723",
    "number": 651,
    "body": "Fix index out of bounds when enable --no-early-stop.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c053ac254a19ac29c724",
    "number": 647,
    "body": "Summary:\nthe current implementation of average_checkpoints requires loading all\nthe model parameters into memory and then do the averaging. To average large\nmodels (e.g., transformer) over a large number of checkpoints (e.g., >50),\nit may require over 100GB memory.\n\nLoading all the parameters is not necessary, as we know the number of models in advance.\n\nDifferential Revision: D15027513\n\n",
    "head_branch": "export-D15027513",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c054ac254a19ac29c725",
    "number": 641,
    "body": "Summary: Fix breaking import\n\nReviewed By: pipibjc\n\nDifferential Revision: D14978454\n\n",
    "head_branch": "export-D14978454",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c055ac254a19ac29c726",
    "number": 639,
    "body": "Summary: Add argument sampling_func in the constructor to enable custom sampling over a list of dataset keys. The default strategy is to sample uniformly as it did previously.\n\nReviewed By: liezl200\n\nDifferential Revision: D14965774\n\n",
    "head_branch": "export-D14965774",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c056ac254a19ac29c727",
    "number": 638,
    "body": "Summary: RT\n\nReviewed By: liezl200\n\nDifferential Revision: D14967268\n\n",
    "head_branch": "export-D14967268",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c056ac254a19ac29c728",
    "number": 635,
    "body": "Summary: Adding a task and relevant models, datasets and criteria needed for training Cross-lingual Language Models similar to Masked Language Model used in XLM (Lample and Conneau, 2019 - https://arxiv.org/abs/1901.07291).\n\nDifferential Revision: D14943776\n\n",
    "head_branch": "export-D14943776",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c057ac254a19ac29c729",
    "number": 634,
    "body": "If arg.keep_interval_updates or args.keep_last_epochs > 0, `checkpoints` would refer to a list of checkpoint files to be removed, which can be empty. So moved the logging code to the right position.",
    "head_branch": "bug_fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c058ac254a19ac29c72a",
    "number": 633,
    "body": "Summary:\nPull Request resolved: https://github.com/pytorch/translate/pull/456\n\nThis diff makes it easier to upgrade the state dict for components that use TransformerEncoderLayer\n\nDifferential Revision: D14916941\n",
    "head_branch": "export-D14916941",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c059ac254a19ac29c72b",
    "number": 630,
    "body": "Summary:\nsacrebleu scorer has stopped working in pytorch_translate (maybe\nfairseq too) probably due to  a recent api change.\n\nDifferential Revision: D14792797\n",
    "head_branch": "export-D14792797",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05aac254a19ac29c72c",
    "number": 629,
    "body": "Summary: Use GeLU as an alternate activation layer for ReLU.\n\nDifferential Revision: D14689851\n",
    "head_branch": "export-D14689851",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05bac254a19ac29c72d",
    "number": 628,
    "body": "Summary: Updating embedding layers in TransformerSentenceEncoder to be compatible with the transformer model.\n\nReviewed By: liezl200\n\nDifferential Revision: D14836883\n",
    "head_branch": "export-D14836883",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05bac254a19ac29c72e",
    "number": 627,
    "body": "Because the size of `unfinalized_scores` is equal to current `bsz` and not initial batch size, we need to index it by `unfin_idx` instead of `sent` in `is_finished`.\r\nFixes #588.\r\n",
    "head_branch": "patch_early_stop",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05cac254a19ac29c72f",
    "number": 626,
    "body": "Summary: While training a model on multiple GPUs, the current fairseq train workflow fails while creating the directory from which to load a checkpoint. This seems to be happening because multiple nodes attempt to create the same directory thus causing some weird interaction with os.makedirs option \"exist_ok=True\". Fixing this by making sure only rank 0 creates this directory.\n\nReviewed By: jingfeidu\n\nDifferential Revision: D14841304\n",
    "head_branch": "export-D14841304",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05dac254a19ac29c730",
    "number": 625,
    "body": "",
    "head_branch": "simplify_make_pos",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05eac254a19ac29c731",
    "number": 624,
    "body": "- Add `--gelu`\r\n- Add `--add-bos-token` option to LM task\r\n- Cleanup utils.py and options.py",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05eac254a19ac29c732",
    "number": 622,
    "body": "Summary: Updating some defaults to more meaningful values\n\nReviewed By: rutyrinott\n\nDifferential Revision: D14761263\n",
    "head_branch": "export-D14761263",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c05fac254a19ac29c733",
    "number": 621,
    "body": "Summary: In this commit, I add some modules to Fairseq needed to set up Bert/XLM style pretraining.\n\nReviewed By: borguz\n\nDifferential Revision: D14719663\n",
    "head_branch": "export-D14719663",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c060ac254a19ac29c734",
    "number": 620,
    "body": "- Add language token to MultilingualTranslation task\r\n- Add back translation and denoising loss to MultilingualTranslation task",
    "head_branch": "bt-cleanup",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c061ac254a19ac29c735",
    "number": 618,
    "body": "Summary: PyTorch export for transformer models was broken because as written, they used a placeholder `None` value during inference for the variable `key_padding_mask` to indicate no padding, but PyTorch is unable trace such values. This diff adds a minor hack to allow the use of an empty tensor for the same purpose.\n\nDifferential Revision: D14581730\n",
    "head_branch": "export-D14581730",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c062ac254a19ac29c736",
    "number": 615,
    "body": "Differential Revision: D14712750\n",
    "head_branch": "export-D14712750",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c062ac254a19ac29c737",
    "number": 614,
    "body": "Differential Revision: D14712321\n",
    "head_branch": "export-D14712321",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c063ac254a19ac29c738",
    "number": 613,
    "body": "Differential Revision: D14712311\n",
    "head_branch": "export-D14712311",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c064ac254a19ac29c739",
    "number": 607,
    "body": "",
    "head_branch": "deprecation_warn",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c065ac254a19ac29c73a",
    "number": 606,
    "body": "",
    "head_branch": "fix_interactive_ids",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c066ac254a19ac29c73b",
    "number": 605,
    "body": "Summary:\nEval and log on a subset of directions for multimodel training\n\nThis reduces code duplication in PyTorch Translate's semi_supervised task and will enable clean multitask setups in the future.\n\nDifferential Revision: D14672779\n",
    "head_branch": "export-D14672779",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c066ac254a19ac29c73c",
    "number": 603,
    "body": "This is pull request was created automatically because we noticed your project was missing a Code of Conduct file.\n\nCode of Conduct files facilitate respectful and constructive communities by establishing expected behaviors for project contributors.\n\nThis PR was crafted with love by Facebook's Open Source Team.",
    "head_branch": "automated_fixup_code_of_conduct_file_exists",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c067ac254a19ac29c73d",
    "number": 600,
    "body": "Hi,\r\n\r\ncurrently, the link to the language model readme is broken on the `examples/language_model/transformer_lm` page.\r\n\r\nThis PR fixes the link :)",
    "head_branch": "doc-examples-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c068ac254a19ac29c73e",
    "number": 598,
    "body": "Correcting the syntax error in assert function cause of a character before error message.\r\n\r\nAssertion and the code is working fine now, Tested with wmt-ende task.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c069ac254a19ac29c73f",
    "number": 597,
    "body": "Summary:\nFixes two issues:\n1. the new Layernorm has issues in exporting\n2. fix tensorboard writing by using the \"RAW\" operator_export_type\n\nDifferential Revision: D14610694\n",
    "head_branch": "export-D14610694",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06aac254a19ac29c740",
    "number": 593,
    "body": "The unfold1d.py has the same name as the function `unfold1d` function, which will cause an error when using DynamicConv1dTBC with `unfold=True`.\r\nThis doesn't affect the NMT models which don't use the unfolding mode though.\r\n\r\nI rename `unfold1d.py` as `unfold.py` to fix this bug.\r\n\r\nOriginally we would get `TypeError` when running this code:\r\n```\r\nimport torch\r\nfrom fairseq.modules import LightweightConv1dTBC, DynamicConv1dTBC\r\n\r\n\r\nx = torch.rand(4, 10, 8)\r\nm = LightweightConv1dTBC(8, 4, 3)\r\no = m(x, unfold=True)\r\n\r\nm = DynamicConv1dTBC(8, 4, 3)\r\no = m(x, unfold=True)\r\n```\r\n",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06aac254a19ac29c741",
    "number": 589,
    "body": "Following discussion in https://github.com/pytorch/fairseq/issues/574:\r\n\r\n - Implemented MMapIndexedDataset and MMapIndexedDatasetBuilder compatible with IndexedDataset/IndexedDatasetBuilder\r\n- Update scripts/read_binarized.py to support new MMapIndexedDataset\r\n- Option '--raw-text' and '--lazy-load' replaced with '--dataset-impl' and moved the option definition custom task args to more high-level options.add_dataset_args() (more appropriate)\r\n- Implemented also utils functions in indexed_dataset: make_dataset(), dataset_exists()",
    "head_branch": "features/mmap_dataset",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06bac254a19ac29c742",
    "number": 587,
    "body": "",
    "head_branch": "tokenizer",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06cac254a19ac29c743",
    "number": 586,
    "body": "",
    "head_branch": "moe_script",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06dac254a19ac29c744",
    "number": 584,
    "body": "",
    "head_branch": "tokenizer_recover",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06dac254a19ac29c745",
    "number": 580,
    "body": "",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06eac254a19ac29c746",
    "number": 579,
    "body": "",
    "head_branch": "myleott-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c06fac254a19ac29c747",
    "number": 577,
    "body": "Changelog:\r\n- 998ba4f: Add language models from Baevski & Auli (2018)\r\n- 4294c4f: Add mixture of experts code from Shen et al. (2019)\r\n- 0049349: Add example for multilingual training\r\n- 48d9afb: Speed improvements, including fused operators from apex\r\n- 44d27e6: Add Tensorboard support\r\n- d17fa85: Add Adadelta optimizer\r\n- 9e1c880: Add `FairseqEncoderModel`\r\n- b65c579: Add `FairseqTask.inference_step` to modularize generate.py\r\n- 2ad1178: Add back `--curriculum`\r\n- Misc bug fixes and other features",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c070ac254a19ac29c748",
    "number": 571,
    "body": "Summary: Enable sampling from Fairseq\n\nDifferential Revision: D13981666\n",
    "head_branch": "export-D13981666",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c071ac254a19ac29c749",
    "number": 567,
    "body": "The regex pattern without parentheses is not correct. The checkpoints are not sorted in descending order",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c071ac254a19ac29c74a",
    "number": 561,
    "body": "Add `\\` to fix for the shell command.",
    "head_branch": "multilingual",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c072ac254a19ac29c74b",
    "number": 554,
    "body": "",
    "head_branch": "curriculum",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c073ac254a19ac29c74c",
    "number": 553,
    "body": "Accessing sys.stdin.fileno() raises an error in multiple contexts\r\n(pytest, joblib, jupyter...).\r\nThus accessing it at the top level of the file can cause other scripts\r\nto crash when they import fairseq.\r\nThis is why it is moved inside the method of MultiprocessingPdb to only\r\nbe accessed at runtime if needed.\r\n\r\nSee  Issue #517",
    "head_branch": "multiprocessing-pdb-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c074ac254a19ac29c74d",
    "number": 551,
    "body": "",
    "head_branch": "fix_pdb",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c075ac254a19ac29c74e",
    "number": 550,
    "body": "",
    "head_branch": "workers",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c075ac254a19ac29c74f",
    "number": 548,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c076ac254a19ac29c750",
    "number": 546,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c077ac254a19ac29c751",
    "number": 545,
    "body": "Differential Revision: D14268580\n",
    "head_branch": "export-D14268580",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c078ac254a19ac29c752",
    "number": 544,
    "body": "",
    "head_branch": "checknewupdate",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c078ac254a19ac29c753",
    "number": 543,
    "body": "",
    "head_branch": "test_moe",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c079ac254a19ac29c754",
    "number": 542,
    "body": "",
    "head_branch": "sacrebleu",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07aac254a19ac29c755",
    "number": 541,
    "body": "Summary:\r\n\"Tokenizer\" part from this discussion: https://github.com/fairinternal/fairseq-py/pull/522/files#diff-00da266658125df13cfd08a3e560a41f\r\n- got rid of Tokenizer class\r\n- moved string line encoding logic to Dictionary\r\n- Moved binarize method to a new Binarizer class\r\n\r\nDifferential Revision: D14251048\r\n",
    "head_branch": "export-D14251048",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07bac254a19ac29c756",
    "number": 538,
    "body": "",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07cac254a19ac29c757",
    "number": 531,
    "body": "* Add FusedLayerNorm and FusedAdam\r\n* Softmax and zero grad optimizations",
    "head_branch": "speedups",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07cac254a19ac29c758",
    "number": 530,
    "body": "Enable with the `--tensorboard-logdir` option.",
    "head_branch": "tensorboard",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07dac254a19ac29c759",
    "number": 529,
    "body": "",
    "head_branch": "fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07eac254a19ac29c75a",
    "number": 528,
    "body": "",
    "head_branch": "gen_lm_interactive",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07fac254a19ac29c75b",
    "number": 527,
    "body": "* Add example for multilingual translation on IWSLT'17\r\n* Match dataset ordering for multilingual_translation and translation\r\n* Fix bug with LegacyDistributedDataParallel when calling forward of sub-modules",
    "head_branch": "multiling_ex",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c07fac254a19ac29c75c",
    "number": 523,
    "body": "",
    "head_branch": "script",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c080ac254a19ac29c75d",
    "number": 522,
    "body": "",
    "head_branch": "readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c081ac254a19ac29c75e",
    "number": 521,
    "body": "Code for the paper: [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](https://arxiv.org/abs/1902.07816).",
    "head_branch": "translation_moe",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c082ac254a19ac29c75f",
    "number": 520,
    "body": "This makes it easier for tasks to plugin to generate.py/interactive.py",
    "head_branch": "modularize_generate",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c083ac254a19ac29c760",
    "number": 516,
    "body": "Changelog:\r\n- 6d8aba8: Add TensorBoard logging\r\n- 7b5d419: Add example for multilingual translation (and bugfix for multi-GPU training with no_c10d backend)\r\n- 4d4bed8: Various speed improvements (e.g., fused ops from apex, improved optimizer speed)",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c083ac254a19ac29c761",
    "number": 513,
    "body": "Summary: Move masking logic to data_utils\n\nReviewed By: kartikayk, jingfeidu\n\nDifferential Revision: D14098403\n",
    "head_branch": "export-D14098403",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c084ac254a19ac29c762",
    "number": 505,
    "body": "",
    "head_branch": "merge_internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c085ac254a19ac29c763",
    "number": 498,
    "body": "",
    "head_branch": "composite_loss",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c086ac254a19ac29c764",
    "number": 495,
    "body": "- fairseq can now be installed via pip: `pip install fairseq`\r\n- command-line tools are globally accessible: `fairseq-preprocess`, `fairseq-train`, `fairseq-generate`, etc.",
    "head_branch": "pip",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c086ac254a19ac29c765",
    "number": 492,
    "body": "Summary: This argument was missing so we cannot export Transformer if we use learned positional embeddings. See also https://github.com/pytorch/translate/pull/335\n\nDifferential Revision: D13984781\n",
    "head_branch": "export-D13984781",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c087ac254a19ac29c766",
    "number": 490,
    "body": "Summary:\nWe currently have 47 broken tests in pytorch_translate (https://fburl.com/tests/0riyt9ly) because when we run tests in mode/opt, machine only sees compiled files with *.pyc extension instead of original *.py files.\n\nTherefore, all of these imports are failing with \"key not found\" errors (https://our.intern.facebook.com/intern/tests/details/562949963428218/debug/1561945).\n\nTherefore, changing the way we are finding files, so that it includes compiled python files as well. This was the least intrusive way of solving this issue, let me know if you have other ideas.\n\nDifferential Revision: D13969981\n",
    "head_branch": "export-D13969981",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c088ac254a19ac29c767",
    "number": 489,
    "body": "",
    "head_branch": "binaries",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c089ac254a19ac29c768",
    "number": 484,
    "body": "",
    "head_branch": "interactive_file",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08aac254a19ac29c769",
    "number": 483,
    "body": "Changelog:\r\n- `4889802`: can now remove detokenize sentencepiece output with `--remove-bpe=sentencepiece` (fixes #331). Also added `--sacrebleu` for computing detokenized BLEU.\r\n- `0d76427`: fix assertion error when training language model with dataset containing empty sentences\r\n- minor bug and style fixes",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08aac254a19ac29c76a",
    "number": 482,
    "body": "Summary: With this change, we can use different dictionary classes when calling build_dictionary and build_and_save_dictionary\n\nReviewed By: liaimi\n\nDifferential Revision: D13855100\n",
    "head_branch": "export-D13855100",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08bac254a19ac29c76b",
    "number": 478,
    "body": "Following discussion on #450, here all `fairseq-*` binaries are installed globally by `setup.py` and can be used without knowing the absolute path of the command.\r\nThis is also fundamental as a pre-condition to wheel distribution.\r\n\r\nIt also fix bug #477 ",
    "head_branch": "features/global-binaries",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08cac254a19ac29c76c",
    "number": 474,
    "body": "Reviewed By: theweiho, akinh\n\nDifferential Revision: D13701447\n",
    "head_branch": "export-D13701447",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08dac254a19ac29c76d",
    "number": 473,
    "body": "Changelog:\r\n- `e330f56`: Add code for the \"Pay Less Attention with Lightweight and Dynamic Convolutions\" paper\r\n- `5e3b98c`: Add scripts for computing tokenized BLEU with compound splitting and sacrebleu\r\n- update READMEs\r\n- misc fixes",
    "head_branch": "merge_internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08eac254a19ac29c76e",
    "number": 472,
    "body": "Summary: Implementation of \"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\" (https://arxiv.org/abs/1804.04235)\n\nDifferential Revision: D13388049\n",
    "head_branch": "export-D13388049",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08eac254a19ac29c76f",
    "number": 471,
    "body": "Differential Revision: D13811966\n",
    "head_branch": "export-D13811966",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c08fac254a19ac29c770",
    "number": 470,
    "body": "",
    "head_branch": "lstm_improvements",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c090ac254a19ac29c771",
    "number": 469,
    "body": "",
    "head_branch": "print_model",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c091ac254a19ac29c772",
    "number": 468,
    "body": "",
    "head_branch": "dict_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c091ac254a19ac29c773",
    "number": 467,
    "body": "Although both are supported by Python 3.6, I think it would be better to unify the usage of string format function.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c092ac254a19ac29c774",
    "number": 460,
    "body": "When opening text files without specifying the encoding (i.e. `open(path, \"r\")` or `open(path, \"w\")`), python3 will use the preferred locale encoding (`locale.getpreferredencoding()`) so the result is platform dependent and can change from one machine to another.\r\n\r\nI believe fairseq should enforce its standard (UTF-8 seems like the best choice to me). This pull request explicity specify UTF-8 encoding when reading text files.",
    "head_branch": "bugfix/utf-8",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c093ac254a19ac29c775",
    "number": 455,
    "body": "Fix iterating from the beginning bug when initializing the GroupedIterator. (https://github.com/pytorch/fairseq/issues/441)\r\n Correct filter criterion for dict type sentence size. (https://github.com/pytorch/fairseq/issues/451)",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c094ac254a19ac29c776",
    "number": 454,
    "body": "",
    "head_branch": "fix_stories",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c095ac254a19ac29c777",
    "number": 453,
    "body": "There was a very subtle bug here ðŸ˜¢When we recently removed this line (7633129ba8d5f0e28bd6b6d6027b14352482ef31), it meant that the learning rate scheduler didn't get initialized until after the first update. Unfortunately pytorch optimizers store the learning rate in their internal state, so some learning rate schedulers use their `__init__` method to reset the learning rate to some sane initial value. This is especially problematic for LR schedulers that include a warmup, where the Optimizer is likely to contain the peak learning rate at initialization, and it's only in the LR scheduler's `__init__` that the (much smaller) warmup value is set.\r\n\r\nFor example, the inverse_sqrt scheduler resets the learning rate upon initialization:\r\nhttps://github.com/pytorch/fairseq/blob/7853818c2e33a63ec17a31bcfe20e4fc75d94130/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py#L48-L50\r\n\r\n**Impact:** For the last ~1.5 weeks, the first training update would use the optimizer's default learning rate instead of the initial rate set by the LR scheduler. All subsequent updates used the correct learning rates. This primarily affects LR schedulers with warmups.",
    "head_branch": "fix_lr_scheduling",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c095ac254a19ac29c778",
    "number": 452,
    "body": "This is useful for averaging the last N checkpoints, ending at some \"best\" checkpoint.",
    "head_branch": "average_checkpoints_bound",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c096ac254a19ac29c779",
    "number": 449,
    "body": "On a multi-gpu training scenario, the `train.py` script spawns new processes with `torch.multiprocessing.spawn`. Unfortunately those child processes don't inherit the modules imported with `--user-dir`.\r\n\r\nThis pull request fixes this problem: custom module import in now explicit on every `main()` function.",
    "head_branch": "bugfix/distributed-userdir",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c097ac254a19ac29c77a",
    "number": 448,
    "body": "The `preprocess.py` script has been refactored in order to:\r\n\r\n1. Use the `options` module for command line arguments  parsing. This will give to `preprocess.py` the ability to load custom modules with `--user-dir` flag (already implemented to all other binaries)\r\n2. Dictionary loading and building code has moved to Task implementation. This allows custom Dictionary classes to be used during the data generation step.",
    "head_branch": "refactoring/preprocess",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c098ac254a19ac29c77b",
    "number": 447,
    "body": "Command line option --user-dir documented in docs/overview.rst",
    "head_branch": "docupdate",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c098ac254a19ac29c77c",
    "number": 446,
    "body": "Correct help message was obfuscated by the transient `ArgumentParser` used only for eagerly read `--user-dir` flag.\r\n\r\nTo reproduce just try:\r\n```bash\r\npython3 train.py --help\r\n```",
    "head_branch": "bugfix/helpmsg",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c099ac254a19ac29c77d",
    "number": 445,
    "body": "Command line option `--user-dir` documented in `docs/overview.rst`",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09aac254a19ac29c77e",
    "number": 442,
    "body": "minor fixes:\r\n1- adding fairseq logo\r\n2- encoder padding for fconv self att\r\n3- legacy ddp change",
    "head_branch": "fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09bac254a19ac29c77f",
    "number": 440,
    "body": "Following discussion on official fairseq (https://github.com/pytorch/fairseq/issues/438), I added the `--user-dir` option to the command line. The user can now specify a path in order to import a custom module with proprietary tasks, architectures and so on.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09cac254a19ac29c780",
    "number": 439,
    "body": "",
    "head_branch": "misc_fixes",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09cac254a19ac29c781",
    "number": 436,
    "body": "https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset is not longer valid, redirects to a blog post listing page.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09dac254a19ac29c782",
    "number": 433,
    "body": "",
    "head_branch": "docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09eac254a19ac29c783",
    "number": 432,
    "body": "",
    "head_branch": "rm_fb_train",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "rm fb_train.py (#432)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c09fac254a19ac29c784",
    "number": 428,
    "body": "",
    "head_branch": "merge_internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a0ac254a19ac29c785",
    "number": 425,
    "body": "",
    "head_branch": "mp_train",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a0ac254a19ac29c786",
    "number": 424,
    "body": "This was broken in 03a57de.",
    "head_branch": "fix_fp16_resume",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a1ac254a19ac29c787",
    "number": 422,
    "body": "- 04cc608: Add `--match-source-len` option to generate.py to for sequence-tagging tasks\r\n- 19f1a40: Add `--no-repeat-ngram-size` option to generate.py for ngram blocking",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a2ac254a19ac29c788",
    "number": 421,
    "body": "Add argument `--no-token-positional-embeddings` to TransformerModel (currently only available in TransformerLanguageModel) to disable positional embeddings.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a3ac254a19ac29c789",
    "number": 419,
    "body": "This improves performance for datasets that load data lazily. Enabled by default since it shouldn't compromise performance for non-lazy datasets.",
    "head_branch": "buffered_itr",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a4ac254a19ac29c78a",
    "number": 410,
    "body": "BacktranslationDataset would throw an error when the underlying dataset was an IndexedCachedDataset because prefetching was not handled correctly. This fixes the error.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a4ac254a19ac29c78b",
    "number": 406,
    "body": "Summary: Static helper function in TranslationTask to load pretrained models\n\nReviewed By: myleott\n\nDifferential Revision: D13345276\n",
    "head_branch": "export-D13345276",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a5ac254a19ac29c78c",
    "number": 404,
    "body": "Previously when training with --fp16, we stored a copy of the model parameters in FP32 for optimization, which consumed a lot of memory. An alternative is to just do the conversions to FP32 on the fly, which allows the caching allocator to reuse/save some memory.\r\n\r\nThis reduces peak memory usage by ~20% with a negligible reduction in training speed (~2% slower) when training a big transformer on 8 GPUs on wmt en-de with --update-freq=16.\r\n\r\nThis does not affect convergence, i.e., models will train exactly as they did before.",
    "head_branch": "fp16_mem_opt",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a6ac254a19ac29c78d",
    "number": 403,
    "body": "The original code reports the size of a valid sample instead of an invalid one when raising an Exception , which will make people confused.",
    "head_branch": "bug-fix",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a7ac254a19ac29c78e",
    "number": 400,
    "body": "",
    "head_branch": "upfreq_c10d",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a8ac254a19ac29c78f",
    "number": 399,
    "body": "Not switching to Black formatting just yet, but adding fmt: off directives in case we decide to later.",
    "head_branch": "fmt",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a8ac254a19ac29c790",
    "number": 398,
    "body": "",
    "head_branch": "assert_lstm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0a9ac254a19ac29c791",
    "number": 397,
    "body": "Summary: Let's only decrease the loss scale if a large enough percentage of batches overflow.\n\nDifferential Revision: D13355159\n",
    "head_branch": "export-D13355159",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0aaac254a19ac29c792",
    "number": 396,
    "body": "This kind of issue should be rare, but the exception that was thrown before (\"UnpicklingError: invalid load key\") was very opaque, so let's use something a bit clearer.",
    "head_branch": "better_sync_error_msg",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0abac254a19ac29c793",
    "number": 393,
    "body": "â€¦.LongTensor but found type torch.cuda.FloatTensor for argument #3 'index' \" error\r\n\r\nin the torch.__version__ == 0.4.0 , \r\nnew_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)  \r\nwill return a float dtype Tensor, when exec the \"line 321: fairseq/fairseq/models/fconv.py \" will throw a RuntimeError",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0abac254a19ac29c794",
    "number": 388,
    "body": "Differential Revision: D13244869\n",
    "head_branch": "export-D13244869",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0acac254a19ac29c795",
    "number": 386,
    "body": "Summary: This allows decoder embedding sharing for denoising autoencoder modules with different decoders (one for src decoding and one for tgt decoding)\n\nReviewed By: dpacgopinath\n\nDifferential Revision: D13133015\n",
    "head_branch": "export-D13133015",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0adac254a19ac29c796",
    "number": 385,
    "body": "Summary:\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/6\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14292\n\nDifferential Revision: D10517864\n",
    "head_branch": "export-D10517864",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0aeac254a19ac29c797",
    "number": 379,
    "body": "This can happen if a module is registered in more than one place in the network.",
    "head_branch": "fix_shared",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0afac254a19ac29c798",
    "number": 374,
    "body": "",
    "head_branch": "merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b0ac254a19ac29c799",
    "number": 373,
    "body": "See discussion in #335.\r\n\r\nI retrained WMT'16 En-De and get nearly identical results. This approach is safer but is also a bit slower.",
    "head_branch": "fp32_layernorm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b0ac254a19ac29c79a",
    "number": 372,
    "body": "",
    "head_branch": "fix_docs",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b1ac254a19ac29c79b",
    "number": 370,
    "body": "This should bring back the speedup with --update-freq that we reported in the Scaling Neural Machine Translation paper.",
    "head_branch": "legacy_ddp",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b2ac254a19ac29c79c",
    "number": 366,
    "body": "",
    "head_branch": "fix_max_tok",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b3ac254a19ac29c79d",
    "number": 364,
    "body": "add Translation class to make server.",
    "head_branch": "myserver",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b4ac254a19ac29c79e",
    "number": 362,
    "body": "Summary:\nPull Request resolved: https://github.com/pytorch/translate/pull/254\n\nThis actually uses the fairseq logic which supports BPE cont / end word marker suffixes.\n\nReviewed By: xianxl\n\nDifferential Revision: D12952766\n",
    "head_branch": "export-D12952766",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b4ac254a19ac29c79f",
    "number": 354,
    "body": "- generalize AppendEosDataset -> TransformEosDataset\r\n- remove EOS logic from BacktranslationDataset (use TransformEosDataset instead)\r\n- BacktranslationDataset takes a backtranslation_fn instead of building the SequenceGenerator itself",
    "head_branch": "oss_bt_refactor",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b5ac254a19ac29c7a0",
    "number": 352,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b6ac254a19ac29c7a1",
    "number": 341,
    "body": "Summary: Use black formatting in test_noising.py\n\nReviewed By: xianxl\n\nDifferential Revision: D12810285\n",
    "head_branch": "export-D12810285",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b7ac254a19ac29c7a2",
    "number": 340,
    "body": "Summary: This allows us to do a lot less copy paste when adding new word shuffle function tests\n\nReviewed By: xianxl\n\nDifferential Revision: D12810304\n",
    "head_branch": "export-D12810304",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b8ac254a19ac29c7a3",
    "number": 339,
    "body": "Currently, if `ignore-case` is set, the same line will be yielded twice - once as lower-cased version, once as original version, leading to lower than expected uncased scores.",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b8ac254a19ac29c7a4",
    "number": 338,
    "body": "Summary:\nPull Request resolved: https://github.com/pytorch/translate/pull/251\n\nWe should use shared encoder and separate decoders as in:\n\nhttps://fb.facebook.com/groups/2156114531381111/permalink/2169028113423086/\n\nGeneration is a hack, ideally the net input should have the lang pair info so that when we pass the sample to the model, it can select the correct encoder/decoder pair.\n\ndiff [2/2] will be for flow integration for basic experimentation\n\nTODO in a future diff: figure out how to generalize this so export will work??\n\nThis works with vocab reduction, but we only support vocab reduction for src-tgt, not src-src model. A future (lowpri) task could be to add word prediction vocab reduction for src-src model to speed up training.\n\nReviewed By: xianxl\n\nDifferential Revision: D10512576\n",
    "head_branch": "export-D10512576",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0b9ac254a19ac29c7a5",
    "number": 337,
    "body": "Differential Revision: D12880352\n",
    "head_branch": "export-D12880352",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0baac254a19ac29c7a6",
    "number": 336,
    "body": "",
    "head_branch": "fix_tests",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bbac254a19ac29c7a7",
    "number": 333,
    "body": "Summary: A tiny hack to speed up inference slightly for transformer beam search after export to graph mode. Specifically, there is no need to transpose a dimension with size 1 (the sequence length of a single decoder time step during beam search) with its neighbor immediately before a view/reshape.\n\nDifferential Revision: D12833011\n",
    "head_branch": "export-D12833011",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bbac254a19ac29c7a8",
    "number": 330,
    "body": "Summary:\nAs part of the semi sueprvised task setup (https://github.com/pytorch/translate/pull/243), this diff adds the ability for LanguagePairDataset to remove EOS from source or append EOS to target. This functionality is required by BacktranslationDataset to use translations as source data.\n\nAlso added changes to BacktranslationDataset to make it work on GPU. We needed to transfer back-translated sentences back to CPU for the LanguagePairDataset to collate.\n\nDifferential Revision: D10846294\n",
    "head_branch": "export-D10846294",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bcac254a19ac29c7a9",
    "number": 326,
    "body": "Summary:\nThis code is heavily inspired (aka copied in parts) from @[139800366:Myle Ott]'s multilingual transformers implementation in [fairseq's PR #385](https://github.com/fairinternal/fairseq-py/pull/385/commits/18fa6e154781cf0c4b1596429dba7e753a545069). I am using pytorch_translate repo as a test bed to play around with the implementation before submitting a clean PR to fairseq.\n\nThe setup uses a new PytorchTranslateSemiSupervised task introduced in D10228123. The basic idea is to use FairseqMultiModel in conjunction with RoundRobinZipDataset to train 4 \"language pairs\":\n1) source-target with LanguagePairDataset\n2) target-source with reverse LanguagePairDataset\n3) source-target_mono with BacktranslationDataset\n4) target-source_mono with BacktranslationDataset\n\n1 and 3, and 2 and 4 use the same models.\n\nThis is not multilingual yet, in that only one source_lang and one target_lang can be provided. However, this can easily be converted to multilingual semi supervised training by removing the dependency on source_lang and target_lang to prepare lang_pairs.\n\nIn the next diffs, we could consider adding support for:\nChar source model\nWeighted dataset\nMultilingual multi_model\n\nReviewed By: liezl200\n\nDifferential Revision: D10318226\n",
    "head_branch": "export-D10318226",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bdac254a19ac29c7aa",
    "number": 325,
    "body": "Summary: RoundRobinZipDataset requires size(index) method implemented in every dataset used. Also added missing return statements in a few methods.\n\nReviewed By: liezl200\n\nDifferential Revision: D10457159\n",
    "head_branch": "export-D10457159",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0beac254a19ac29c7ab",
    "number": 324,
    "body": "Summary: BacktranslationDataset was introduced recently but was not exposed as part of the fairseq.data module\n\nReviewed By: liezl200\n\nDifferential Revision: D10412717\n",
    "head_branch": "export-D10412717",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bfac254a19ac29c7ac",
    "number": 321,
    "body": "Reviewed By: alexeib\n\nDifferential Revision: D10430186\n",
    "head_branch": "export-D10430186",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0bfac254a19ac29c7ad",
    "number": 320,
    "body": "Modify Error message of bleu.\r\nFix the issue:  https://github.com/pytorch/fairseq/issues/284",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c0ac254a19ac29c7ae",
    "number": 317,
    "body": "Summary:\nWhen upgrading `state_dict` variable, `upgrade_state_dict` function in TransformerEncoder/TransformerDecoder doesn't handle multiple encoders/decoders, however, D10052908 will be the case.\n\nBefore the change, we will hit error message [1] when loading checkpoint for multilingual_transformer model in D10052908. This diff will fix it.\n\nDifferential Revision: D10375418\n",
    "head_branch": "export-D10375418",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c1ac254a19ac29c7af",
    "number": 316,
    "body": "Summary: This code should actually be keeping the padded positions as `padding_idx` (though note that this is on the ONNX export path, and it has no effect in the most common case when using the exported network to do un-batched inference).\n\nDifferential Revision: D10431872\n",
    "head_branch": "export-D10431872",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c2ac254a19ac29c7b0",
    "number": 315,
    "body": "Patching #314 ",
    "head_branch": "patch-3",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c3ac254a19ac29c7b1",
    "number": 306,
    "body": "Summary: This uses a source dataset to generate a batch of {source: noisy source, target: original clean source} which allows us to train a denoising autoencoding component as part of a seq2seq model.\n\nReviewed By: xianxl\n\nDifferential Revision: D10078981\n",
    "head_branch": "export-D10078981",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c3ac254a19ac29c7b2",
    "number": 305,
    "body": "Summary: Previously, noising code assumed that every sentence had an EOS which had to be excluded from noising operations (since we shouldn't drop, blank, or shuffle EOS). This logic allows the noising module to handle sentences with EOS and without EOS\n\nReviewed By: xianxl\n\nDifferential Revision: D10114425\n",
    "head_branch": "export-D10114425",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c4ac254a19ac29c7b3",
    "number": 304,
    "body": "Summary:\nThough transpose operations are essentially free during PyTorch execution, they can result in costly operations when exported to Caffe2 inference nets via ONNX tracing, especially when applied repeatedly to large tensors.\n\nFor this reason, we update `MultiheadAttention` to store its incremental state with shape (bsz, num_heads, seq_len, head_dim), that is after transposing the projected input. This should result in non-trivially faster exported models without changing the semantics or speed of PyTorch execution.\n\nDifferential Revision: D10186506\n",
    "head_branch": "export-D10186506",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c5ac254a19ac29c7b4",
    "number": 302,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c6ac254a19ac29c7b5",
    "number": 300,
    "body": "",
    "head_branch": "oss-master",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c7ac254a19ac29c7b6",
    "number": 299,
    "body": "",
    "head_branch": "michaelauli-patch-1",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c7ac254a19ac29c7b7",
    "number": 296,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c8ac254a19ac29c7b8",
    "number": 295,
    "body": "Changelog:\r\n- `90f52a1`: Support loading subsets of the data on each worker with the `--fix-batches-to-gpus` flag. This should fix #217 and #266.\r\n- `6eda0a9`: Update README for replicating the \"Scaling Neural Machine Translation\" paper\r\n- `b14c7cf`: Fallback to no_c10d backend for pytorch 0.4.1 (fixes #294)",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0c9ac254a19ac29c7b9",
    "number": 293,
    "body": "fixing bugs with error \"argument must be tuple of ints, not Tensor\"",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0caac254a19ac29c7ba",
    "number": 291,
    "body": "Changelog:\r\n- 4908863: Switch to DistributedDataParallelC10d and bump version 0.5.0 -> 0.6.0\r\n  - no more FP16Trainer, we just have an FP16Optimizer wrapper\r\n  - most of the distributed code is moved to a new wrapper class called DistributedFairseqModel, which behaves like DistributedDataParallel and a FairseqModel at the same time\r\n  - Trainer now requires an extra dummy_batch argument at initialization, which we do fwd/bwd on when there's an uneven number of batches per worker. We hide the gradients from these dummy batches by multiplying the loss by 0\r\n  - Trainer.train_step now takes a list of samples, which will allow cleaner --update-freq\r\n- 1c56b58: parallelize preprocessing\r\n- Misc bug fixes and features",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Online backtranslation module\n\nCo-authored-by: liezl200 <lie@fb.com>"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0cbac254a19ac29c7bb",
    "number": 290,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0cbac254a19ac29c7bc",
    "number": 287,
    "body": "",
    "head_branch": "oss-master",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #287 from pytorch/oss-master\n\nUpdate readme with WMT'18 model (#433)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ccac254a19ac29c7bd",
    "number": 279,
    "body": "",
    "head_branch": "oss-master",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #279 from pytorch/oss-master\n\nOss master"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0cdac254a19ac29c7be",
    "number": 268,
    "body": "Changelog:\r\n- `9526b7f`: Add documentation\r\n- `3b21982`: Add \"cosine\" and \"triangular\" LR schedulers\r\n- `f67d535`: Factor out search logic making it easier to add new search algorithms\r\n- `14036b6`: Add diverse beam search\r\n- `3a3b60c`: Add character-level token embedding layer\r\n- `3c11ff4`: Refactor FairseqTask so it's easier to extend and add new data batching strategies\r\n- misc bug fixes and other features",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add documentation"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ceac254a19ac29c7bf",
    "number": 249,
    "body": "Fixes #239.",
    "head_branch": "fix_bidir_lstm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix bidirectional LSTM concatenation (#249)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0cfac254a19ac29c7c0",
    "number": 244,
    "body": "tf's implementation with normalizations preceding ops also has normalizations at the end of stack. \r\nhttps://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py#L343\r\nhttps://github.com/tensorflow/models/blob/master/official/transformer/model/transformer.py#L417\r\nThis PR makes fairseq consistent with tf, otherwise t2t config explodes. ",
    "head_branch": "t2t_norm",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "add end-of-stack normalizations in case normalize_before has been set (#244)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0cfac254a19ac29c7c1",
    "number": 240,
    "body": "Previously there is label smoothing for cross-entropy.  We want to add label smoothing for adaptive_loss.\r\n\r\n1. refactored the adaptive_loss\r\n2. added a LabelSmoothedAdaptiveLoss criterion.\r\n3. add unit test",
    "head_branch": "add_label_smooth_for_adaptive_loss",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d0ac254a19ac29c7c2",
    "number": 236,
    "body": "",
    "head_branch": "oss-fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix bug when training with FP32 and --update-freq (#236)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d1ac254a19ac29c7c3",
    "number": 235,
    "body": "Resolves the problem described in #223 ",
    "head_branch": "ensemble",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add ensemble for different architectures (#235)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d2ac254a19ac29c7c4",
    "number": 234,
    "body": "The helper messages / description of the arguments are not exactly right for `preprocess.py`:\r\n\r\n**Currently**: \r\n\r\n```python\r\nparser.add_argument('--trainpref', metavar='FP', default=None, help='target language')\r\nparser.add_argument('--validpref', metavar='FP', default=None, help='comma separated, valid language prefixes')\r\nparser.add_argument('--testpref', metavar='FP', default=None, help='comma separated, test language prefixes')\r\n```\r\n\r\nIn details:\r\n\r\n - `--trainpref` isn't the target language, that's the `-t` / `--target-language` argument\r\n     - It should be the training file prefix, hence `help='train file prefix'`  edit\r\n - `--validpref` and `--testpref` isn't really the language prefix, it's the file prefix before the language, hence:\r\n     - `help='comman separated, valid file prefixes'`\r\n     - `help='comman separated, test file prefixes'`",
    "head_branch": "patch-2",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Correct the help name of the prefixes arguments (#234)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d3ac254a19ac29c7c5",
    "number": 230,
    "body": "Initially it's pointing to some arbitrary `/data/iwslt14.tokenized.de-en`, but given the steps above to cd `examples/translation` and then `cd ../..`, the appropriate path that contains the `iwslt14.tokenized.de-en` is `examples/translation/iwslt14.tokenized.de-en`",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Correct path in the pre-processing example (#230)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d3ac254a19ac29c7c6",
    "number": 229,
    "body": "",
    "head_branch": "load_optim",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add load_optim option to load checkpoint but not optimizer state (#229)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d4ac254a19ac29c7c7",
    "number": 228,
    "body": "Changelog:\r\n- `f472d14`: Support tied embeddings in LSTM encoder/decoder\r\n- `89e19d4`: Don't print alignment by default (use `--print-alignment` to re-enable it)\r\n- `d2e2a1d`: Add Transformer-based language model\r\n- `c279407`: Add new Transformer configuration for IWSLT\r\n- `2fbfda0`: Misc changes for pytorch-translate\r\n- Miscellaneous bug fixes",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge internal changes\n\nChangelog:\r\n- `f472d14`: Support tied embeddings in LSTM encoder/decoder\r\n- `89e19d4`: Don't print alignment by default (use `--print-alignment` to re-enable it)\r\n- `d2e2a1d`: Add Transformer-based language model\r\n- `c279407`: Add new Transformer configuration for IWSLT\r\n- `2fbfda0`: Misc changes for pytorch-translate\r\n- Miscellaneous bug fixes"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d5ac254a19ac29c7c8",
    "number": 226,
    "body": "",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "fixed output_proj's input_dim in attention (#226)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d6ac254a19ac29c7c9",
    "number": 215,
    "body": "This is a simple update for the `sequence_generator.py` script to support generation from a language model trained on the target vocabulary. Changes include:\r\n\r\n1) Clone `max_positions` for source / target if it's set by a monolingual  LM;\r\n2) Set the encoder output dictionary to None when using LM;\r\n3) Check for None values of encoder output dictionary when calling `reorder_encoder_out`\r\n4) Assert targets are given for adaptive softmax only at training time;\r\n5) Copy attention values only if provided by the decoder model;\r\n6) Squeeze the `probs` tensor instead of `decoder_out` as the adaptive softmax probability function expects 3D inputs.\r\n\r\nThis is a follow up on the issues: #213 and #212. I'm basically replicating the LM results from the hierarchical story generation paper (https://arxiv.org/abs/1805.04833), but I can't find the LM generation code in the repo. I'd appreciate if @huihuifan can take a look at this, in case it's a replicated or redundant pull request. \r\n",
    "head_branch": "generate_lm",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d7ac254a19ac29c7ca",
    "number": 214,
    "body": "fixes #213 ",
    "head_branch": "assert_vocab_size",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "assert that vocab size >= adaptive softmax cutoff (#214)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d7ac254a19ac29c7cb",
    "number": 211,
    "body": "This deals with issue 181. Generally I removed defaults from parser.add_argument and instead added those defaults to where the model architectures were registered. The main exception is for the fconv_self_att_wp model where I made it's defaults consistent with what is in the stories readme so that'd it match closer with the paper. I didn't change the readme, although if wanted I can simplify the train line a bit now as the model options used there now consider to the ones it'll have without specifying them.",
    "head_branch": "default_model_options",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix up model defaults (#211)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d8ac254a19ac29c7cc",
    "number": 208,
    "body": "",
    "head_branch": "refactor_dro",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0d9ac254a19ac29c7cd",
    "number": 205,
    "body": "I think it's a typo",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "fix decoder_normalize_before typo (#205)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0daac254a19ac29c7ce",
    "number": 203,
    "body": "- use newstest2013 for validation instead of splitting the training set\r\n- apply length filtering before BPE\r\n- final dataset is ~4.5M documents\r\n- confirmed this new dataset gives results on par with the Scaling NMT paper",
    "head_branch": "preprocess_wmt_en_de",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix preprocessing for WMT14 En-De to replicate Scaling NMT paper (#203)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0dbac254a19ac29c7cf",
    "number": 200,
    "body": "",
    "head_branch": "fix_prep",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove unnecessary assert (fixes #199) (#200)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0dbac254a19ac29c7d0",
    "number": 198,
    "body": "",
    "head_branch": "rm_variable",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove more Variable() calls (#198)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0dcac254a19ac29c7d1",
    "number": 197,
    "body": "",
    "head_branch": "fix_attn_test",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix attention order in unit tests (fixes #195) (#197)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ddac254a19ac29c7d2",
    "number": 196,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Misc changes for pytorch-translate"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0deac254a19ac29c7d3",
    "number": 193,
    "body": "The current version of eval_lm, while it accepts a task only really works if the task language_modeling since tokens_per_sample isn't an option for translation. This one line fix makes it work for translation.",
    "head_branch": "minor_fixes",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #193 from hmc-cs-mdrissi/minor_fixes\n\ndefault samples_per_token in eval_lm"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0dfac254a19ac29c7d4",
    "number": 192,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Add steps to reproduce WMT En-De results from Scaling NMT paper"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0dfac254a19ac29c7d5",
    "number": 191,
    "body": "The preprocessing options added are an option to choose which tokenization algorithm used (current choices being the old one as default and nltk's word_tokenize) and a max_length for both source/target. The max_length just truncates any sequences as needed. If no max_length is desired than -1 works and is the default.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e0ac254a19ac29c7d6",
    "number": 190,
    "body": "",
    "head_branch": "fix_raw",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix `--output-format raw` option to preprocess.py (Fixes #188) (#190)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e1ac254a19ac29c7d7",
    "number": 189,
    "body": "",
    "head_branch": "fix_readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix translation README (fixes #186) (#189)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e2ac254a19ac29c7d8",
    "number": 185,
    "body": "",
    "head_branch": "classic_seqlevel",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e3ac254a19ac29c7d9",
    "number": 180,
    "body": "Otherwise, even with padding_factor specified, dictionary size remains non-multiple of padding factor. ",
    "head_branch": "fin_dict",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "add count for padding words (#180)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e3ac254a19ac29c7da",
    "number": 178,
    "body": "Changelog:\r\n- 97b58b4: add Transformer model from Vaswani et al. (2017)\r\n- b2374e5: faster Transformer inference with improved caching\r\n- 2d27ae0: simulate large mini-batch training with delayed updates (`--update-freq`)\r\n- 7ee1d28: add FP16 training support (`--fp16`)\r\n- 2a84f46: faster inference by removing completed sentences from the batch\r\n- 663fd80: batched interactive generation\r\n- 4c2ef2d: add language modeling / gated convolutional model from Dauphin et al. (2017)\r\n- b59815b: add Hierarchical Neural Story Generation model from Fan et al. (2018)\r\n- ff68a9e: add FairseqTask to modularize task definitions (e.g., translation, language modeling)",
    "head_branch": "0.5.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "0.4.0 -> 0.5.0\n\nChangelog:\r\n- 97b58b4: add Transformer model from Vaswani et al. (2017)\r\n- b2374e5: faster Transformer inference with improved caching\r\n- 2d27ae0: simulate large mini-batch training with delayed updates (`--update-freq`)\r\n- 7ee1d28: add FP16 training support (`--fp16`)\r\n- 2a84f46: faster inference by removing completed sentences from the batch\r\n- 663fd80: batched interactive generation\r\n- 4c2ef2d: add language modeling / gated convolutional model from Dauphin et al. (2017)\r\n- b59815b: add Hierarchical Neural Story Generation model from Fan et al. (2018)\r\n- ff68a9e: add FairseqTask to modularize task definitions (e.g., translation, language modeling)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e4ac254a19ac29c7db",
    "number": 177,
    "body": "  Add flag: workers. Use multiprocess to parallelize Tokenizer.binarize.",
    "head_branch": "parallel-preprocess",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e5ac254a19ac29c7dc",
    "number": 171,
    "body": "In interactive.py and generate.py , if we try to load a model which was trained pre trained embeddings, it will try to load the files, eventhough the model checkpoints files we give have the embedding weights.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e6ac254a19ac29c7dd",
    "number": 163,
    "body": "Changelog:\r\n- Add support for bidirectional LSTM encoder and more granular hidden/output dims\r\n- Small changes for pytorch/translate",
    "head_branch": "merge_internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge internal changes (#163)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e7ac254a19ac29c7de",
    "number": 162,
    "body": "",
    "head_branch": "dataload",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e7ac254a19ac29c7df",
    "number": 161,
    "body": "For use by https://github.com/pytorch/translate/pull/62",
    "head_branch": "data",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Update dataset code for use by https://github.com/pytorch/translate/pull/62 (#161)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e8ac254a19ac29c7e0",
    "number": 157,
    "body": "We need to add these args in case they don't exist in the checkpoint.",
    "head_branch": "fix_pretrained_embed",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix old model checkpoints after #151 (fixes #156) (#157)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0e9ac254a19ac29c7e1",
    "number": 152,
    "body": "",
    "head_branch": "no_padding",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "use implicit padding when possible (#152)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0eaac254a19ac29c7e2",
    "number": 151,
    "body": "I have added new flags to set pre-trained embeddings for encoder and decoder.\r\n\r\nEach line of the pre-trained embedding file should word followed by the dimension values of the corresponding vector.\r\nExample for embedding with 4 dimensions.\r\n```\r\nHi 0.2 0.1 0.7\r\nThis 0.33 0.22 0.44\r\n```",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Add pretrained embedding support (#151)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ebac254a19ac29c7e3",
    "number": 147,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update README.md"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ebac254a19ac29c7e4",
    "number": 142,
    "body": "Regardless of where the script is called from",
    "head_branch": "setup",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ecac254a19ac29c7e5",
    "number": 136,
    "body": "Changes:\r\n- 7d19e36: Add `--sampling` flag to generate.py to sample instead of doing beam search\r\n- c777340: Add `scripts/average_checkpoints.py` to average multiple checkpoints into a combined model\r\n- 3ea882c: Add `--max-update` option to train.py to stop training after a given number of updates\r\n- small bugfixes for distributed training, LSTM, inverse square root LR scheduler",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge internal changes (#136)\n\nChanges:\r\n- 7d19e36: Add `--sampling` flag to generate.py to sample instead of doing beam search\r\n- c777340: Add `scripts/average_checkpoints.py` to average multiple checkpoints into a combined model\r\n- 3ea882c: Add `--max-update` option to train.py to stop training after a given number of updates\r\n- small bugfixes for distributed training, LSTM, inverse square root LR scheduler"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0edac254a19ac29c7e6",
    "number": 134,
    "body": "Update training commands in data/README to match the latest version of this project according to #132.\r\n\r\n- Motivation: in the previous data/README, the commands are obsolete and will cause the error \"unrecognized arguments: --label-smoothing 0.1 --force-anneal 50\". \r\n- What's changed: add arguments \"--criterion label_smoothed_cross_entropy\" and \"--lr-scheduler fixed\" to the training commands of all 3 datasets.\r\n- Result: the new commands run without error on all 3 datasets.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #134 from hitvoice/master\n\nUpdate training commands"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0eeac254a19ac29c7e7",
    "number": 131,
    "body": "Change \"awailable\" to \"available\". It looks like a spelling error in the original version.",
    "head_branch": "master",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "fix typo in data/README (#131)\n\nChange \"awailable\" to \"available\"."
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0efac254a19ac29c7e8",
    "number": 121,
    "body": "Fixes #119.",
    "head_branch": "enforce_maxlen",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Enforce upper-bound on maximum generation length (#121)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0efac254a19ac29c7e9",
    "number": 116,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #116 from facebookresearch/oss-merge-internal\n\nOss merge internal"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f0ac254a19ac29c7ea",
    "number": 114,
    "body": "Fixes #110.",
    "head_branch": "more_pytorch",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "More updates for PyTorch (#114)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f1ac254a19ac29c7eb",
    "number": 113,
    "body": "Fixes #109",
    "head_branch": "fix_topk",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "More fixes for recent PyTorch (incl. topk issue) (#113)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f2ac254a19ac29c7ec",
    "number": 107,
    "body": "",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #107 from facebookresearch/oss-merge-internal\n\nOss merge internal changes"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f3ac254a19ac29c7ed",
    "number": 106,
    "body": "This PR includes breaking API changes to modularize fairseq-py and adds support for distributed training across multiple nodes.\r\n\r\nChanges:\r\n- c7033ef: add support for distributed training! See updated README for usage.\r\n- e016299: modularize fairseq-py, adding support for register_model, register_criterion, register_optimizer, etc.\r\n- 154e440: update LSTM implementation to use PackedSequence objects in the encoder, better following best practices and improving perf\r\n- 90c2973 and 1da6265: improve unit test coverage",
    "head_branch": "distributed",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "fairseq-py goes distributed (#106)\n\nThis PR includes breaking API changes to modularize fairseq-py and adds support for distributed training across multiple nodes.\r\n\r\nChanges:\r\n- c7033ef: add support for distributed training! See updated README for usage.\r\n- e016299: modularize fairseq-py, adding support for register_model, register_criterion, register_optimizer, etc.\r\n- 154e440: update LSTM implementation to use PackedSequence objects in the encoder, better following best practices and improving perf\r\n- 90c2973 and 1da6265: improve unit test coverage"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f4ac254a19ac29c7ee",
    "number": 101,
    "body": "The default maxlen in SequenceGenerator should be dictated by the decoder, not set arbitrarily to 200.\r\n\r\nNote that the default value for `--max-len-b` is still 200, but this PR allows this limit to be increased.",
    "head_branch": "fix_maxlen",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Allow larger maxlen (fixes #100) (#101)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f4ac254a19ac29c7ef",
    "number": 95,
    "body": "To be compatible with multi-bleu.\r\nThis seems to only affect the result_string.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Merge pull request #95 from bastings/patch-1\n\nBLEU ratio should be predlen/reflen not reflen/predlen"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f5ac254a19ac29c7f0",
    "number": 94,
    "body": "Changes:\r\n- 7a3ad09: Add support for sharded generation\r\n- cdb38ed: Add support for sharing decoder input/output embed in fconv models\r\n- 395cd62 and 4066dc3: Improvements to NAG and Adam optimizers\r\n- Misc fixes for new versions of PyTorch (fixes #93)\r\n",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fixed Weight Decay Regularization in Adam\n\nSee https://arxiv.org/abs/1711.05101"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f6ac254a19ac29c7f1",
    "number": 91,
    "body": "I'm testing these scripts at the moment. Once the testing is done, I'll update and land this PR",
    "head_branch": "prepare_wmt",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #91 from facebookresearch/prepare_wmt\n\nPrepare scripts for WMT14 (#88)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f7ac254a19ac29c7f2",
    "number": 77,
    "body": "Changes:\r\n- a233fce: Catch and recover from OOM in some cases and improve memory usage\r\n- 9f3ccaa: Fix weight norm dimension in decoder (fixes #73)\r\n- 0a83627: Fix conv padding for even kernel widths\r\n- Misc other features",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #77 from facebookresearch/oss-merge-internal"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f8ac254a19ac29c7f3",
    "number": 75,
    "body": "This is really cool work and I found these typos while reading through it.\r\n\r\nTo make sure there weren't any more, I also spellchecked it all with PyCharm.\r\n\r\nGotta start my open source career somewhere ðŸ•´ ",
    "head_branch": "fix_2_typos",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fixed 2 typos (#75)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f8ac254a19ac29c7f4",
    "number": 68,
    "body": "The lstm model losses a projection from lstm hidden state to out_embed. Is it a bug?",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "fix bug in lstm model (#68)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0f9ac254a19ac29c7f5",
    "number": 66,
    "body": "",
    "head_branch": "aten_convtbc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Use ATen built-in conv_tbc method (#66)\n\nRemove custom ConvTBC code"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0faac254a19ac29c7f6",
    "number": 62,
    "body": "Fixes #59.",
    "head_branch": "req",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update requirements.txt and fix flake8 (#62)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0fbac254a19ac29c7f7",
    "number": 61,
    "body": "",
    "head_branch": "python36",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove Python3.6 format string from preprocess.py (fixes #60) (#61)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0fcac254a19ac29c7f8",
    "number": 58,
    "body": "",
    "head_branch": "python36",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Remove more Python 3.6 format strings (fixes #57) (#58)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0fcac254a19ac29c7f9",
    "number": 56,
    "body": "",
    "head_branch": "python3",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #56 from facebookresearch/python3\n\nRemove Python 3.6 format strings (fixes #55)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0fdac254a19ac29c7fa",
    "number": 54,
    "body": "Release notes:\r\n- 5c7f495: Added simple LSTM model with input feeding and attention\r\n- 6e4b7e2: Refactored model definitions and incremental generation to be cleaner\r\n- 7ae79c1: Split interactive generation out of generate.py and into a new binary: interactive.py\r\n- 19a3865: Subtle correctness fix in beam search decoder. Previously, for a beam size of k, we might emit a hypotheses if the <eos> was among the top 2*k candidates. Now we only emit hypotheses for which the <eos> is among the top-k candidates. This may subtly change generation results, and in the case of k=1 we will now produce strictly greedy outputs.\r\n- 97d7fcb: Fixed bug in padding direction, where previously we right-padded the source and left-padded the target. We now left-pad the source and right-pad the target. This should not effect existing trained models, but may change (usually improves) the quality of new models.\r\n- f442f89: Add support for batching based on the number of sentences (`--max-sentences`) in addition to the number of tokens (`--max-tokens`). When batching by the number of sentences, one can optionally normalize the gradients by the number of sentences with `--sentence-avg` (the default is to normalize by the number of tokens).\r\n- c6d6256: Add `--log-format` option and JSON logger",
    "head_branch": "oss-v0.2.0",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #54: Version 0.1.0 -> 0.2.0\n\nRelease notes:\r\n- 5c7f495: Added simple LSTM model with input feeding and attention\r\n- 6e4b7e2: Refactored model definitions and incremental generation to be cleaner\r\n- 7ae79c1: Split interactive generation out of generate.py and into a new binary: interactive.py\r\n- 19a3865: Subtle correctness fix in beam search decoder. Previously, for a beam size of k, we might emit a hypotheses\r\n           if the <eos> was among the top 2*k candidates. Now we only emit hypotheses for which the <eos> is among the\r\n           top-k candidates. This may subtly change generation results, and in the case of k=1 we will now produce\r\n           strictly greedy outputs.\r\n- 97d7fcb: Fixed bug in padding direction, where previously we right-padded the source and left-padded the target. We\r\n           now left-pad the source and right-pad the target. This should not effect existing trained models, but may\r\n           change (usually improves) the quality of new models.\r\n- f442f89: Add support for batching based on the number of sentences (`--max-sentences`) in addition to the number of\r\n           tokens (`--max-tokens`). When batching by the number of sentences, one can optionally normalize the gradients\r\n           by the number of sentences with `--sentence-avg` (the default is to normalize by the number of tokens).\r\n- c6d6256: Add `--log-format` option and JSON logger"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0feac254a19ac29c7fb",
    "number": 53,
    "body": "This reduces the size of model checkpoints, especially when resuming training. It partially addresses #51.",
    "head_branch": "last_optim",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Only save most recent optimizer state in checkpoints (#53)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c0ffac254a19ac29c7fc",
    "number": 49,
    "body": "",
    "head_branch": "update_readme",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update README with note about Docker (#49)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c100ac254a19ac29c7fd",
    "number": 46,
    "body": "The note is taken from https://github.com/pytorch/pytorch#from-source. If someone uses fairseq with docker and does not set these flags he will get a very obscurve `Bus error`.",
    "head_branch": "patch-1",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c100ac254a19ac29c7fe",
    "number": 42,
    "body": "",
    "head_branch": "clang-fixes",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix for building under clang: specify C++ build and use C++ linkage (#42)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c101ac254a19ac29c7ff",
    "number": 33,
    "body": "Changes:\r\n- Add support for NCCL v2\r\n- Add support for additional optimizers\r\n- SequenceGenerator returns attention matrix\r\n- Misc bugfixes (e.g., fixes #32) and cleanup",
    "head_branch": "oss-merge-internal",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Merge pull request #33 from facebookresearch/oss-merge-internal\n\nChanges:\r\nAdd support for NCCL v2\r\nAdd support for additional optimizers\r\nSequenceGenerator returns attention matrix\r\nMisc bugfixes (e.g., fixes #32) and cleanup"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c102ac254a19ac29c800",
    "number": 30,
    "body": "",
    "head_branch": "playma",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [],
    "has_test_file": false,
    "was_merged": false,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c103ac254a19ac29c801",
    "number": 22,
    "body": "This should work with both the updated ATen ordering and the previous version\r\n\r\nFixes #20 ",
    "head_branch": "convtbc",
    "is_a_fork": true,
    "comments": [],
    "commit_messages": [
      "Fix call ordering to ATen addmm and sum (#22)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c104ac254a19ac29c802",
    "number": 21,
    "body": "- simplify `simple_progress_bar` so that it doesn't depend on tqdm anymore\r\n- add `refresh=False` to `set_postfix` calls in train.py and generate.py (fixes fast refresh problem introduced by new versions of tqdm and referenced in #17)",
    "head_branch": "tqdm",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Update progress_bar to be more robust to changes in tqdm (#21)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c105ac254a19ac29c803",
    "number": 11,
    "body": "Since PyTorch initializes gradient buffers lazily, it's important that the first batch doesn't contain any empty samples. This PR replaces empty samples by cycling through the given samples instead of using None.",
    "head_branch": "fix_empty_batches",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix handling of partially-empty initial batch (#11)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  },
  {
    "_id": "6621c105ac254a19ac29c804",
    "number": 8,
    "body": "Previously, generation would fail when the output vocabulary was small relative to the beam size. This PR fixes generation in this case by enforcing an upper bound on the beam size.",
    "head_branch": "small_vocab_fix",
    "is_a_fork": false,
    "comments": [],
    "commit_messages": [
      "Fix generation when vocabulary is small relative to beam size (fixes #7)"
    ],
    "has_test_file": false,
    "was_merged": true,
    "documentation_file_was_changed": false,
    "documentation_or_comments_in_code_were_changed": false
  }
]